ISSUE: No Mass Delete Option for Task Instances Similar to What DAGRuns Have in UI
### Apache Airflow version

2.1.3 (latest released)

### Operating System

macOS Big Sur 11.3.1

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

default Astronomer deployment with some test DAGs

### What happened

In the UI for DAGRuns there are checkboxes that allow multiple DAGRuns to be selected.

![image](https://user-images.githubusercontent.com/89415310/133841290-94cda771-9bb3-4677-8530-8c2861525719.png)

Within the Actions menu on this same view, there is a Delete option which allows multiple DAGRuns to be deleted at the same time.

![image](https://user-images.githubusercontent.com/89415310/133841362-3c7fc81d-a823-4f41-8b1d-64be425810ce.png)

Task instance view on the UI does not offer the same option, even though Task Instances can be individually deleted with the trash can button.

![image](https://user-images.githubusercontent.com/89415310/133841442-3fee0930-a6bb-4035-b172-1d10b85f3bf6.png)


### What you expected to happen

I expect that the Task Instances can also be bulk deleted, in the same way that DAGRuns can be.

### How to reproduce

Open up Task Instance and DAGRun views from the Browse tab and compare the options in the Actions dropdown menus.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/googleapis/google-cloud-python/packages/google-apps-card/google/apps/card_v1/types/card.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/googleapis/google-cloud-python/packages/google-cloud-automl/google/cloud/automl_v1/types/io.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.0

=========================================================

ISSUE: [Smart sensor] Runtime error: dictionary changed size during iteration
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->


**What happened**:

<!-- (please include exact error messages if you can) -->
Smart Sensor TI crashes with a Runtime error. Here's the logs:
```
RuntimeError: dictionary changed size during iteration
  File "airflow/sentry.py", line 159, in wrapper
    return func(task_instance, *args, session=session, **kwargs)
  File "airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "airflow/sensors/smart_sensor.py", line 736, in execute
    self.flush_cached_sensor_poke_results()
  File "airflow/sensors/smart_sensor.py", line 681, in flush_cached_sensor_poke_results
    for ti_key, sensor_exception in self.cached_sensor_exceptions.items():
```


**What you expected to happen**:

<!-- What do you think went wrong? -->
Smart sensor should always execute without any runtime error.

**How to reproduce it**:
I haven't been able to reproduce it consistently since it sometimes works and sometimes errors.

**Anything else we need to know**:
It's a really noisy error in Sentry. In just 4 days, 3.8k events were reported in Sentry.
<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py']
Ground Truth : ['a/airflow/sensors/smart_sensor.py']
Current Recall: 0.0

=========================================================

ISSUE: WASB remote logging too verbose in task logger
**Apache Airflow version**: 2.1.0

**Environment**:
apache-airflow-providers-microsoft-azure == 2.0.0

**What happened**:

When wasb remote logger wants to create container (always), the log is part of the task logger.

```
[2021-06-02 13:55:51,619] {wasb.py:385} INFO - Attempting to create container:  xxxxx
[2021-06-02 13:55:51,479] {_universal.py:419} INFO - Request URL: 'XXXXXXX'
[2021-06-02 13:55:51,483] {_universal.py:420} INFO - Request method: 'HEAD'
[2021-06-02 13:55:51,490] {_universal.py:421} INFO - Request headers:
[2021-06-02 13:55:51,495] {_universal.py:424} INFO -     'x-ms-version': 'REDACTED'
[2021-06-02 13:55:51,499] {_universal.py:424} INFO -     'Accept': 'application/xml'
[2021-06-02 13:55:51,507] {_universal.py:424} INFO -     'User-Agent': 'azsdk-python-storage-blob/12.8.1 Python/3.7.10 (Linux-5.4.0-1046-azure-x86_64-with-debian-10.9)'
[2021-06-02 13:55:51,511] {_universal.py:424} INFO -     'x-ms-date': 'REDACTED'
[2021-06-02 13:55:51,517] {_universal.py:424} INFO -     'x-ms-client-request-id': ''
[2021-06-02 13:55:51,523] {_universal.py:424} INFO -     'Authorization': 'REDACTED'
[2021-06-02 13:55:51,529] {_universal.py:437} INFO - No body was attached to the request
[2021-06-02 13:55:51,541] {_universal.py:452} INFO - Response status: 404
[2021-06-02 13:55:51,550] {_universal.py:453} INFO - Response headers:
[2021-06-02 13:55:51,556] {_universal.py:456} INFO -     'Transfer-Encoding': 'chunked'
[2021-06-02 13:55:51,561] {_universal.py:456} INFO -     'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'
[2021-06-02 13:55:51,567] {_universal.py:456} INFO -     'x-ms-request-id': 'xxxx'
[2021-06-02 13:55:51,573] {_universal.py:456} INFO -     'x-ms-client-request-id': 'xxxx'
[2021-06-02 13:55:51,578] {_universal.py:456} INFO -     'x-ms-version': 'REDACTED'
[2021-06-02 13:55:51,607] {_universal.py:456} INFO -     'x-ms-error-code': 'REDACTED'
[2021-06-02 13:55:51,613] {_universal.py:456} INFO -     'Date': 'Wed, 02 Jun 2021 13:55:50 GMT'
```
**What you expected to happen**:
HTTP verbose logging not showing and "Attempting to create container" maybe it's not useful in the task log


ref: https://github.com/Azure/azure-sdk-for-python/issues/9422


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_renderedtifields.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/macros/test_macros.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_error_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/utils/test_task_log_fetcher.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/googleapis/google-cloud-python/packages/google-maps-places/google/maps/places_v1/types/place.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/googleapis/google-cloud-python/packages/google-analytics-data/google/analytics/data_v1alpha/types/data.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/googleapis/google-cloud-python/packages/google-cloud-securitycenter/google/cloud/securitycenter_v1/types/mitre_attack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/googleapis/google-cloud-python/packages/google-cloud-securitycenter/google/cloud/securitycenter_v2/types/mitre_attack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/telegram/operators/test_telegram.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/googleapis/google-cloud-python/packages/google-cloud-language/google/cloud/language_v1beta2/types/language_service.py']
Ground Truth : ['a/airflow/providers/microsoft/azure/hooks/wasb.py']
Current Recall: 0.0

=========================================================

ISSUE: CloudWatch task handler doesn't fall back to local logs when Amazon CloudWatch logs aren't found
This is really a CloudWatch handler issue - not "airflow" core.

### Discussed in https://github.com/apache/airflow/discussions/27395

<div type='discussions-op-text'>

<sup>Originally posted by **matthewblock** October 24, 2022</sup>
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

We recently activated AWS Cloudwatch logs. We were hoping the logs server would gracefully handle task logs that previously existed but were not written to Cloudwatch, but when fetching the remote logs failed (expected), the logs server didn't fall back to local logs.

```
*** Reading remote log from Cloudwatch log_group: <our log group> log_stream: <our log stream>
```

### What you think should happen instead

According to documentation [Logging for Tasks](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-tasks.html#writing-logs-locally), when fetching remote logs fails, the logs server should fall back to looking for local logs:

> In the Airflow UI, remote logs take precedence over local logs when remote logging is enabled. If remote logs can not be found or accessed, local logs will be displayed.

This should be indicated by the message `*** Falling back to local log`.

If this is not the intended behavior, the documentation should be modified to reflect the intended behavior.

### How to reproduce

1. Run a test DAG without [AWS CloudWatch logging configured](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/cloud-watch-task-handlers.html)
2. Configure AWS CloudWatch remote logging and re-run a test DAG

### Operating System

Debian buster-slim

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/airflow_local_settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/batch.py']
Ground Truth : ['a/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: No Mass Delete Option for Task Instances Similar to What DAGRuns Have in UI
### Apache Airflow version

2.1.3 (latest released)

### Operating System

macOS Big Sur 11.3.1

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

default Astronomer deployment with some test DAGs

### What happened

In the UI for DAGRuns there are checkboxes that allow multiple DAGRuns to be selected.

![image](https://user-images.githubusercontent.com/89415310/133841290-94cda771-9bb3-4677-8530-8c2861525719.png)

Within the Actions menu on this same view, there is a Delete option which allows multiple DAGRuns to be deleted at the same time.

![image](https://user-images.githubusercontent.com/89415310/133841362-3c7fc81d-a823-4f41-8b1d-64be425810ce.png)

Task instance view on the UI does not offer the same option, even though Task Instances can be individually deleted with the trash can button.

![image](https://user-images.githubusercontent.com/89415310/133841442-3fee0930-a6bb-4035-b172-1d10b85f3bf6.png)


### What you expected to happen

I expect that the Task Instances can also be bulk deleted, in the same way that DAGRuns can be.

### How to reproduce

Open up Task Instance and DAGRun views from the Browse tab and compare the options in the Actions dropdown menus.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : []
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.0

=========================================================

ISSUE: [Smart sensor] Runtime error: dictionary changed size during iteration
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->


**What happened**:

<!-- (please include exact error messages if you can) -->
Smart Sensor TI crashes with a Runtime error. Here's the logs:
```
RuntimeError: dictionary changed size during iteration
  File "airflow/sentry.py", line 159, in wrapper
    return func(task_instance, *args, session=session, **kwargs)
  File "airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "airflow/sensors/smart_sensor.py", line 736, in execute
    self.flush_cached_sensor_poke_results()
  File "airflow/sensors/smart_sensor.py", line 681, in flush_cached_sensor_poke_results
    for ti_key, sensor_exception in self.cached_sensor_exceptions.items():
```


**What you expected to happen**:

<!-- What do you think went wrong? -->
Smart sensor should always execute without any runtime error.

**How to reproduce it**:
I haven't been able to reproduce it consistently since it sometimes works and sometimes errors.

**Anything else we need to know**:
It's a really noisy error in Sentry. In just 4 days, 3.8k events were reported in Sentry.
<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : []
Ground Truth : ['a/airflow/sensors/smart_sensor.py']
Current Recall: 0.0

=========================================================

ISSUE: WASB remote logging too verbose in task logger
**Apache Airflow version**: 2.1.0

**Environment**:
apache-airflow-providers-microsoft-azure == 2.0.0

**What happened**:

When wasb remote logger wants to create container (always), the log is part of the task logger.

```
[2021-06-02 13:55:51,619] {wasb.py:385} INFO - Attempting to create container:  xxxxx
[2021-06-02 13:55:51,479] {_universal.py:419} INFO - Request URL: 'XXXXXXX'
[2021-06-02 13:55:51,483] {_universal.py:420} INFO - Request method: 'HEAD'
[2021-06-02 13:55:51,490] {_universal.py:421} INFO - Request headers:
[2021-06-02 13:55:51,495] {_universal.py:424} INFO -     'x-ms-version': 'REDACTED'
[2021-06-02 13:55:51,499] {_universal.py:424} INFO -     'Accept': 'application/xml'
[2021-06-02 13:55:51,507] {_universal.py:424} INFO -     'User-Agent': 'azsdk-python-storage-blob/12.8.1 Python/3.7.10 (Linux-5.4.0-1046-azure-x86_64-with-debian-10.9)'
[2021-06-02 13:55:51,511] {_universal.py:424} INFO -     'x-ms-date': 'REDACTED'
[2021-06-02 13:55:51,517] {_universal.py:424} INFO -     'x-ms-client-request-id': ''
[2021-06-02 13:55:51,523] {_universal.py:424} INFO -     'Authorization': 'REDACTED'
[2021-06-02 13:55:51,529] {_universal.py:437} INFO - No body was attached to the request
[2021-06-02 13:55:51,541] {_universal.py:452} INFO - Response status: 404
[2021-06-02 13:55:51,550] {_universal.py:453} INFO - Response headers:
[2021-06-02 13:55:51,556] {_universal.py:456} INFO -     'Transfer-Encoding': 'chunked'
[2021-06-02 13:55:51,561] {_universal.py:456} INFO -     'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'
[2021-06-02 13:55:51,567] {_universal.py:456} INFO -     'x-ms-request-id': 'xxxx'
[2021-06-02 13:55:51,573] {_universal.py:456} INFO -     'x-ms-client-request-id': 'xxxx'
[2021-06-02 13:55:51,578] {_universal.py:456} INFO -     'x-ms-version': 'REDACTED'
[2021-06-02 13:55:51,607] {_universal.py:456} INFO -     'x-ms-error-code': 'REDACTED'
[2021-06-02 13:55:51,613] {_universal.py:456} INFO -     'Date': 'Wed, 02 Jun 2021 13:55:50 GMT'
```
**What you expected to happen**:
HTTP verbose logging not showing and "Attempting to create container" maybe it's not useful in the task log


ref: https://github.com/Azure/azure-sdk-for-python/issues/9422


Retrieved_files : []
Ground Truth : ['a/airflow/providers/microsoft/azure/hooks/wasb.py']
Current Recall: 0.0

=========================================================

ISSUE: CloudWatch task handler doesn't fall back to local logs when Amazon CloudWatch logs aren't found
This is really a CloudWatch handler issue - not "airflow" core.

### Discussed in https://github.com/apache/airflow/discussions/27395

<div type='discussions-op-text'>

<sup>Originally posted by **matthewblock** October 24, 2022</sup>
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

We recently activated AWS Cloudwatch logs. We were hoping the logs server would gracefully handle task logs that previously existed but were not written to Cloudwatch, but when fetching the remote logs failed (expected), the logs server didn't fall back to local logs.

```
*** Reading remote log from Cloudwatch log_group: <our log group> log_stream: <our log stream>
```

### What you think should happen instead

According to documentation [Logging for Tasks](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-tasks.html#writing-logs-locally), when fetching remote logs fails, the logs server should fall back to looking for local logs:

> In the Airflow UI, remote logs take precedence over local logs when remote logging is enabled. If remote logs can not be found or accessed, local logs will be displayed.

This should be indicated by the message `*** Falling back to local log`.

If this is not the intended behavior, the documentation should be modified to reflect the intended behavior.

### How to reproduce

1. Run a test DAG without [AWS CloudWatch logging configured](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/cloud-watch-task-handlers.html)
2. Configure AWS CloudWatch remote logging and re-run a test DAG

### Operating System

Debian buster-slim

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>

Retrieved_files : []
Ground Truth : ['a/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py']
Current Recall: 0.0

=========================================================

ISSUE: No Mass Delete Option for Task Instances Similar to What DAGRuns Have in UI
### Apache Airflow version

2.1.3 (latest released)

### Operating System

macOS Big Sur 11.3.1

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

default Astronomer deployment with some test DAGs

### What happened

In the UI for DAGRuns there are checkboxes that allow multiple DAGRuns to be selected.

![image](https://user-images.githubusercontent.com/89415310/133841290-94cda771-9bb3-4677-8530-8c2861525719.png)

Within the Actions menu on this same view, there is a Delete option which allows multiple DAGRuns to be deleted at the same time.

![image](https://user-images.githubusercontent.com/89415310/133841362-3c7fc81d-a823-4f41-8b1d-64be425810ce.png)

Task instance view on the UI does not offer the same option, even though Task Instances can be individually deleted with the trash can button.

![image](https://user-images.githubusercontent.com/89415310/133841442-3fee0930-a6bb-4035-b172-1d10b85f3bf6.png)


### What you expected to happen

I expect that the Task Instances can also be bulk deleted, in the same way that DAGRuns can be.

### How to reproduce

Open up Task Instance and DAGRun views from the Browse tab and compare the options in the Actions dropdown menus.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.0

=========================================================

ISSUE: No Mass Delete Option for Task Instances Similar to What DAGRuns Have in UI
### Apache Airflow version

2.1.3 (latest released)

### Operating System

macOS Big Sur 11.3.1

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

default Astronomer deployment with some test DAGs

### What happened

In the UI for DAGRuns there are checkboxes that allow multiple DAGRuns to be selected.

![image](https://user-images.githubusercontent.com/89415310/133841290-94cda771-9bb3-4677-8530-8c2861525719.png)

Within the Actions menu on this same view, there is a Delete option which allows multiple DAGRuns to be deleted at the same time.

![image](https://user-images.githubusercontent.com/89415310/133841362-3c7fc81d-a823-4f41-8b1d-64be425810ce.png)

Task instance view on the UI does not offer the same option, even though Task Instances can be individually deleted with the trash can button.

![image](https://user-images.githubusercontent.com/89415310/133841442-3fee0930-a6bb-4035-b172-1d10b85f3bf6.png)


### What you expected to happen

I expect that the Task Instances can also be bulk deleted, in the same way that DAGRuns can be.

### How to reproduce

Open up Task Instance and DAGRun views from the Browse tab and compare the options in the Actions dropdown menus.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.0

=========================================================

ISSUE: [Smart sensor] Runtime error: dictionary changed size during iteration
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->


**What happened**:

<!-- (please include exact error messages if you can) -->
Smart Sensor TI crashes with a Runtime error. Here's the logs:
```
RuntimeError: dictionary changed size during iteration
  File "airflow/sentry.py", line 159, in wrapper
    return func(task_instance, *args, session=session, **kwargs)
  File "airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "airflow/sensors/smart_sensor.py", line 736, in execute
    self.flush_cached_sensor_poke_results()
  File "airflow/sensors/smart_sensor.py", line 681, in flush_cached_sensor_poke_results
    for ti_key, sensor_exception in self.cached_sensor_exceptions.items():
```


**What you expected to happen**:

<!-- What do you think went wrong? -->
Smart sensor should always execute without any runtime error.

**How to reproduce it**:
I haven't been able to reproduce it consistently since it sometimes works and sometimes errors.

**Anything else we need to know**:
It's a really noisy error in Sentry. In just 4 days, 3.8k events were reported in Sentry.
<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py']
Ground Truth : ['a/airflow/sensors/smart_sensor.py']
Current Recall: 0.0

=========================================================

ISSUE: WASB remote logging too verbose in task logger
**Apache Airflow version**: 2.1.0

**Environment**:
apache-airflow-providers-microsoft-azure == 2.0.0

**What happened**:

When wasb remote logger wants to create container (always), the log is part of the task logger.

```
[2021-06-02 13:55:51,619] {wasb.py:385} INFO - Attempting to create container:  xxxxx
[2021-06-02 13:55:51,479] {_universal.py:419} INFO - Request URL: 'XXXXXXX'
[2021-06-02 13:55:51,483] {_universal.py:420} INFO - Request method: 'HEAD'
[2021-06-02 13:55:51,490] {_universal.py:421} INFO - Request headers:
[2021-06-02 13:55:51,495] {_universal.py:424} INFO -     'x-ms-version': 'REDACTED'
[2021-06-02 13:55:51,499] {_universal.py:424} INFO -     'Accept': 'application/xml'
[2021-06-02 13:55:51,507] {_universal.py:424} INFO -     'User-Agent': 'azsdk-python-storage-blob/12.8.1 Python/3.7.10 (Linux-5.4.0-1046-azure-x86_64-with-debian-10.9)'
[2021-06-02 13:55:51,511] {_universal.py:424} INFO -     'x-ms-date': 'REDACTED'
[2021-06-02 13:55:51,517] {_universal.py:424} INFO -     'x-ms-client-request-id': ''
[2021-06-02 13:55:51,523] {_universal.py:424} INFO -     'Authorization': 'REDACTED'
[2021-06-02 13:55:51,529] {_universal.py:437} INFO - No body was attached to the request
[2021-06-02 13:55:51,541] {_universal.py:452} INFO - Response status: 404
[2021-06-02 13:55:51,550] {_universal.py:453} INFO - Response headers:
[2021-06-02 13:55:51,556] {_universal.py:456} INFO -     'Transfer-Encoding': 'chunked'
[2021-06-02 13:55:51,561] {_universal.py:456} INFO -     'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'
[2021-06-02 13:55:51,567] {_universal.py:456} INFO -     'x-ms-request-id': 'xxxx'
[2021-06-02 13:55:51,573] {_universal.py:456} INFO -     'x-ms-client-request-id': 'xxxx'
[2021-06-02 13:55:51,578] {_universal.py:456} INFO -     'x-ms-version': 'REDACTED'
[2021-06-02 13:55:51,607] {_universal.py:456} INFO -     'x-ms-error-code': 'REDACTED'
[2021-06-02 13:55:51,613] {_universal.py:456} INFO -     'Date': 'Wed, 02 Jun 2021 13:55:50 GMT'
```
**What you expected to happen**:
HTTP verbose logging not showing and "Attempting to create container" maybe it's not useful in the task log


ref: https://github.com/Azure/azure-sdk-for-python/issues/9422


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/macros/test_macros.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_renderedtifields.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_error_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/parallel.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/utils/test_task_log_fetcher.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_health_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py']
Ground Truth : ['a/airflow/providers/microsoft/azure/hooks/wasb.py']
Current Recall: 0.0

=========================================================

ISSUE: CloudWatch task handler doesn't fall back to local logs when Amazon CloudWatch logs aren't found
This is really a CloudWatch handler issue - not "airflow" core.

### Discussed in https://github.com/apache/airflow/discussions/27395

<div type='discussions-op-text'>

<sup>Originally posted by **matthewblock** October 24, 2022</sup>
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

We recently activated AWS Cloudwatch logs. We were hoping the logs server would gracefully handle task logs that previously existed but were not written to Cloudwatch, but when fetching the remote logs failed (expected), the logs server didn't fall back to local logs.

```
*** Reading remote log from Cloudwatch log_group: <our log group> log_stream: <our log stream>
```

### What you think should happen instead

According to documentation [Logging for Tasks](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/logging-tasks.html#writing-logs-locally), when fetching remote logs fails, the logs server should fall back to looking for local logs:

> In the Airflow UI, remote logs take precedence over local logs when remote logging is enabled. If remote logs can not be found or accessed, local logs will be displayed.

This should be indicated by the message `*** Falling back to local log`.

If this is not the intended behavior, the documentation should be modified to reflect the intended behavior.

### How to reproduce

1. Run a test DAG without [AWS CloudWatch logging configured](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/cloud-watch-task-handlers.html)
2. Configure AWS CloudWatch remote logging and re-run a test DAG

### Operating System

Debian buster-slim

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/airflow_local_settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/alibaba/cloud/log/oss_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/s3_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: Use private docker repository with K8S operator and XCOM sidecar container
**Use private docker repository with K8S operator and XCOM sidecar container**

An extra parameter to KubernetesPodOperator: docker_repository, this allows to specify the repository where the sidecar container is located

**My company force docker proxy usage for K8S**

I need to use my company docker repository, images that are not proxifed by the company docker repository are not allowed


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/testing_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/container_instances.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/xcom_sidecar.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/main_command.py']
Ground Truth : ['a/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py', 'a/airflow/providers/cncf/kubernetes/hooks/kubernetes.py', 'a/airflow/providers/cncf/kubernetes/utils/xcom_sidecar.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: Tree view is slow for recombining DAGs
The Tree View has a very cool approach to representing graphs that aren't actually trees by letting users collapse and uncollapse leaves that appear in multiple places. Unfortunately, this approach comes with an extreme computational burden since before it can display the tree, it must build a data object containing _every possible path from the root to every single leaf_. When DAGs recombine (meaning if it the edges weren't directed, it would be a cyclic graph), the number of possible paths explodes. For example, my machine totally freezes when trying to load Tree View for this relatively simple graph:

``` python
import airflow
from airflow.operators import DummyOperator
import datetime

dag = airflow.DAG(
    dag_id='break_the_tree',
    default_args=dict(owner='airflow' , start_date=datetime.datetime.now()))

group, prev_group = None, None

for i in range(5):
    ops = [DummyOperator(task_id='dummy{}{}'.format(i, j), dag=dag) for j in range(10)]
    if prev_group:
        prev_group.set_downstream(ops)
    group = DummyOperator(task_id='group{}'.format(i), dag=dag)
    group.set_upstream(ops)
    prev_group = group
```

![screen shot 2015-08-24 at 6 33 37 pm](https://cloud.githubusercontent.com/assets/153965/9454271/ad310714-4a8e-11e5-92b9-aff6ebc62539.png)

However, if we are willing to abandon the collapse/uncollapse functionality, then we could implement a much more efficient traversal algorithm. I did, and now I can generate this Tree View for the above graph almost instantly:
![screen shot 2015-08-25 at 7 50 53 am](https://cloud.githubusercontent.com/assets/153965/9465845/26e90402-4afe-11e5-8dd0-3b14080c936d.png)

I really like the collapse/expand functionality, but I think it's much more important to be able to see the tree than worrying that I'm going to freeze my server by clicking the wrong button. Maybe there's a way to have a checkbox on the Tree View page to turn on the fancy collapse functionality, if the user wants it? I could submit a PR that checks an argument `expand_collapse` and runs the efficient search when it's `False` and the old way if it's `True`, but I don't know how to build the UI piece.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/exampleinclude.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/dags/elastic_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/www/app.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: Create `BaseOperator.map` and basic mapping framework
We need to create a `map()` method on BaseOperator instance that returns a new object that looks somewhat like BaseOperator, but that gives us a place to add the DAG-authoring mapping API, and that the serializer can understand.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/context.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jenkins/operators/jenkins_job_trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/operator_helpers.py']
Ground Truth : ['a/airflow/utils/task_group.py', 'a/airflow/models/skipmixin.py', 'a/airflow/decorators/base.py', 'a/airflow/models/baseoperator.py', 'a/airflow/models/taskmixin.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/models/xcom_arg.py', 'a/airflow/decorators/task_group.py', 'a/airflow/models/dag.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: Airflow 2.1.0 Oauth for google Too Many Redirects b/c Google User does not have Role
The issue is similar to this ticket [16587](https://github.com/apache/airflow/issues/16587)  and [14829](https://github.com/apache/airflow/issues/14829) however I have an updated airflow version AND updated packages than the ones suggested here and I am still getting the same outcome. When using google auth in airflow and attempting to sign in, we get an ERR_TOO_MANY_REDIRECTS.  I know what causes the symptom of this,  but hoping to find a resolution of keeping a Role in place to avoid the REDIRECTS.

- **Apache Airflow version**:
Version: v2.1.0
Git Version: .release:2.1.0+304e174674ff6921cb7ed79c0158949b50eff8fe

- **Kubernetes version (if you are using kubernetes)** (use `kubectl version`):
Client Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.7", GitCommit:"1dd5338295409edcfff11505e7bb246f0d325d15", GitTreeState:"clean", BuildDate:"2021-01-13T13:23:52Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"19+", GitVersion:"v1.19.10-gke.1600", GitCommit:"7b8e568a7fb4c9d199c2ba29a5f7d76f6b4341c2", GitTreeState:"clean", BuildDate:"2021-05-07T09:18:53Z", GoVersion:"go1.15.10b5", Compiler:"gc", Platform:"linux/amd64"}

- **Environment**: Staging

- **Cloud provider or hardware configuration**: GKE on 

- **OS** (e.g. from /etc/os-release):
PRETTY_NAME="Debian GNU/Linux 10 (buster)"
NAME="Debian GNU/Linux"
VERSION_ID="10"
VERSION="10 (buster)"
VERSION_CODENAME=buster
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
- **Kernel** (e.g. `uname -a`):
 Linux margins-scheduler-97b6fb867-fth8p 5.4.89+ #1 SMP Sat Feb 13 19:45:14 PST 2021 x86_64 GNU/Linux
- **Install tools**: pip freeze below

**What happened**:
When using google auth in airflow and attempting to sign in, we get an ERR_TOO_MANY_REDIRECTS. 

**What you expected to happen**:
I expect to log in as my user and it assigns a default Role of Viewer at the very least OR uses our mappings in web_server config python file. But the Role is blank in Database.

<!-- What do you think went wrong? -->
We realized that we get stuck in the loop, b/c the user will be in the users table in airflow but without a Role (its literally empty). Therefore it goes from the /login to /home to /login to /home over and over again.

**How to reproduce it**:

I add the Admin role in the database for my user, and  the page that has the redirects  refreshes and lets me in to the Airflow UI. However, when I sign out and signin in again, my users Role is then erased and it starts the redirect cycle again.

As you can see there is no Role (this happens when I attempt to login)
```
id | username                     | email                   | first_name | last_name | roles
===+==============================+=========================+============+===========+======
1  | admin                        | admin@example.com       | admin      | admin     | Admin
2  | google_############ | msaenz@company.com | Cat     | Says     | 
```
I run the command: `airflow users add-role -r Admin -u google_#################`

Then the page takes me to the UI and the table now looks like this:
```
id | username                     | email                   | first_name | last_name | roles
===+==============================+=========================+============+===========+======
1  | admin                        | admin@example.com       | admin      | admin     | Admin
2  | google_############ | msaenz@company.com | Cat     | Says     |  Admin
```

How often does this problem occur? Once? Every time etc?  This occurs all the time

Here is the webserver_config.py
```
  import os
      from flask_appbuilder.security.manager import AUTH_OAUTH
      AUTH_TYPE = AUTH_OAUTH
      AUTH_ROLE_ADMIN="Admin"
      AUTH_USER_REGISTRATION = False
      AUTH_USER_REGISTRATION_ROLE = "Admin"
      OIDC_COOKIE_SECURE = False
      CSRF_ENABLED = False
      WTF_CSRF_ENABLED = True
      AUTH_ROLES_MAPPING = {"Engineering": ["Ops"],"Admins": ["Admin"]}
      AUTH_ROLES_SYNC_AT_LOGIN = True
      OAUTH_PROVIDERS = [
          {
              'name': 'google', 'icon': 'fa-google',
              'token_key': 'access_token',
              'remote_app': {
                  'client_id': '#####################.apps.googleusercontent.com',
                  'client_secret': '######################',
                  'api_base_url': 'https://www.googleapis.com/oauth2/v2/',
                  'whitelist': ['@company.com'],  # optional
                  'client_kwargs': {
                      'scope': 'email profile'
                  },
                  'request_token_url': None,
                  'access_token_url': 'https://accounts.google.com/o/oauth2/token',
                  'authorize_url': 'https://accounts.google.com/o/oauth2/auth'},
          }
      ]


```
Here is the pip freeze:
```
adal==1.2.7
alembic==1.6.2
amqp==2.6.1
anyio==3.2.1
apache-airflow==2.1.0
apache-airflow-providers-amazon==1.4.0
apache-airflow-providers-celery==1.0.1
apache-airflow-providers-cncf-kubernetes==1.2.0
apache-airflow-providers-docker==1.2.0
apache-airflow-providers-elasticsearch==1.0.4
apache-airflow-providers-ftp==1.1.0
apache-airflow-providers-google==3.0.0
apache-airflow-providers-grpc==1.1.0
apache-airflow-providers-hashicorp==1.0.2
apache-airflow-providers-http==1.1.1
apache-airflow-providers-imap==1.0.1
apache-airflow-providers-microsoft-azure==2.0.0
apache-airflow-providers-mysql==1.1.0
apache-airflow-providers-postgres==1.0.2
apache-airflow-providers-redis==1.0.1
apache-airflow-providers-sendgrid==1.0.2
apache-airflow-providers-sftp==1.2.0
apache-airflow-providers-slack==3.0.0
apache-airflow-providers-sqlite==1.0.2
apache-airflow-providers-ssh==1.3.0
apispec==3.3.2
appdirs==1.4.4
argcomplete==1.12.3
async-generator==1.10
attrs==20.3.0
azure-batch==10.0.0
azure-common==1.1.27
azure-core==1.13.0
azure-cosmos==3.2.0
azure-datalake-store==0.0.52
azure-identity==1.5.0
azure-keyvault==4.1.0
azure-keyvault-certificates==4.2.1
azure-keyvault-keys==4.3.1
azure-keyvault-secrets==4.2.0
azure-kusto-data==0.0.45
azure-mgmt-containerinstance==1.5.0
azure-mgmt-core==1.2.2
azure-mgmt-datafactory==1.1.0
azure-mgmt-datalake-nspkg==3.0.1
azure-mgmt-datalake-store==0.5.0
azure-mgmt-nspkg==3.0.2
azure-mgmt-resource==16.1.0
azure-nspkg==3.0.2
azure-storage-blob==12.8.1
azure-storage-common==2.1.0
azure-storage-file==2.1.0
Babel==2.9.1
bcrypt==3.2.0
billiard==3.6.4.0
blinker==1.4
boto3==1.17.71
botocore==1.20.71
cached-property==1.5.2
cachetools==4.2.2
cattrs==1.0.0
celery==4.4.7
certifi==2020.12.5
cffi==1.14.5
chardet==3.0.4
click==7.1.2
clickclick==20.10.2
cloudpickle==1.4.1
colorama==0.4.4
colorlog==5.0.1
commonmark==0.9.1
contextvars==2.4
croniter==1.0.13
cryptography==3.4.7
dask==2021.3.0
dataclasses==0.7
defusedxml==0.7.1
dill==0.3.1.1
distlib==0.3.1
distributed==2.19.0
dnspython==1.16.0
docker==3.7.3
docker-pycreds==0.4.0
docutils==0.17.1
elasticsearch==7.5.1
elasticsearch-dbapi==0.1.0
elasticsearch-dsl==7.3.0
email-validator==1.1.2
eventlet==0.31.0
filelock==3.0.12
Flask==1.1.2
Flask-AppBuilder==3.3.0
Flask-Babel==1.0.0
Flask-Caching==1.10.1
Flask-JWT-Extended==3.25.1
Flask-Login==0.4.1
Flask-OpenID==1.2.5
Flask-SQLAlchemy==2.5.1
Flask-WTF==0.14.3
flower==0.9.7
gevent==21.1.2
google-ads==4.0.0
google-api-core==1.26.3
google-api-python-client==1.12.8
google-auth==1.30.0
google-auth-httplib2==0.1.0
google-auth-oauthlib==0.4.4
google-cloud-automl==2.3.0
google-cloud-bigquery==2.16.0
google-cloud-bigquery-datatransfer==3.1.1
google-cloud-bigquery-storage==2.4.0
google-cloud-bigtable==1.7.0
google-cloud-container==1.0.1
google-cloud-core==1.6.0
google-cloud-datacatalog==3.1.1
google-cloud-dataproc==2.3.1
google-cloud-dlp==1.0.0
google-cloud-kms==2.2.0
google-cloud-language==1.3.0
google-cloud-logging==2.3.1
google-cloud-memcache==0.3.0
google-cloud-monitoring==2.2.1
google-cloud-os-login==2.1.0
google-cloud-pubsub==2.4.2
google-cloud-redis==2.1.0
google-cloud-secret-manager==1.0.0
google-cloud-spanner==1.19.1
google-cloud-speech==1.3.2
google-cloud-storage==1.38.0
google-cloud-tasks==2.2.0
google-cloud-texttospeech==1.0.1
google-cloud-translate==1.7.0
google-cloud-videointelligence==1.16.1
google-cloud-vision==1.0.0
google-cloud-workflows==0.3.0
google-crc32c==1.1.2
google-resumable-media==1.2.0
googleapis-common-protos==1.53.0
graphviz==0.16
greenlet==1.1.0
grpc-google-iam-v1==0.12.3
grpcio==1.37.1
grpcio-gcp==0.2.2
gunicorn==20.1.0
h11==0.12.0
HeapDict==1.0.1
httpcore==0.13.6
httplib2==0.17.4
httpx==0.18.2
humanize==3.5.0
hvac==0.10.11
idna==2.10
immutables==0.15
importlib-metadata==1.7.0
importlib-resources==1.5.0
inflection==0.5.1
iso8601==0.1.14
isodate==0.6.0
itsdangerous==1.1.0
Jinja2==2.11.3
jmespath==0.10.0
json-merge-patch==0.2
jsonschema==3.2.0
kombu==4.6.11
kubernetes==11.0.0
lazy-object-proxy==1.4.3
ldap3==2.9
libcst==0.3.18
lockfile==0.12.2
Mako==1.1.4
Markdown==3.3.4
MarkupSafe==1.1.1
marshmallow==3.12.1
marshmallow-enum==1.5.1
marshmallow-oneofschema==2.1.0
marshmallow-sqlalchemy==0.23.1
msal==1.11.0
msal-extensions==0.3.0
msgpack==1.0.2
msrest==0.6.21
msrestazure==0.6.4
mypy-extensions==0.4.3
mysql-connector-python==8.0.22
mysqlclient==2.0.3
numpy==1.19.5
oauthlib==2.1.0
openapi-schema-validator==0.1.5
openapi-spec-validator==0.3.0
packaging==20.9
pandas==1.1.5
pandas-gbq==0.14.1
paramiko==2.7.2
pendulum==2.1.2
pep562==1.0
plyvel==1.3.0
portalocker==1.7.1
prison==0.1.3
prometheus-client==0.8.0
proto-plus==1.18.1
protobuf==3.16.0
psutil==5.8.0
psycopg2-binary==2.8.6
pyarrow==3.0.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycparser==2.20
pydata-google-auth==1.2.0
Pygments==2.9.0
PyJWT==1.7.1
PyNaCl==1.4.0
pyOpenSSL==19.1.0
pyparsing==2.4.7
pyrsistent==0.17.3
pysftp==0.2.9
python-daemon==2.3.0
python-dateutil==2.8.1
python-editor==1.0.4
python-http-client==3.3.2
python-ldap==3.3.1
python-nvd3==0.15.0
python-slugify==4.0.1
python3-openid==3.2.0
pytz==2021.1
pytzdata==2020.1
PyYAML==5.4.1
redis==3.5.3
requests==2.25.1
requests-oauthlib==1.1.0
rfc3986==1.5.0
rich==9.2.0
rsa==4.7.2
s3transfer==0.4.2
sendgrid==6.7.0
setproctitle==1.2.2
six==1.16.0
slack-sdk==3.5.1
sniffio==1.2.0
sortedcontainers==2.3.0
SQLAlchemy==1.3.24
SQLAlchemy-JSONField==1.0.0
SQLAlchemy-Utils==0.37.2
sshtunnel==0.1.5
starkbank-ecdsa==1.1.0
statsd==3.3.0
swagger-ui-bundle==0.0.8
tabulate==0.8.9
tblib==1.7.0
tenacity==6.2.0
termcolor==1.1.0
text-unidecode==1.3
toolz==0.11.1
tornado==6.1
typing==3.7.4.3
typing-extensions==3.7.4.3
typing-inspect==0.6.0
unicodecsv==0.14.1
uritemplate==3.0.1
urllib3==1.25.11
vine==1.3.0
virtualenv==20.4.6
watchtower==0.7.3
websocket-client==0.59.0
Werkzeug==1.0.1
WTForms==2.3.3
zict==2.0.0
zipp==3.4.1
zope.event==4.5.0
zope.interface==5.4.0

```

Thanks in advance.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/default_webserver_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_memorystore.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/credentials_provider.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/docs_build/third_party_inventories.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/www/auth.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: Latest SQLAlchemy (1.4) Incompatible with latest sqlalchemy_utils
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 2.0.1


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): N/A

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): Mac OS Big Sur
- **Kernel** (e.g. `uname -a`):
- **Install tools**: pip 20.1.1
- **Others**:

**What happened**:

Our CI environment broke due to the release of SQLAlchemy 1.4, which is incompatible with the latest version of sqlalchemy-utils. ([Related issue](https://github.com/kvesteri/sqlalchemy-utils/issues/505))

Partial stacktrace:
```
  File "/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/airflow/www/utils.py", line 27, in <module>
    from flask_appbuilder.models.sqla.interface import SQLAInterface
  File "/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/flask_appbuilder/models/sqla/interface.py", line 16, in <module>
    from sqlalchemy_utils.types.uuid import UUIDType
  File "/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/sqlalchemy_utils/__init__.py", line 1, in <module>
    from .aggregates import aggregated  # noqa
  File "/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/sqlalchemy_utils/aggregates.py", line 372, in <module>
    from .functions.orm import get_column_key
  File "/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/sqlalchemy_utils/functions/__init__.py", line 1, in <module>
    from .database import (  # noqa
  File "/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/sqlalchemy_utils/functions/database.py", line 11, in <module>
    from .orm import quote
  File "/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/sqlalchemy_utils/functions/orm.py", line 14, in <module>
    from sqlalchemy.orm.query import _ColumnEntity
ImportError: cannot import name '_ColumnEntity' from 'sqlalchemy.orm.query' (/Users/samwheating/Desktop/tmp_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py)
```

I'm not sure what the typical procedure is in the case of breaking changes to dependencies, but seeing as there's an upcoming release I thought it might be worth pinning sqlalchemy to 1.3.x? (Or pin the version of sqlalchemy-utils to a compatible version if one is released before Airflow 2.0.2)

**What you expected to happen**:

`airflow db init` to run successfully. 

<!-- What do you think went wrong? -->

**How to reproduce it**:

1) Create a new virtualenv
2) `pip install apache-airflow`
3) `airflow db init`

<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py']
Ground Truth : ['a/airflow/upgrade/rules/fix_conf_not_importable_from_airflow.py', 'a/setup.py', 'a/airflow/upgrade/version.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: Incorrect await_container_completion in KubernetesPodOperator
### Apache Airflow version

2.3.4

### What happened

The [await_container_completion](
https://github.com/apache/airflow/blob/2.4.0/airflow/providers/cncf/kubernetes/utils/pod_manager.py#L259) function has a negated condition

```
while not self.container_is_running(pod=pod, container_name=container_name):
```
that causes our Airflow tasks running <1 s to never be completed, causing an infinite loop. 

I see this was addressed and released in https://github.com/apache/airflow/pull/23883, but later reverted in https://github.com/apache/airflow/pull/24474. How come it was reverted? The thread on that revert PR with comments from @jedcunningham and @potiuk didn't really address why the fix was reverted.

### What you think should happen instead

Pods finishing within 1s should be properly handled.

### How to reproduce

_No response_

### Operating System

Linux

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==4.3.0

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

Incorrect await_container_completion in KubernetesPodOperator
### Apache Airflow version

2.3.4

### What happened

The [await_container_completion](
https://github.com/apache/airflow/blob/2.4.0/airflow/providers/cncf/kubernetes/utils/pod_manager.py#L259) function has a negated condition

```
while not self.container_is_running(pod=pod, container_name=container_name):
```
that causes our Airflow tasks running <1 s to never be completed, causing an infinite loop. 

I see this was addressed and released in https://github.com/apache/airflow/pull/23883, but later reverted in https://github.com/apache/airflow/pull/24474. How come it was reverted? The thread on that revert PR with comments from @jedcunningham and @potiuk didn't really address why the fix was reverted.

### What you think should happen instead

Pods finishing within 1s should be properly handled.

### How to reproduce

_No response_

### Operating System

Linux

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==4.3.0

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py']
Ground Truth : ['a/airflow/providers/cncf/kubernetes/utils/pod_manager.py']
Current Recall: 0.0021413276231263384

=========================================================

ISSUE: ExternalTaskSensor does not fail when failed_states is set along with a execution_date_fn
**Apache Airflow version**: 2.x including main

**What happened**:

I am using an `execution_date_fn` in an `ExternalTaskSensor` that also sets `allowed_states=['success']` and `failed_states=['failed']`. When one of the N upstream tasks fails, the sensor will hang forever in the `poke` method because there is a bug in checking for failed_states.

**What you expected to happen**:

I would expect the `ExternalTaskSensor` to fail.

I think this is due to a bug in the `poke` method where it should check if `count_failed > 0` as opposed to checking `count_failed == len(dttm_filter)`. I've created a fix locally that works for my case and have submitted a PR #16205 for it as reference.

**How to reproduce it**:

Create any `ExternalTaskSensor` that checks for `failed_states` and have one of the external DAGs tasks fail while others succeed.

E.g.
```
ExternalTaskSensor(
        task_id='check_external_dag',                                                     
        external_dag_id='external_dag',                                                               
        external_task_id=None,                              
        execution_date_fn=dependent_date_fn,
        allowed_states=['success'],
        failed_states=['failed'],                                                                     
        check_existence=True)
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_external_task_marker_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/triggers/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/triggers/test_external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/trigger_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/core/example_external_task_parent_deferrable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py']
Ground Truth : ['a/airflow/sensors/external_task.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: JdbcOperator should pass handler parameter to JdbcHook.run
### Apache Airflow version

2.2.0 (latest released)

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

apache-airflow-providers-jdbc==1!2.0.1

### Deployment

Astronomer

### Deployment details

Astro CLI Version: 0.25.4, Git Commit: e95011fbf800fda9fdef1dc1d5149d62bc017aed

### What happened

The execute method calls the [hook run method](https://github.com/apache/airflow/blob/20847fdbf8ecd3be394d24d47ce151c26d018ea1/airflow/providers/jdbc/operators/jdbc.py#L70) without passing any optional `handler` parameter to the DbApiHook parent class of JdbcHook.

### What you expected to happen

Without the handler, the results of the DbApiHook, and the JdbcOperator, [will always be an empty list](https://github.com/apache/airflow/blob/20847fdbf8ecd3be394d24d47ce151c26d018ea1/airflow/hooks/dbapi.py#L206). In the case of the JdbcOperator, the underlying `JayDeBeApi` connection uses a handler of the form `lambda x: x.fetchall()` to return results.

### How to reproduce

1. Using the astro cli
2. Download [postgresql-42.3.0.jar](https://jdbc.postgresql.org/download/postgresql-42.3.0.jar) into the working directory
3. Download [zulu11.52.13-ca-jdk11.0.13-linux_x64.tar.gz](https://cdn.azul.com/zulu/bin/zulu11.52.13-ca-jdk11.0.13-linux_x64.tar.gz) to the working directory
4. Copy the following to the Dockerfile
```
FROM quay.io/astronomer/ap-airflow:2.2.0-buster-onbuild

COPY postgresql-42.3.0.jar /usr/local/airflow/.

USER root
RUN cd /opt && mkdir java
COPY zulu11.52.13-ca-jdk11.0.13-linux_x64.tar.gz /opt/java
RUN cd /opt/java && pwd && ls && tar xfvz ./zulu11.52.13-ca-jdk11.0.13-linux_x64.tar.gz

ENV JAVA_HOME /opt/java/zulu11.52.13-ca-jdk11.0.13-linux_x64
RUN export JAVA_HOME
```
5.  Copy the following into the `airflow_settings.yaml` file
```yml
airflow:
  connections:
    - conn_id: my_jdbc_connection
      conn_type: jdbc
      conn_host: "jdbc:postgresql://postgres:5432/"
      conn_schema:
      conn_login: postgres
      conn_password: postgres
      conn_port: 
      conn_extra: '{"extra__jdbc__drv_clsname":"org.postgresql.Driver", "extra__jdbc__drv_path":"/usr/local/airflow/postgresql-42.3.0.jar"}'
```
6. Copy the following DAG and run it. No data will be passed to XCOM
```python
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.providers.jdbc.operators.jdbc import JdbcOperator

with DAG(
    dag_id='example_jdbc_operator',
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    dagrun_timeout=timedelta(minutes=60),
    tags=['example'],
    catchup=False,
) as dag:

    run_this_last = DummyOperator(task_id='run_this_last')

    query_dags_data = JdbcOperator(
        task_id='query_dags_data',
        sql='SELECT dag_id, is_active FROM dag',
        jdbc_conn_id='my_jdbc_connection',
        autocommit=True,
    )

    query_dags_data >> run_this_last 
```
7. Copy the following DAG and run it. All DAGs and their active status will be put onto XCOM
```python
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.providers.jdbc.operators.jdbc import JdbcOperator
from airflow.providers.jdbc.hooks.jdbc import JdbcHook


class JdbcHandlerOperator(JdbcOperator):
    def execute(self, context) -> None:
        self.log.info('Executing: %s', self.sql)
        hook = JdbcHook(jdbc_conn_id=self.jdbc_conn_id)
        return hook.run(
            self.sql,
            self.autocommit,
            parameters=self.parameters,
            # Defined by how JayDeBeApi operates 
            handler=lambda x: x.fetchall()
        )


with DAG(
    dag_id='example_jdbc_operator',
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    dagrun_timeout=timedelta(minutes=60),
    tags=['example'],
    catchup=False,
) as dag:

    run_this_last = DummyOperator(task_id='run_this_last')

    query_dags_data = JdbcHandlerOperator(
        task_id='query_dags_data',
        sql='SELECT dag_id, is_active FROM dag',
        jdbc_conn_id='my_jdbc_connection',
        autocommit=True,
    )

    query_dags_data >> run_this_last 
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/jdbc/example_jdbc_queries.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jdbc/hooks/jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/dataflow/example_dataflow_native_java.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/microsoft/winrm/example_winrm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/providers/jdbc/operators/jdbc.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: CORS access_control_allow_origin header never returned
### Apache Airflow version

2.2.2 (latest released)

### What happened

To fix CORS problem added the [access_control_allow_headers](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#access-control-allow-headers), [access_control_allow_methods](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#access-control-allow-methods), [access_control_allow_origins](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#access-control-allow-origins) variables to the 2.2.2 docker-compose file provided in documentation. Both header, and methods returns with the correct value, but origins never does.

### What you expected to happen

The CORS response returning with provided origin header value.

### How to reproduce

Download the latest docker-compose from documentation add the following lines:
`AIRFLOW__API__ACCESS_CONTROL_ALLOW_HEADERS: 'content-type, origin, authorization, accept'`
`AIRFLOW__API__ACCESS_CONTROL_ALLOW_METHODS: 'GET, POST, OPTIONS, DELETE'`
`AIRFLOW__API__ACCESS_CONTROL_ALLOW_ORIGINS: '*'`

run and call with a CORS preflight

### Operating System

Windows 11

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

It's repeatable regardless of ORIGINS value. There was a name change on this variable that's possibly not handled.
On 2.1.4 the same works without problems.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/test_cors.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py']
Ground Truth : ['a/airflow/www/extensions/init_views.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: Bytes cast to String in `airflow.providers.google.cloud.transfers.gcs_to_local.GCSToLocalFilesystemOperator` ~142
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

```
$ pip freeze | grep apache-airflow-providers
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==6.3.0
apache-airflow-providers-http==2.0.2
apache-airflow-providers-imap==2.1.0
apache-airflow-providers-pagerduty==2.1.0
apache-airflow-providers-sftp==2.4.0
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.3.0
```

### Apache Airflow version

2.2.3 (latest released)

### Operating System

Ubuntu 20.04.3 LTS

### Deployment

Composer

### Deployment details

_No response_

### What happened

Using `airflow.providers.google.cloud.transfers.gcs_to_local.GCSToLocalFilesystemOperator` to load the contents of a file into `xcom` unexpectedly casts the file bytes to string.

### What you expected to happen

`GCSToLocalFilesystemOperator` should not cast to string

### How to reproduce

Store a file on gcs;
```
Hello World!
```

Read file to xcom
```
my_task = GCSToLocalFilesystemOperator(
    task_id='my_task',
    bucket=bucket,
    object_name=object_path,
    store_to_xcom_key='my_xcom_key',
)
```

Access via jinja;
```
{{ ti.xcom_pull(task_ids="my_task", key="my_xcom_key") }}
```

XCom result is;
```
b'Hello World!'
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

Bytes cast to String in `airflow.providers.google.cloud.transfers.gcs_to_local.GCSToLocalFilesystemOperator` ~142
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

```
$ pip freeze | grep apache-airflow-providers
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==6.3.0
apache-airflow-providers-http==2.0.2
apache-airflow-providers-imap==2.1.0
apache-airflow-providers-pagerduty==2.1.0
apache-airflow-providers-sftp==2.4.0
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.3.0
```

### Apache Airflow version

2.2.3 (latest released)

### Operating System

Ubuntu 20.04.3 LTS

### Deployment

Composer

### Deployment details

_No response_

### What happened

Using `airflow.providers.google.cloud.transfers.gcs_to_local.GCSToLocalFilesystemOperator` to load the contents of a file into `xcom` unexpectedly casts the file bytes to string.

### What you expected to happen

`GCSToLocalFilesystemOperator` should not cast to string

### How to reproduce

Store a file on gcs;
```
Hello World!
```

Read file to xcom
```
my_task = GCSToLocalFilesystemOperator(
    task_id='my_task',
    bucket=bucket,
    object_name=object_path,
    store_to_xcom_key='my_xcom_key',
)
```

Access via jinja;
```
{{ ti.xcom_pull(task_ids="my_task", key="my_xcom_key") }}
```

XCom result is;
```
b'Hello World!'
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

Bytes cast to String in `airflow.providers.google.cloud.transfers.gcs_to_local.GCSToLocalFilesystemOperator` ~142
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

```
$ pip freeze | grep apache-airflow-providers
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==6.3.0
apache-airflow-providers-http==2.0.2
apache-airflow-providers-imap==2.1.0
apache-airflow-providers-pagerduty==2.1.0
apache-airflow-providers-sftp==2.4.0
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.3.0
```

### Apache Airflow version

2.2.3 (latest released)

### Operating System

Ubuntu 20.04.3 LTS

### Deployment

Composer

### Deployment details

_No response_

### What happened

Using `airflow.providers.google.cloud.transfers.gcs_to_local.GCSToLocalFilesystemOperator` to load the contents of a file into `xcom` unexpectedly casts the file bytes to string.

### What you expected to happen

`GCSToLocalFilesystemOperator` should not cast to string

### How to reproduce

Store a file on gcs;
```
Hello World!
```

Read file to xcom
```
my_task = GCSToLocalFilesystemOperator(
    task_id='my_task',
    bucket=bucket,
    object_name=object_path,
    store_to_xcom_key='my_xcom_key',
)
```

Access via jinja;
```
{{ ti.xcom_pull(task_ids="my_task", key="my_xcom_key") }}
```

XCom result is;
```
b'Hello World!'
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/secrets/secret_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/cloud_build/example_cloud_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/gcs_to_local.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: Version 2.3.3 breaks "Plugins as Python packages" feature
### Apache Airflow version

2.3.3 (latest released)

### What happened

In 2.3.3

If I use https://airflow.apache.org/docs/apache-airflow/stable/plugins.html#plugins-as-python-packages feature, then I see these Error:

short:
`ValueError: The name 'airs' is already registered for this blueprint. Use 'name=' to provide a unique name.`

long:
> i'm trying to reproduce it...

If I don't use it(workarounding by AIRFLOW__CORE__PLUGINS_FOLDER), errors doesn't occur.

It didn't happend in 2.3.2 and earlier 

### What you think should happen instead

Looks like plugins are import multiple times if it is plugins-as-python-packages.

Perhaps flask's major version change  is the main cause.
Presumably, in flask 1.0, duplicate registration of blueprint was quietly filtered out, but in 2.0 it seems to have been changed to generate an error. (I am trying to find out if this hypothesis is correct)

Anyway, use the latest version of FAB is important. we will have to adapt to this change, so plugins will have to be imported once regardless how it defined.

### How to reproduce

> It was reproduced in the environment used at work, but it is difficult to disclose or explain it.
> I'm working to reproduce it with the breeze command, and I open the issue first with the belief that it's not just me.

### Operating System

CentOS Linux release 7.9.2009 (Core)

### Versions of Apache Airflow Providers

```sh
$ SHIV_INTERPRETER=1 airsflow -m pip freeze | grep apache-
apache-airflow==2.3.3
apache-airflow-providers-apache-hive==3.1.0
apache-airflow-providers-apache-spark==2.1.0
apache-airflow-providers-celery==3.0.0
apache-airflow-providers-common-sql==1.0.0
apache-airflow-providers-ftp==3.1.0
apache-airflow-providers-http==3.0.0
apache-airflow-providers-imap==3.0.0
apache-airflow-providers-postgres==5.1.0
apache-airflow-providers-redis==3.0.0
apache-airflow-providers-sqlite==3.1.0
```

but I think these are irrelevant.

### Deployment

Other 3rd-party Helm chart

### Deployment details

docker image based on centos7, python 3.9.10 interpreter, self-written helm2 chart ....

... but I think these are irrelevant.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py']
Ground Truth : ['a/airflow/utils/entry_points.py', 'a/airflow/plugins_manager.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: Uploading List of Files to Google Cloud Storage
**Description**

In the [current implementation](https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/file_to_gcs/index.html) of `FileToGoogleCloudStorageOperator`, a single file is inputted to be uploaded to Google Cloud Storage bucket.

I'd like to implement functionality to allow input of a list of files, directory path or perhaps even a wildcard string (`/path/to/*.csv` for example).

**Related Issues**

Not that I can tell from searching, but there is this [stackoverflow question](https://stackoverflow.com/questions/58945050/uploading-local-directory-to-gcs-using-airflow) that @kaxil answered. 


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/local_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/samba/transfers/gcs_to_samba.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/sql_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/gcs_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/ads/transfers/ads_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/sftp_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/azure_fileshare_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/cassandra_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/operators/analytics.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/adls_to_gcs.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/local_to_gcs.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: Error when querying on the Browse view with empty date picker
**Apache Airflow version**: 2.0.2

**What happened**:

Under Browse, when querying with any empty datetime fields, I received the mushroom cloud.

```
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.7/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/security/decorators.py", line 109, in wraps
    return f(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/views.py", line 551, in list
    widgets = self._list()
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/baseviews.py", line 1127, in _list
    page_size=page_size,
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/baseviews.py", line 1026, in _get_list_widget
    page_size=page_size,
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/models/sqla/interface.py", line 425, in query
    count = self.query_count(query, filters, select_columns)
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/models/sqla/interface.py", line 347, in query_count
    query, filters, select_columns=select_columns, aliases_mapping={}
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/models/sqla/interface.py", line 332, in _apply_inner_all
    query = self.apply_filters(query, inner_filters)
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/models/sqla/interface.py", line 187, in apply_filters
    return filters.apply_all(query)
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/models/filters.py", line 298, in apply_all
    query = flt.apply(query, value)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/utils.py", line 373, in apply
    value = timezone.parse(value, timezone=timezone.utc)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/timezone.py", line 173, in parse
    return pendulum.parse(string, tz=timezone or TIMEZONE, strict=False)  # type: ignore
  File "/usr/local/lib/python3.7/site-packages/pendulum/parser.py", line 29, in parse
    return _parse(text, **options)
  File "/usr/local/lib/python3.7/site-packages/pendulum/parser.py", line 45, in _parse
    parsed = base_parse(text, **options)
  File "/usr/local/lib/python3.7/site-packages/pendulum/parsing/__init__.py", line 74, in parse
    return _normalize(_parse(text, **_options), **_options)
  File "/usr/local/lib/python3.7/site-packages/pendulum/parsing/__init__.py", line 120, in _parse
    return _parse_common(text, **options)
  File "/usr/local/lib/python3.7/site-packages/pendulum/parsing/__init__.py", line 177, in _parse_common
    return date(year, month, day)
ValueError: year 0 is out of range
```

**What you expected to happen**:

Perhaps give a warning/error banner that indicate Airflow cannot perform the search with bad input. I think it'll also work if the datetime picker defaults the timestamp to the current time.

It looks like some fields are equipped to do that but not all.

**How to reproduce it**:
1. Go under Browse
2. Try to query with empty datetime picket

**Anything else we need to know**:
![Screen Shot 2021-05-20 at 5 12 54 PM](https://user-images.githubusercontent.com/5952735/119063940-12b13f80-b98f-11eb-9b6f-a4d5c396e971.png)
![Screen Shot 2021-05-20 at 5 13 36 PM](https://user-images.githubusercontent.com/5952735/119063945-1349d600-b98f-11eb-91cd-92d813414eba.png)
![Screen Shot 2021-05-20 at 5 12 35 PM](https://user-images.githubusercontent.com/5952735/119063948-13e26c80-b98f-11eb-945f-1439a263fc58.png)
![Screen Shot 2021-05-20 at 5 14 17 PM](https://user-images.githubusercontent.com/5952735/119063949-147b0300-b98f-11eb-8e8c-d5ee1e23bfc1.png)
![Screen Shot 2021-05-20 at 5 14 37 PM](https://user-images.githubusercontent.com/5952735/119063950-147b0300-b98f-11eb-9055-c89518bf8524.png)
![Screen Shot 2021-05-20 at 5 15 01 PM](https://user-images.githubusercontent.com/5952735/119063951-147b0300-b98f-11eb-8323-7602bf673205.png)



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/sensors/test_emr_step.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py']
Ground Truth : ['a/airflow/www/widgets.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: Bad parsing for Cloudwatch Group ARN for logging
### Apache Airflow version

2.1.3

### Operating System

All

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### What happened

I am deploying Airflow in AWS ECS,
I am trying to send tasks logs to cloudwatch.
Usually log groups in AWS have this format /aws/name_of_service/name_of_component

I configured my env variables as follow
```
    {
      name  = "AIRFLOW__LOGGING__REMOTE_LOGGING",
      value = "true"
    },
    {
      name = "AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER",
      value = "cloudwatch://arn:aws:logs:aaaa:bbbbb:log-group:/aws/ecs/ccccc"
    },
    {
      name  = "AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID",
      value = "aws_default"
    },
```
I am getting this error in my tasks logs
```
*** Reading remote log from Cloudwatch log_group:  log_stream: hello_airflow2/hello_task/2021-11-23T16_29_21.191922+00_00/1.log.
Could not read remote logs from log_group:  log_stream: hello_airflow2/hello_task/2021-11-23T16_29_21.191922+00_00/1.log.
```
It's throwed because the log group is empty
The reason behind this error is this line
https://github.com/apache/airflow/blob/c4fd84accd143977cba57e4daf6daef2af2ff457/airflow/config_templates/airflow_local_settings.py#L202

the result of netloc is "arn:aws:logs:aaaa:bbbbb:log-group:"
```
urlparse("cloudwatch://arn:aws:logs:aaaa:bbbbb:log-group:/aws/ecs/ccccc")
ParseResult(scheme='cloudwatch', netloc='arn:aws:logs:aaaa:bbbbb:log-group:', path='/aws/ecs/ccccc', params='', query='', fragment='')
```
and this line
https://github.com/apache/airflow/blob/86a2a19ad2bdc87a9ad14bb7fde9313b2d7489bb/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py#L53
which will result an empty log group




### What you expected to happen

_No response_

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/triggers/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/utils/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/links/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_batch_client.py']
Ground Truth : ['a/airflow/config_templates/airflow_local_settings.py']
Current Recall: 0.004282655246252677

=========================================================

ISSUE: Realtime ECS logging
**Description**

Currently when `ECSOperator` is run, the logs of the ECS task are fetched only when the task is done. That's not so convenient, especially when the task takes some good amount of time. In order to understand what's happening with the task, I need to go to Cloudwatch and search for the tasks logs. It would be good to have some parallel process that could fetch ECS task logs from Cloudwatch and make them visible in a realtime. 

**Are you willing to submit a PR?**

I can try, but I need to be guided.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/logging_mixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/triggers/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/utils/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/ecs.py']
Current Recall: 0.006423982869379015

=========================================================

ISSUE: subdag doesn't work with LocalExecutor/SequentialExecutor
I'm working with Airflow 1.6.2 on OSX; 

When running SubdagOperator using LocalExecutor/SequentialExecutor, it always hit the exception of "DAG {sub_dag_name} could not be found in {dag_folder}'", from https://github.com/airbnb/airflow/blob/master/airflow/bin/cli.py#L159

It looks to me that with LocalExecutor/SequentialExecutor, dags are not pickled, so when subdag operator uses backfill_job to execute subdag as a dag, it always tries to load the subdag from dag folder, which causes error because we hide the subdag using a factory method. (follow http://pythonhosted.org/airflow/concepts.html#subdags)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_subdag_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/subdag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/mark_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dags/test_impersonation_subdag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_blocked.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dags/test_subdag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_serialized_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_subdag_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_cli_util.py']
Ground Truth : ['a/airflow/models.py']
Current Recall: 0.006423982869379015

=========================================================

ISSUE: Inconsistent 'owner' field in examples
Dear Airflow Maintainers,
### Environment
- Version of Airflow (e.g. a release version, running your own fork, running off master -- provide a git log snippet): **1.7.0**
- Screen shots of your DAG's graph and tree views:
  ![owner field](http://i.imgur.com/9U8rzil.png)
- Operating System: (Windows Version or `$ uname -a`) **Ubuntu 14.04**
- Python Version: `$ python --version` **2.7**
### Description of Issue
- What did you expect to happen? **All of the examples have a consistent owner, probably 'airflow'**
- What happened instead? **[Some](https://github.com/airbnb/airflow/blob/master/airflow/example_dags/example_python_operator.py) examples have `airflow`, [some](https://github.com/airbnb/airflow/blob/master/airflow/example_dags/example_passing_params_via_test_command.py) have `me`**
### Reproduction Steps
1. install airflow 1.7.0 via pip
2. start the webserver
3. look at the web UI, probably http://localhost:8080

**Inconsistent hard-coding in the examples will likely lead to confusion for new users.**


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/hooks/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py']
Ground Truth : ['a/airflow/example_dags/example_trigger_controller_dag.py']
Current Recall: 0.006423982869379015

=========================================================

ISSUE: Use appropriate TaskInstance object related to TaskFail to get execution_date
`TaskFail` instance has no execution_date attribute. Fetch execution_date from the relevant `TaskInstance` object to form the dictionary key tuple.

closes: #22933
related: #22933

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_core.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/skipmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/schemas/task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_experimental/common/test_delete_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/experimental/get_task_instance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/stackdriver_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/subdag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/operator_helpers.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/models/taskfail.py']
Current Recall: 0.006423982869379015

=========================================================

ISSUE: API Endpoint - CRUD - Variable
Hello 

We need to create several endpoints that perform basic CRUD operations on **Variable**. We need the following endpoints:

POST /variables
DELETE /variables/{variable_key}
PATCH /variables/{variable_key}

Detailed information is available in the issue:
https://github.com/apache/airflow/issues/8118

LOVE,

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/variable_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/secrets/lockbox.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataprep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/secrets/test_systems_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/secrets/test_secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/hashicorp/secrets/test_vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py']
Ground Truth : ['a/airflow/models/variable.py', '/dev/null', 'a/airflow/api_connexion/endpoints/variable_endpoint.py', 'a/airflow/www/extensions/init_views.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Grid view details shows `_PythonDecoratedOperator` instead of `@task.python`
### Apache Airflow version

main (development)

### What happened

I ran a dag and navigated to the details drawer in the grid view.  Saw this:

<img width="358" alt="Screen Shot 2022-04-05 at 12 49 03 PM" src="https://user-images.githubusercontent.com/5834582/161830203-60b79ff7-3984-4c8b-b010-efd0bd6470d3.png">


### What you think should happen instead

The class name was descriptive enough, but it's not a class I had ever directly referenced.  It would be easier for me to correlate this detail with my code if `@task` showed up somewhere in whatever is displayed.

### How to reproduce

Run any dag with task decorators, view details about those task instances

### Operating System

Mac OS 12.3.1

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

installed via `pip install ~/src/airflow` cloned at 34154803a
ran via `airflow webserver`

### Anything else

I'm under the impression that a new endpoint might be needed to expose friendlier operator descriptions.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/decorators/base.py', 'a/airflow/www/views.py', 'a/airflow/decorators/python_virtualenv.py', 'a/airflow/models/abstractoperator.py', 'a/airflow/models/baseoperator.py', 'a/airflow/api_connexion/schemas/task_schema.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/decorators/python.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Airflow Stable REST API [GET api/v1/pools] issue
**Apache Airflow version**: v2.0.2

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): N/A

**Environment**: AWS

- **Cloud provider or hardware configuration**: AWS EC2 Instance
- **OS** (e.g. from /etc/os-release): Ubuntu Server 20.04 LTS
- **Kernel** (e.g. `uname -a`): Linux ip-172-31-23-31 5.4.0-1048-aws #50-Ubuntu SMP Mon May 3 21:44:17 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**: 
- **Others**: Python version: 3.8.5

**What happened**: Using Airflow Stable REST API [GET api/v1/pools] results in Ooops!  This only occurs when the pools have "Running Slots".  If no tasks are running and the slots are zero, then it works just fine.

<!-- (please include exact error messages if you can) -->

Something bad has happened.
Please consider letting us know by creating a bug report using GitHub.

Python version: 3.8.5
Airflow version: 2.0.2
Node: ip-172-31-23-31.ec2.internal
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/tool/gto_env/lib/python3.8/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/tool/gto_env/lib/python3.8/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/tool/gto_env/lib/python3.8/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/home/tool/gto_env/lib/python3.8/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/home/tool/gto_env/lib/python3.8/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/tool/gto_env/lib/python3.8/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/decorators/decorator.py", line 48, in wrapper
    response = function(request)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/decorators/uri_parsing.py", line 144, in wrapper
    response = function(request)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/decorators/validation.py", line 384, in wrapper
    return function(request)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/decorators/response.py", line 104, in wrapper
    return _wrapper(request, response)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/decorators/response.py", line 89, in _wrapper
    self.operation.api.get_connexion_response(response, self.mimetype)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/apis/abstract.py", line 351, in get_connexion_response
    response = cls._response_from_handler(response, mimetype)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/apis/abstract.py", line 331, in _response_from_handler
    return cls._build_response(mimetype=mimetype, data=response, extra_context=extra_context)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/apis/flask_api.py", line 173, in _build_response
    data, status_code, serialized_mimetype = cls._prepare_body_and_status_code(data=data, mimetype=mimetype, status_code=status_code, extra_context=extra_context)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/apis/abstract.py", line 403, in _prepare_body_and_status_code
    body, mimetype = cls._serialize_data(data, mimetype)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/apis/flask_api.py", line 190, in _serialize_data
    body = cls.jsonifier.dumps(data)
  File "/home/tool/gto_env/lib/python3.8/site-packages/connexion/jsonifier.py", line 44, in dumps
    return self.json.dumps(data, **kwargs) + '\n'
  File "/home/tool/gto_env/lib/python3.8/site-packages/flask/json/__init__.py", line 211, in dumps
    rv = _json.dumps(obj, **kwargs)
  File "/usr/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/usr/lib/python3.8/json/encoder.py", line 201, in encode
    chunks = list(chunks)
  File "/usr/lib/python3.8/json/encoder.py", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File "/usr/lib/python3.8/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/usr/lib/python3.8/json/encoder.py", line 325, in _iterencode_list
    yield from chunks
  File "/usr/lib/python3.8/json/encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "/usr/lib/python3.8/json/encoder.py", line 438, in _iterencode
    o = _default(o)
  File "/home/tool/gto_env/lib/python3.8/site-packages/airflow/utils/json.py", line 74, in _default
    raise TypeError(f"Object of type '{obj.__class__.__name__}' is not JSON serializable")
TypeError: Object of type 'Decimal' is not JSON serializable

**What you expected to happen**:  I expect the appropriate JSON response

<!-- What do you think went wrong? -->

**How to reproduce it**:
On an Airflow instance, run some tasks and while the tasks are running query the pools via the API.  NOTE: That you have to query the specific pool that has tasks running, if you avoid the pool using limit and/or offset then the issue will not occur.  You must try to return a pool with running_slots > 0

**Anything else we need to know**:

Not really


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/sbom_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/ads/hooks/ads.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py']
Ground Truth : ['a/airflow/utils/json.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Secret masking fails on io objects
**Apache Airflow version**: 2.1.0


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): N/A

**Environment**: *NIX

- **Cloud provider or hardware configuration**: N/A
- **OS** (e.g. from /etc/os-release): N/A
- **Kernel** (e.g. `uname -a`): N/A
- **Install tools**:
- **Others**:

**What happened**:

Due to the new secrets masker, logging will fail when an IO object is passed to a logging call.

**What you expected to happen**:

Logging should succeed when an IO object is passed to the logging cal.

**How to reproduce it**:

Sample DAG:

```python
import logging
from datetime import datetime

from airflow import DAG
from airflow.operators.python import PythonOperator


log = logging.getLogger(__name__)


def log_io():
    file = open("/tmp/foo", "w")
    log.info("File: %s", file)


# Create the DAG -----------------------------------------------------------------------
dag = DAG(
    dag_id="Test_Log_IO",
    schedule_interval=None,
    catchup=False,
    default_args={
        "owner": "madison.swain-bowden",
        "depends_on_past": False,
        "start_date": datetime(2021, 5, 4),
    },
)


with dag:
    PythonOperator(
        task_id="log_io",
        python_callable=log_io,
    )
```

Logging that occurs when run on Airflow (task subsequently fails):

```
[2021-05-25 11:27:08,080] {logging_mixin.py:104} INFO - Running <TaskInstance: Test_Log_IO.log_io 2021-05-25T18:25:17.679660+00:00 [running]> on host Madisons-MacBook-Pro
[2021-05-25 11:27:08,137] {taskinstance.py:1280} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=madison.swain-bowden
AIRFLOW_CTX_DAG_ID=Test_Log_IO
AIRFLOW_CTX_TASK_ID=log_io
AIRFLOW_CTX_EXECUTION_DATE=2021-05-25T18:25:17.679660+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-25T18:25:17.679660+00:00
[2021-05-25 11:27:08,138] {taskinstance.py:1481} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1137, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/operators/python.py", line 150, in execute
    return_value = self.execute_callable()
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/operators/python.py", line 161, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/madison/git/airflow-dags/ookla/dags/Test_Log_IO/log_io.py", line 13, in log_io
    log.info("File: %s", file)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/logging/__init__.py", line 1446, in info
    self._log(INFO, msg, args, **kwargs)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/logging/__init__.py", line 948, in handle
    rv = self.filter(record)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/logging/__init__.py", line 806, in filter
    result = f.filter(record)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/utils/log/secrets_masker.py", line 157, in filter
    record.__dict__[k] = self.redact(v)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/utils/log/secrets_masker.py", line 203, in redact
    return tuple(self.redact(subval) for subval in item)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/utils/log/secrets_masker.py", line 203, in <genexpr>
    return tuple(self.redact(subval) for subval in item)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/utils/log/secrets_masker.py", line 205, in redact
    return list(self.redact(subval) for subval in item)
  File "/Users/madison/programs/anaconda3/envs/ookla-airflow/lib/python3.9/site-packages/airflow/utils/log/secrets_masker.py", line 205, in <genexpr>
    return list(self.redact(subval) for subval in item)
io.UnsupportedOperation: not readable
[2021-05-25 11:27:08,145] {taskinstance.py:1524} INFO - Marking task as FAILED. dag_id=Test_Log_IO, task_id=log_io, execution_date=20210525T182517, start_date=20210525T182707, end_date=20210525T182708
[2021-05-25 11:27:08,197] {local_task_job.py:151} INFO - Task exited with return code 1
```


**Anything else we need to know**:

If I set the value defined here to `False`, the task completes successfully and the line is logged appropriately: https://github.com/apache/airflow/blob/2.1.0/airflow/cli/commands/task_command.py#L205

Example output (when set to `False`):

```
[2021-05-25 11:48:54,185] {logging_mixin.py:104} INFO - Running <TaskInstance: Test_Log_IO.log_io 2021-05-25T18:48:45.911082+00:00 [running]> on host Madisons-MacBook-Pro
[2021-05-25 11:48:54,262] {taskinstance.py:1280} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=madison.swain-bowden
AIRFLOW_CTX_DAG_ID=Test_Log_IO
AIRFLOW_CTX_TASK_ID=log_io
AIRFLOW_CTX_EXECUTION_DATE=2021-05-25T18:48:45.911082+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-05-25T18:48:45.911082+00:00
[2021-05-25 11:48:54,264] {log_io.py:13} INFO - File: <_io.TextIOWrapper name='/tmp/foo' mode='w' encoding='UTF-8'>
[2021-05-25 11:48:54,264] {python.py:151} INFO - Done. Returned value was: None
[2021-05-25 11:48:54,274] {taskinstance.py:1184} INFO - Marking task as SUCCESS. dag_id=Test_Log_IO, task_id=log_io, execution_date=20210525T184845, start_date=20210525T184854, end_date=20210525T184854
[2021-05-25 11:48:54,305] {taskinstance.py:1245} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2021-05-25 11:48:54,339] {local_task_job.py:151} INFO - Task exited with return code 0
```

Unfortunately the logging that caused this problem for me originally is being done by a third party library, so I can't alter the way this works on our end.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py']
Ground Truth : ['a/airflow/utils/log/secrets_masker.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: [Graph view] After clearing the task (and its downstream tasks) in a task group the task group becomes disconnected from the dag
### Apache Airflow version

2.3.4

### What happened

n the graph view of the dag, after clearing the task (and its downstream tasks) in a task group and refreshing the page the browser the task group becomes disconnected from the dag. See attached gif.
![airflow_2_3_4_task_group_bug](https://user-images.githubusercontent.com/6542519/187409008-767e13e6-ab91-4875-9f3e-bd261b346d0f.gif)
The issue is not persistent and consistent. The graph view becomes disconnected from time to time as you can see on the attached video.

### What you think should happen instead

The graph should be rendered properly and consistently.

### How to reproduce

1. Add the following dag to the dag folder:
```
import logging
import time
from typing import List

import pendulum
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup


def log_function(message: str, **kwargs):
    logging.info(message)
    time.sleep(3)


def create_file_handling_task_group(supplier):
    with TaskGroup(group_id=f"file_handlig_task_group_{supplier}", ui_color='#666666') as file_handlig_task_group:
        entry = PythonOperator(
            task_id='entry',
            python_callable=log_function,
            op_kwargs={'message': 'create_file_handlig_task_group-Entry-task'}
        )
        with TaskGroup(group_id=f"file_handling_task_sub_group-{supplier}",
                       ui_color='#666666') as file_handlig_task_sub_group:
            sub_group_submit = PythonOperator(
                task_id='sub_group_submit',
                python_callable=log_function,
                op_kwargs={'message': 'create_file_handlig_sub_group_submit'}
            )
            sub_group_monitor = PythonOperator(
                task_id='sub_group_monitor',
                python_callable=log_function,
                op_kwargs={'message': 'create_file_handlig_sub_group_monitor'}
            )
            sub_group_submit >> sub_group_monitor
        entry >> file_handlig_task_sub_group
    return file_handlig_task_group


def get_stage_1_taskgroups(supplierlist: List) -> List[TaskGroup]:
    return [create_file_handling_task_group(supplier) for supplier in supplierlist]


def connect_stage1_to_stage2(self, stage1_tasks: List[TaskGroup], stage2_tasks: List[TaskGroup]) -> None:
    if stage2_tasks:
        for stage1_task in stage1_tasks:
            supplier_code: str = self.get_supplier_code(stage1_task)
            stage2_task = self.get_suppliers_tasks(supplier_code, stage2_tasks)
            stage1_task >> stage2_task


def get_stage_2_taskgroup(taskgroup_id: str):
    with TaskGroup(group_id=taskgroup_id, ui_color='#666666') as stage_2_taskgroup:
        sub_group_submit = PythonOperator(
            task_id='sub_group_submit',
            python_callable=log_function,
            op_kwargs={'message': 'create_file_handlig_sub_group_submit'}
        )
        sub_group_monitor = PythonOperator(
            task_id='sub_group_monitor',
            python_callable=log_function,
            op_kwargs={'message': 'create_file_handlig_sub_group_monitor'}
        )
        sub_group_submit >> sub_group_monitor
    return stage_2_taskgroup


def create_dag():
    with DAG(
            dag_id="horizon-task-group-bug",
            start_date=pendulum.datetime(2021, 1, 1, tz="UTC"),
            catchup=False,
            description="description"
    ) as dag:
        start = PythonOperator(
            task_id='start_main',
            python_callable=log_function,
            op_kwargs={'message': 'Entry-task'}
        )
        end = PythonOperator(
            task_id='end_main',
            python_callable=log_function,
            op_kwargs={'message': 'End-task'}
        )
        with TaskGroup(group_id=f"main_file_task_group", ui_color='#666666') as main_file_task_group:
            end_main_file_task_stage_1 = PythonOperator(
                task_id='end_main_file_task_stage_1',
                python_callable=log_function,
                op_kwargs={'message': 'end_main_file_task_stage_1'}
            )
            first_stage = get_stage_1_taskgroups(['9001', '9002'])
            first_stage >> get_stage_2_taskgroup("stage_2_1_taskgroup")
            first_stage >> get_stage_2_taskgroup("stage_2_2_taskgroup")
            first_stage >> end_main_file_task_stage_1
        start >> main_file_task_group >> end
        return dag


dag = create_dag()

```
2. Go to de graph view of the dag.
3. Run the dag.
4. After the dag run has finished. Clear the "sub_group_submit" task within the "stage_2_1_taskgroup" with downstream tasks.
5. Refresh the page multiple times and notice how from time to time the "stage_2_1_taskgroup" becomes disconnected from the dag.
6.  Clear the "sub_group_submit" task within the "stage_2_2_taskgroup" with downstream tasks.
7.  Refresh the page multiple times and notice how from time to time the "stage_2_2_taskgroup" becomes disconnected from the dag.

### Operating System

Mac OS, Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

Custom docker image based on apache/airflow:2.3.4-python3.10

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_python_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_edgemodifier.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/cloud_batch/example_cloud_batch.py']
Ground Truth : ['a/airflow/models/dag.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Upgrade `importlib-resources` version
### Description

The version for `importlib-resources` constraint sets it to be [v1.5.0](https://github.com/python/importlib_resources/tree/v1.5.0) which is over a year old. For compatibility sake (for instance with something like Datapane) I would suggest upgrading it. 

### Use case/motivation

Upgrade a an old dependency to keep code up to date.

### Related issues

Not that I am aware of, maybe somewhat #12120, or #15991.

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/exampleinclude.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/logging_mixin.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Refactor poll_query_status in EMRContainerHook
### Body

The goal is to refactor the code so that we can remove this TODO
https://github.com/apache/airflow/blob/7640ba4e8ee239d6e2bbf950d53d624b9df93059/airflow/providers/amazon/aws/hooks/emr_containers.py#L174-L176

More information about the concerns can be found on https://github.com/apache/airflow/pull/16766#discussion_r668089559

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/pubsub.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py']
Ground Truth : ['a/airflow/providers/amazon/aws/hooks/emr.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: [AIP-31] Create XComArg model
**Description**

XComArg is a class that references an XCom entry from an operator. Can be used to explicitly set XCom dependencies and explictly pass results between operators.

_Class attributes:_
- `operator: BaseOperator`: Origin operator instance.
- `key: str`: Stores key for XCom value. Defaults to `airflow.models.xcom. XCOM_RETURN_KEY`

_Class methods_
- `get(context: dict)-> Any`: Resolves XCom value given `operator` and `key`.
- `__getitem__(key, str) -> XComArg`: Easy method to create new XComArg using the same operator but a different key.
- `add_downstream_dependent(operator: BaseOperator)->None`: Add an operator as a downstream dependency of `operator`. [Optional] Makes this task simpler.

Proposed implementation: https://github.com/casassg/corrent/blob/master/corrent/xcom_arg.py

**Use case / motivation**

- Explicit way to pass around results between operators. 
- This object is a reference to an XCom value that has not been created and will need to be resolved in the future. 
- It can generate new XComArgs that point to the same operator but has a different key by using the key accessor. E.g: output['train'].
- To do so, it includes as class attributes both the originating operator instance and a key (both needed to pull the XCom).
- Has a property op  that allows access to the origin operator.


**Stretch goal**
- Store type class for the XCom and lie to MyPy when trying to check class signature.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/skipmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/pubsub.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py']
Ground Truth : ['/dev/null', 'a/airflow/models/baseoperator.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Unable to configure Google Secrets Manager in 2.3.4
### Apache Airflow version

2.3.4

### What happened

I am attempting to configure a Google Secrets Manager secrets backend using the `gcp_keyfile_dict` param in a `.env` file with the following ENV Vars:

```
AIRFLOW__SECRETS__BACKEND=airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend
AIRFLOW__SECRETS__BACKEND_KWARGS='{"connections_prefix": "airflow-connections", "variables_prefix": "airflow-variables", "gcp_keyfile_dict": <json-keyfile>}'
```

In previous versions including 2.3.3 this worked without issue

After upgrading to Astro Runtime 5.0.8 I get the following error taken from the scheduler container logs. The scheduler, webserver, and triggerer are continually restarting

```
Traceback (most recent call last):
  File "/usr/local/bin/airflow", line 5, in <module>
    from airflow.__main__ import main
  File "/usr/local/lib/python3.9/site-packages/airflow/__init__.py", line 35, in <module>
    from airflow import settings
  File "/usr/local/lib/python3.9/site-packages/airflow/settings.py", line 35, in <module>
    from airflow.configuration import AIRFLOW_HOME, WEBSERVER_CONFIG, conf  # NOQA F401
  File "/usr/local/lib/python3.9/site-packages/airflow/configuration.py", line 1618, in <module>
    secrets_backend_list = initialize_secrets_backends()
  File "/usr/local/lib/python3.9/site-packages/airflow/configuration.py", line 1540, in initialize_secrets_backends
    custom_secret_backend = get_custom_secret_backend()
  File "/usr/local/lib/python3.9/site-packages/airflow/configuration.py", line 1523, in get_custom_secret_backend
    return _custom_secrets_backend(secrets_backend_cls, **alternative_secrets_config_dict)
TypeError: unhashable type: 'dict'
```





### What you think should happen instead

Containers should remain healthy and the secrets backend should successfully be added

### How to reproduce

`astro dev init` a fresh project

Dockerfile:
`FROM quay.io/astronomer/astro-runtime:5.0.8`


`.env` file: 
```
AIRFLOW__SECRETS__BACKEND=airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend
AIRFLOW__SECRETS__BACKEND_KWARGS='{"connections_prefix": "airflow-connections", "variables_prefix": "airflow-variables", "gcp_keyfile_dict": <service-acct-json-keyfile>}'
```

`astro dev start`


### Operating System

macOS 11.6.8

### Versions of Apache Airflow Providers

[apache-airflow-providers-google](https://airflow.apache.org/docs/apache-airflow-providers-google/8.1.0/) 8.1.0

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/secrets/secret_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Deleted DAG raises SerializedDagNotFound exception when accessing webserver
**Apache Airflow version**: 2.1.2


**Environment**:

- **Docker image**: official apache/airflow:2.1.2 extended with pip installed packages
- **Executor**: CeleryExecutor
- **Database**: PostgreSQL (engine version: 12.5)

**What happened**:

I've encountered `SerializedDagNotFound: DAG 'my_dag_id' not found in serialized_dag table` after deleting DAG via web UI. Exception is raised each time when UI is accessed. The exception itself does not impact UI, but still an event is sent to Sentry each time I interact with it.

**What you expected to happen**:

After DAG deletion I've expected that all records of it apart from logs would be deleted, but it's DAG run was still showing up in Webserver UI (even though, I couldn't find any records of DAG in the metadb, apart from dag_tag table still containing records related to deleted DAG (Which may be a reason for a new issue), but manual deletion of those records had no impact. 

After I've deleted DAG Run via Web UI, the exception is no longer raised.

**How to reproduce it**:

To this moment, I've only encountered this with one DAG, which was used for debugging purposes. It consists of multiple PythonOperators and it uses the same boilerplate code I use for dynamic DAG generation (return DAG object from `create_dag` function, add it to globals), but in it I've replaced dynamic generation logic with just `dag = create_dag(*my_args, **my_kwargs)`. I think this may have been caused by DAG run still running, when DAG was deleted, but cannot support this theory with actual information. 

<details>
  <summary>Traceback</summary> 

```python
SerializedDagNotFound: DAG 'my_dag_id' not found in serialized_dag table
  File "flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "flask/_compat.py", line 39, in reraise
    raise value
  File "flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "airflow/www/auth.py", line 34, in decorated
    return func(*args, **kwargs)
  File "airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "airflow/www/views.py", line 1679, in blocked
    dag = current_app.dag_bag.get_dag(dag_id)
  File "airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "airflow/models/dagbag.py", line 186, in get_dag
    self._add_dag_from_db(dag_id=dag_id, session=session)
  File "airflow/models/dagbag.py", line 258, in _add_dag_from_db
    raise SerializedDagNotFound(f"DAG '{dag_id}' not found in serialized_dag table")
```
</details>


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Mapped KubernetesPodOperater not rendering nested templates
### Apache Airflow version

2.3.3

### What happened

Nested values, such as `env_vars` for the `KubernetesPodOperater` are not being rendered when used as a dynamically mapped operator.

Assuming the following:

```python
op = KubernetesPodOperater.partial(
    env_vars=[k8s.V1EnvVar(name='AWS_ACCESS_KEY_ID', value='{{ var.value.aws_access_key_id }}')],
    # Other arguments
).expand(arguments=[[1], [2]])
```

The *Rendered Template* results for `env_vars` should be:
```
("[{'name': 'AWS_ACCESS_KEY_ID', 'value': 'some-super-secret-value', 'value_from': None}]")
```

Instead the actual *Rendered Template* results for `env_vars` are un-rendered:
```
("[{'name': 'AWS_ACCESS_KEY_ID', 'value': '{{ var.value.aws_access_key_id }}', 'value_from': None}]")
```

This is probably caused by the fact that `MappedOperator` is not calling [`KubernetesPodOperater._render_nested_template_fields`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py#L286).

### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Operating System

Ubuntu 18.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/operators/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py']
Ground Truth : ['a/airflow/models/mappedoperator.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: A minor typo in NotPreviouslySkippedDep
**Apache Airflow version**: master


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): n/a

**Environment**: n/a

- **Cloud provider or hardware configuration**: n/a
- **OS** (e.g. from /etc/os-release): n/a
- **Kernel** (e.g. `uname -a`): n/a
- **Install tools**: n/a
- **Others**: n/a

**What happened**:

Found a typo in `airflow/airflow/ti_deps/deps/not_previously_skipped_dep.py`:
```python
class NotPreviouslySkippedDep(BaseTIDep):
    """
    Determines if any of the task's direct upstream relatives have decided this task should
    be skipped.
    """

    NAME = "Not Previously Skipped"
    IGNORABLE = True
    IS_TASK_DEP = True
```

**What you expected to happen**:

`IGNORABLE`, though lexically correct, should have been `IGNOREABLE`, as specified in `BaseTIDep`:
```python
class BaseTIDep:
    """
    Abstract base class for dependencies that must be satisfied in order for task
    instances to run. For example, a task that can only run if a certain number of its
    upstream tasks succeed. This is an abstract class and must be subclassed to be used.
    """

    # If this dependency can be ignored by a context in which it is added to. Needed
    # because some dependencies should never be ignoreable in their contexts.
    IGNOREABLE = False
```

It is being used here:
```python
@provide_session
    def get_dep_statuses(self, ti, session, dep_context=None):
        """
        Wrapper around the private _get_dep_statuses method that contains some global
        checks for all dependencies.

        :param ti: the task instance to get the dependency status for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: the context for which this dependency should be evaluated for
        :type dep_context: DepContext
        """
        if dep_context is None:
            dep_context = DepContext()

        if self.IGNOREABLE and dep_context.ignore_all_deps:
            yield self._passing_status(
                reason="Context specified all dependencies should be ignored.")
            return

        if self.IS_TASK_DEP and dep_context.ignore_task_deps:
            yield self._passing_status(
                reason="Context specified all task dependencies should be ignored.")
            return

        yield from self._get_dep_statuses(ti, session, dep_context)
```

**How to reproduce it**:

n/a

**Anything else we need to know**:

I can help out with this issue if given the green light  

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/base_ti_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/not_previously_skipped_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/prev_dagrun_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/ready_to_reschedule.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/pool_slots_available_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/valid_state_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/dep_context.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/dagrun_backfill_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py']
Ground Truth : ['a/airflow/ti_deps/deps/not_previously_skipped_dep.py', 'a/airflow/models/taskinstance.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Superfluous TypeError when passing not-iterables to `expand()`
### Apache Airflow version

main (development)

### What happened

Here's a problematic dag.  `False` is invalid here.
```python3
from airflow.models import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

with DAG(
    dag_id="singleton_expanded",
    schedule_interval=timedelta(days=365),
    start_date=datetime(2001, 1, 1),
) as dag:

    # has problem
    PythonOperator.partial(
        task_id="foo",
        python_callable=lambda x: "hi" if x else "bye",
    ).expand(op_args=False)
```

When I check for errors like `python dags/the_dag.py` I get the following error:
```
Traceback (most recent call last):
  File "/Users/matt/2022/03/30/dags/the_dag.py", line 13, in <module>
    PythonOperator.partial(
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 187, in expand
    validate_mapping_kwargs(self.operator_class, "expand", mapped_kwargs)
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 116, in validate_mapping_kwargs
    raise ValueError(error)
ValueError: PythonOperator.expand() got an unexpected type 'bool' for keyword argument op_args
Exception ignored in: <function OperatorPartial.__del__ at 0x10c63b1f0>
Traceback (most recent call last):
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 182, in __del__
    warnings.warn(f"{self!r} was never mapped!")
  File "/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/warnings.py", line 109, in _showwarnmsg
    sw(msg.message, msg.category, msg.filename, msg.lineno,
  File "/Users/matt/src/airflow/airflow/settings.py", line 115, in custom_show_warning
    from rich.markup import escape
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 925, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1414, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1380, in _get_spec
TypeError: 'NoneType' object is not iterable
```

### What you think should happen instead

I'm not sure what's up with that type error, the ValueError is what I needed to see.  So I expected this:

```
Traceback (most recent call last):
  File "/Users/matt/2022/03/30/dags/the_dag.py", line 13, in <module>
    PythonOperator.partial(
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 187, in expand
    validate_mapping_kwargs(self.operator_class, "expand", mapped_kwargs)
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 116, in validate_mapping_kwargs
    raise ValueError(error)
ValueError: PythonOperator.expand() got an unexpected type 'bool' for keyword argument op_args
```

### How to reproduce

_No response_

### Operating System

Mac OS

### Versions of Apache Airflow Providers

n/a

### Deployment

Virtualenv installation

### Deployment details

- cloned main at 327eab3e2
- created fresh venv and used pip to install
- `airflow info`
- `airflow db init`
- add the dag
- `python dags/the_dag.py`


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py']
Ground Truth : ['a/airflow/models/mappedoperator.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Make Docs builds fallback in case external docs sources are missing
Every now and then our docs builds start to fail because of external dependency (latest example here #14985). And while we are doing caching now of that information, it does not help when the initial retrieval fails. This information does not change often but with the number of dependencies we have it will continue to fail regularly simply because many of those depenencies are not very reliable - they are just a web page hosted somewhere. They are nowhere near the stabilty of even PyPI or Apt sources and we have no mirroring in case of problem.

Maybe we could 

a) see if we can use some kind of mirroring scheme (do those sites have mirrrors ? )
b) if not, simply write a simple script that will dump the cached content for those to S3, refresh it in the CI scheduled (nightly) master builds ad have a fallback mechanism to download that from there in case of any problems in CI?

 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py']
Ground Truth : ['a/docs/exts/docs_build/fetch_inventories.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Make sure that we have no conflicting dependencies when installing.
In October 2020 PIP will start failing in case of some conflicting dependencies. We should run the latest version of PIP and make sure to use the ``--use-feature=2020-resolver`` to test the resolution of packages and fix them,

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/docker_tests_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/reinstall.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py']
Ground Truth : ['a/setup.py', 'a/airflow/providers/microsoft/azure/hooks/wasb.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: DAG with schedule interval '@once' cannot be scheduled
Dear Airflow Maintainers,

Before I tell you about my issue, let me describe my environment:
# Environment
- Version of Airflow (e.g. a release version, running your own fork, running off master -- provide a git log snippet) : **1.7.0**
- Example code to reproduce the bug (as a code snippet in markdown) **example_xcom from stock examples**
- Stack trace if applicable:

```
[2016-03-30 15:36:49,858] {models.py:204} INFO - Importing /usr/local/lib/python2.7/dist-packages/airflow/example_dags/example_xcom.py
[2016-03-30 15:36:49,863] {models.py:296} INFO - Loaded DAG <DAG: example_xcom>
[2016-03-30 15:36:49,874] {jobs.py:642} ERROR - list index out of range
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/airflow/jobs.py", line 638, in _execute
    self.schedule_dag(dag)
  File "/usr/local/lib/python2.7/dist-packages/airflow/jobs.py", line 397, in schedule_dag
    next_run_date = dag.date_range(latest_run, -5)[0]
IndexError: list index out of range
```
- Operating System: (Windows Version or `$ uname -a`) : **Ubuntu 14.04**
- Python Version: `$ python --version` **2.7.6**

Now that you know a little about me, let me tell you about the issue I am having:
# Description of Issue
- What did you expect to happen? **For the DAG to be scheduled**
- What happened instead? **Error shown above**
- Here is how you can reproduce this issue on your machine:
## Reproduction Steps
1. Make sure the stock example DAG `example_xcom` is unpaused
2. If there is no example_xcom example, create any DAG with a schedule_interval of `@once`
3. Run the scheduler


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/jobs.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Manually triggering a dagrun when already at max_active_runs leads to nothing being scheduled.
If you have a DAG with `max_active_runs=1` and it is running some tasks, and then you manually trigger another run of this DAG, both runs end up not scheduling anything and you see this message over and over in the logs:

```
[2020-10-16 14:00:34,118]  2437572 {{airflow.jobs.scheduler_job.SchedulerJob scheduler_job.py:1657}} INFO - DAG scenario1_case2_01_1 already has 1 active runs, not queuing any more tasks
```

A longer term fix for this would be to introduce a "queued" state for DagRuns, and then when manually creating dag runs set it to queued, and only have the scheduler set DagRuns to running.

That may or may not be less work than fixing the current mechanism.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_quicksight.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Unable to start scheduler after stopped
**Apache Airflow version**: 2.0.0rc3

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**: Linux
- **OS** (e.g. from /etc/os-release): Ubuntu
- **Kernel** (e.g. `uname -a`): 
- **Install tools**:
- **Others**:

**What happened**:

After shutting down the scheduler, while tasks were in running state, trying to restart the scheduler results in pk violations..

```[2020-12-15 22:43:29,673] {scheduler_job.py:1293} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/home/jcoder/git/airflow_2.0/pyenv/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/jcoder/git/airflow_2.0/pyenv/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 593, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "dag_run_dag_id_run_id_key"
DETAIL:  Key (dag_id, run_id)=(example_task_group, scheduled__2020-12-14T04:31:00+00:00) already exists.
```


**What you expected to happen**:

Scheduler restarts and picks up where it left off.

**How to reproduce it**:

Set example dag ( I used task_group) to schedule_interval `* * * * *` and start the scheduler and let it run for a few minutes. 
Shut down the scheduler
Attempt to restart the scheduler

**Anything else we need to know**:
I came across this doing testing using the LocalExecutor in a virtual env. If no else is able to reproduce it, I'll try again in a clean virtual env.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/scheduler_dag_execution_timing.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py']
Ground Truth : ['a/airflow/models/serialized_dag.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dagbag.py', 'a/airflow/serialization/serialized_objects.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Expanding operators inside of task groups causes KeyError
### Apache Airflow version

main (development)

### What happened

Given this DAG:
```python3
foo_var = {"VAR1": "FOO"}
bar_var = {"VAR1": "BAR"}
hi_cmd = 'echo "hello $VAR1"'
bye_cmd = 'echo "goodbye $VAR1"'

@task
def envs():
    return [foo_var, bar_var]

@task
def cmds():
    return [hi_cmd, bye_cmd]

with DAG(dag_id="mapped_bash", start_date=datetime(1970, 1, 1)) as dag:
    with TaskGroup(group_id="dynamic"):
        dynamic = BashOperator.partial(task_id="bash").expand(
            env=envs(), bash_command=cmds()
        )
```

I ran `airflow dags test mapped_bash $(date +%Y-%m-%dT%H:%M:%SZ)`

Got this output:

```
[2022-03-17 09:21:24,590] {taskinstance.py:1451} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=mapped_bash
AIRFLOW_CTX_TASK_ID=dynamic.bash
AIRFLOW_CTX_EXECUTION_DATE=2022-03-17T09:21:12+00:00
AIRFLOW_CTX_DAG_RUN_ID=backfill__2022-03-17T09:21:12+00:00
[2022-03-17 09:21:24,590] {subprocess.py:62} INFO - Tmp dir root location:
 /var/folders/5m/nvs9yfcs6mlfm_63gnk6__3r0000gn/T
[2022-03-17 09:21:24,591] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'echo "hello $VAR1"']
[2022-03-17 09:21:24,597] {subprocess.py:85} INFO - Output:
[2022-03-17 09:21:24,601] {subprocess.py:92} INFO - hello FOO
[2022-03-17 09:21:24,601] {subprocess.py:96} INFO - Command exited with return code 0
[2022-03-17 09:21:24,616] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=mapped_bash, task_id=dynamic.bash, execution_date=20220317T092112, start_date=20220317T152114, end_date=20220317T152124
[2022-03-17 09:21:24,638] {taskinstance.py:1752} WARNING - We expected to get frame set in local storage but it was not. Please report this as an issue with full logs at https://github.com/apache/airflow/issues/new
Traceback (most recent call last):
  File "/Users/matt/src/airflow/airflow/models/taskinstance.py", line 1335, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/Users/matt/src/airflow/airflow/models/taskinstance.py", line 1437, in _execute_task_with_callbacks
    self.render_templates(context=context)
  File "/Users/matt/src/airflow/airflow/models/taskinstance.py", line 2091, in render_templates
    task = self.task.render_template_fields(context)
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 602, in render_template_fields
    unmapped_task = self.unmap()
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 454, in unmap
    dag._remove_task(self.task_id)
  File "/Users/matt/src/airflow/airflow/models/dag.py", line 2188, in _remove_task
    task = self.task_dict.pop(task_id)
KeyError: 'dynamic.bash'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/matt/src/airflow/airflow/models/taskinstance.py", line 1750, in get_truncated_error_traceback
    execution_frame = _TASK_EXECUTION_FRAME_LOCAL_STORAGE.frame
AttributeError: '_thread._local' object has no attribute 'frame'
```

### What you think should happen instead

No error

### How to reproduce

Trigger the dag above

### Operating System

Mac OS 11.6

### Versions of Apache Airflow Providers

n/a

### Deployment

Virtualenv installation

### Deployment details

```
 cd ~/src/airflow
 git rev-parse --short HEAD
df6058c86

 airflow info

Apache Airflow
version                | 2.3.0.dev0
executor               | SequentialExecutor
task_logging_handler   | airflow.utils.log.file_task_handler.FileTaskHandler
sql_alchemy_conn       | sqlite:////Users/matt/2022/03/16/airflow.db
dags_folder            | /Users/matt/2022/03/16/dags
plugins_folder         | /Users/matt/2022/03/16/plugins
base_log_folder        | /Users/matt/2022/03/16/logs
remote_base_log_folder |


System info
OS              | Mac OS
architecture    | x86_64
uname           | uname_result(system='Darwin', node='LIGO', release='20.6.0', version='Darwin Kernel Version 20.6.0: Mon Aug 30 06:12:21 PDT 2021; root:xnu-7195.141.6~3/RELEASE_X86_64', machine='x86_64')
locale          | ('en_US', 'UTF-8')
python_version  | 3.9.10 (main, Jan 15 2022, 11:48:00)  [Clang 13.0.0 (clang-1300.0.29.3)]
python_location | /Users/matt/src/qa-scenario-dags/venv/bin/python3.9
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/setup_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/presto/hooks/test_presto.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py']
Ground Truth : ['a/airflow/models/mappedoperator.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/models/baseoperator.py', 'a/airflow/decorators/base.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: `airflow db clean task_instance` takes a long time
### Apache Airflow version

2.3.1

### What happened

When I ran the `airflow db clean task_instance` command, it can take up to 9 hours to complete. The database around 3215220 rows in the `task_instance` table and  51602 rows in the `dag_run` table. The overall size of the database is around 1 TB.

I believe the issue is because of the cascade constraints on others tables as well as the lack of indexes on task_instance foreign keys. 

Running delete on a small number of rows gives this shows most of the time is spent in xcom and task_fail tables

```
explain (analyze,buffers,timing) delete from task_instance t1 where t1.run_id = 'manual__2022-05-11T01:09:05.856703+00:00'; rollback;
Trigger for constraint task_reschedule_ti_fkey: time=3.208 calls=23
Trigger for constraint task_map_task_instance_fkey: time=1.848 calls=23
Trigger for constraint xcom_task_instance_fkey: time=4457.779 calls=23
Trigger for constraint rtif_ti_fkey: time=3.135 calls=23
Trigger for constraint task_fail_ti_fkey: time=1164.183 calls=23
```

I temporarily fixed it by adding these indexes.

```
create index idx_task_reschedule_dr_fkey on task_reschedule (dag_id, run_id);
create index idx_xcom_ti_fkey on xcom (dag_id, task_id, run_id, map_index);
create index idx_task_fail_ti_fkey on task_fail (dag_id, task_id, run_id, map_index);
```

### What you think should happen instead

It should not take 9 hours to complete a clean up process. Before upgrading to 2.3.x, it was taking no more than 5 minutes.

### How to reproduce

_No response_

### Operating System

N/A

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0093_2_2_0_taskinstance_keyed_to_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0100_2_3_0_add_taskmap_and_map_id_on_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py']
Ground Truth : ['a/airflow/models/xcom.py', '/dev/null', 'a/airflow/models/taskfail.py', 'a/airflow/models/taskreschedule.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: BeamRunGoPipelineOperator: temp dir with Go file from GCS is removed before starting the pipeline
### Apache Airflow Provider(s)

apache-beam

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-beam==4.1.0
apache-airflow-providers-google==8.6.0

### Apache Airflow version

2.5.0

### Operating System

macOS 13.1

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

When using the `BeamRunGoPipelineOperator` with a `go_file` on GCS, the object is downloaded to a temporary directory, however the directory with the file has already been removed by the time it is needed, i.e. when executing `go mod init` and starting the pipeline.

### What you think should happen instead

The `BeamRunGoPipelineOperator.execute` method enters into a `tempfile.TemporaryDirectory` context manager using [with](https://github.com/apache/airflow/blob/2.5.0/airflow/providers/apache/beam/operators/beam.py#L588) when downloading the `go_file` from GCS to the local filesystem. On completion of the context, this temporary directory is removed. `BeamHook.start_go_pipeline`, which uses the file, is called outside of the context however, which means the file no longer exists when `go mod init` is called.

A suggested solution is to use the `enter_context` method of the existing `ExitStack` to also enter into the TemporaryDirectory context manager. This allows the go_file to still exist when it is time to initialize the go module and start the pipeline:

```python
with ExitStack() as exit_stack:
    if self.go_file.lower().startswith("gs://"):
        gcs_hook = GCSHook(self.gcp_conn_id, self.delegate_to)
        
        tmp_dir = exit_stack.enter_context(tempfile.TemporaryDirectory(prefix="apache-beam-go"))
        tmp_gcs_file = exit_stack.enter_context(
            gcs_hook.provide_file(object_url=self.go_file, dir=tmp_dir)
        )
        
        self.go_file = tmp_gcs_file.name
        self.should_init_go_module = True
```

### How to reproduce

The problem can be reproduced by creating a DAG which uses the `BeamRunGoPipelineOperator` and passing a `go_file` with a GS URI:

```python
import pendulum
from airflow import DAG
from airflow.providers.apache.beam.operators.beam import BeamRunGoPipelineOperator


with DAG(
    dag_id="beam_go_dag",
    start_date=pendulum.today("UTC"),
) as dag:
    BeamRunGoPipelineOperator(
        task_id="beam_go_pipeline",
        go_file="gs://my-bucket/main.go"
    )
```

### Anything else

Relevant logs:

```
[2023-01-01T12:41:06.155+0100] {taskinstance.py:1303} INFO - Executing <Task(BeamRunGoPipelineOperator): beam_go_pipeline> on 2023-01-01 00:00:00+00:00
[2023-01-01T12:41:06.411+0100] {taskinstance.py:1510} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=beam_go_dag
AIRFLOW_CTX_TASK_ID=beam_go_pipeline
AIRFLOW_CTX_EXECUTION_DATE=2023-01-01T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=backfill__2023-01-01T00:00:00+00:00
[2023-01-01T12:41:06.430+0100] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2023-01-01T12:41:06.441+0100] {credentials_provider.py:323} INFO - Getting connection using `google.auth.default()` since no key file is defined for hook.
[2023-01-01T12:41:08.701+0100] {gcs.py:323} INFO - File downloaded to /var/folders/1_/7h5npt456j5f063tq7ngyxdw0000gn/T/apache-beam-gosmk3lv_4/tmp6j9g5090main.go
[2023-01-01T12:41:08.704+0100] {process_utils.py:179} INFO - Executing cmd: go mod init main
[2023-01-01T12:41:08.712+0100] {taskinstance.py:1782} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/providers/google/cloud/hooks/gcs.py", line 402, in provide_file
    yield tmp_file
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/providers/apache/beam/operators/beam.py", line 621, in execute
    self.beam_hook.start_go_pipeline(
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/providers/apache/beam/hooks/beam.py", line 339, in start_go_pipeline
    init_module("main", working_directory)
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/providers/google/go_module_utils.py", line 37, in init_module
    execute_in_subprocess(go_mod_init_cmd, cwd=go_module_path)
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/utils/process_utils.py", line 168, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/utils/process_utils.py", line 180, in execute_in_subprocess_with_kwargs
    with subprocess.Popen(
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/subprocess.py", line 969, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/subprocess.py", line 1845, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: '/var/folders/1_/7h5npt456j5f063tq7ngyxdw0000gn/T/apache-beam-gosmk3lv_4'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/providers/apache/beam/operators/beam.py", line 584, in execute
    with ExitStack() as exit_stack:
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/contextlib.py", line 576, in __exit__
    raise exc_details[1]
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/contextlib.py", line 561, in __exit__
    if cb(*exc_details):
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/johannaojeling/repo/johannaojeling/airflow/airflow/providers/google/cloud/hooks/gcs.py", line 399, in provide_file
    with NamedTemporaryFile(suffix=file_name, dir=dir) as tmp_file:
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/tempfile.py", line 502, in __exit__
    self.close()
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/tempfile.py", line 509, in close
    self._closer.close()
  File "/Users/johannaojeling/.pyenv/versions/3.10.6/lib/python3.10/tempfile.py", line 446, in close
    unlink(self.name)
FileNotFoundError: [Errno 2] No such file or directory: '/var/folders/1_/7h5npt456j5f063tq7ngyxdw0000gn/T/apache-beam-gosmk3lv_4/tmp6j9g5090main.go'
[2023-01-01T12:41:08.829+0100] {taskinstance.py:1321} INFO - Marking task as FAILED. dag_id=beam_go_dag, task_id=beam_go_pipeline, execution_date=20230101T000000, start_date=20230101T114106, end_date=20230101T114108
[...]
```

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/operators/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/beam/operators/test_beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py']
Ground Truth : ['a/airflow/providers/apache/beam/operators/beam.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Remove usage of `days_ago()` from test suite 
### Body

`days_ago()` is deprecated https://github.com/apache/airflow/pull/21653

There are many test using this function thus raising warnings.
This task is to replace all usages of `days_ago()` in the tests with alternatives.
A stale PR that started to work on it https://github.com/apache/airflow/pull/21830 but didn't finish

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/dates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/pubsub.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/psrp/hooks/psrp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py']
Ground Truth : ['a/airflow/example_dags/example_subdag_operator.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Improve TI modal layout on smaller screens
Improves the presentation of the Task Instance modal when the browser width is >992px (tablets, mobile). 

| Before | After |
|---|---|
|  <img width="489" alt="Image 2020-11-18 at 12 00 02 PM" src="https://user-images.githubusercontent.com/3267/99562683-69b16a00-2996-11eb-8609-75dabd4be7eb.png"> | <img width="489" alt="Image 2020-11-18 at 11 58 50 AM" src="https://user-images.githubusercontent.com/3267/99562695-6ae29700-2996-11eb-813c-b82666aa666f.png">  |




Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_dates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/sensors/test_spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/generative_model.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/pagerduty/hooks/pagerduty_events.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/setup_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/pagerduty/notifications/pagerduty.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/sensors/test_sqs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/generative_model.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/smtp/notifications/test_smtp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_runnable_exec_date_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/postgres/example_postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/pagerduty/hooks/pagerduty.py']
Ground Truth : ['a/airflow/providers/microsoft/mssql/hooks/mssql.py', 'a/airflow/providers/grpc/hooks/grpc.py', 'a/airflow/providers/mongo/hooks/mongo.py', 'a/airflow/providers/mysql/hooks/mysql.py', '/dev/null', 'a/airflow/providers/google/cloud/hooks/compute_ssh.py', 'a/airflow/providers/microsoft/azure/hooks/azure_cosmos.py', 'a/airflow/providers/salesforce/hooks/tableau.py', 'a/airflow/plugins_manager.py', 'a/airflow/providers/google/cloud/hooks/bigquery.py', 'a/airflow/providers/exasol/hooks/exasol.py', 'a/airflow/providers/apache/pig/hooks/pig.py', 'a/airflow/models/connection.py', 'a/airflow/providers/google/cloud/hooks/cloud_sql.py', 'a/airflow/providers/vertica/hooks/vertica.py', 'a/airflow/providers/microsoft/azure/hooks/azure_data_lake.py', 'a/airflow/providers/cncf/kubernetes/hooks/kubernetes.py', 'a/airflow/providers/google/cloud/hooks/dataprep.py', 'a/airflow/cli/commands/provider_command.py', 'a/airflow/providers/cloudant/hooks/cloudant.py', 'a/airflow/providers/postgres/hooks/postgres.py', 'a/airflow/providers/odbc/hooks/odbc.py', 'a/airflow/providers/oracle/hooks/oracle.py', 'a/airflow/providers/jdbc/hooks/jdbc.py', 'a/airflow/providers_manager.py', 'a/airflow/providers/microsoft/azure/hooks/wasb.py', 'a/airflow/providers/imap/hooks/imap.py', 'a/airflow/providers/sqlite/hooks/sqlite.py', 'a/airflow/providers/presto/hooks/presto.py', 'a/scripts/ci/pre_commit/pre_commit_check_provider_yaml_files.py', 'a/airflow/providers/snowflake/hooks/snowflake.py', 'a/airflow/providers/elasticsearch/hooks/elasticsearch.py', 'a/airflow/cli/cli_parser.py', 'a/airflow/providers/jira/hooks/jira.py', 'a/airflow/providers/docker/hooks/docker.py', 'a/airflow/providers/apache/hive/hooks/hive.py', 'a/airflow/providers/redis/hooks/redis.py', 'a/airflow/providers/microsoft/azure/hooks/azure_batch.py', 'a/airflow/providers/apache/cassandra/hooks/cassandra.py']
Current Recall: 0.006959314775160599

=========================================================

ISSUE: Facebook Ads Provider uses a deprecated version of the API
### Apache Airflow Provider(s)

facebook

### Versions of Apache Airflow Providers

2.0.1

### Apache Airflow version

2.1.1

### Operating System

Ubuntu 20.04

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

Task fails because the hook uses a deprecated Facebook API version. The hook is calling v6.0 which is longer supported.



### What you expected to happen

I expected this task to connected to the Facebook API and fetch the requested data. 

My log files for the failed task output the following message:

```
facebook_business.exceptions.FacebookRequestError: 

  Message: Call was not successful
  Method:  POST
  Path:    https://graph.facebook.com/v6.0/act_1210763848963620/insights
  Params:  {'level': 'ad', 'date_preset': 'yesterday', 'fields': '["campaign_name","campaign_id","ad_id","clicks","impressions"]'}

  Status:  400
  Response:
    {
      "error": {
        "message": "(#2635) You are calling a deprecated version of the Ads API. Please update to the latest version: v11.0.",
        "type": "OAuthException",
        "code": 2635,
        "fbtrace_id": "AGRidwR5VhjU3kAJVUSkvuz"
      }
    }
```

Line 69 of https://github.com/apache/airflow/blob/main/airflow/providers/facebook/ads/hooks/ads.py should be changed to a newer API version.

### How to reproduce

Run the sample DAG posted here: https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/example_dags/example_facebook_ads_to_gcs.html 

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/facebook/ads/hooks/ads.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/facebook_ads_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/kubernetes_engine.py']
Ground Truth : ['a/airflow/providers/facebook/ads/hooks/ads.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: API Endpoint - Health - Spec and impelementataion
**Description**

There is no endpoint to check if the instance is in good condition

We need to prepare the change to the API specification and then implement this change.

More information about API Endpoints:
https://github.com/apache/airflow/issues/8118

**Use case / motivation**

N/A

**Related Issues**

N/a

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/airbyte/hooks/airbyte.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataprep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/client/json_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/dbt/cloud/hooks/dbt.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py']
Ground Truth : ['a/airflow/api_connexion/endpoints/health_endpoint.py', '/dev/null']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: Security Views broken
Thanks, @KostyaEsmukov for finding this bug. I was able to replicate this bug.


**Apache Airflow version**: 1.10.10.rc3


**Environment**:

- **OS** (e.g. from /etc/os-release): MacOs
- **Kernel** (e.g. `uname -a`): `Darwin MacBook-Pro.local 19.3.0 Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64 x86_64`

**What happened**:

Most of the tabs under **Security** dropdown are broken:

![image](https://user-images.githubusercontent.com/8811558/78505788-a7e27c00-776d-11ea-9204-ac0b60810e3e.png)

For example when clicking on **List Users** above: it shows the following error:

```

                          ____/ (  (    )   )  \___
                         /( (  (  )   _    ))  )   )\
                       ((     (   )(    )  )   (   )  )
                     ((/  ( _(   )   (   _) ) (  () )  )
                    ( (  ( (_)   ((    (   )  .((_ ) .  )_
                   ( (  )    (      (  )    )   ) . ) (   )
                  (  (   (  (   ) (  _  ( _) ).  ) . ) ) ( )
                  ( (  (   ) (  )   (  ))     ) _)(   )  )  )
                 ( (  ( \ ) (    (_  ( ) ( )  )   ) )  )) ( )
                  (  (   (  (   (_ ( ) ( _    )  ) (  )  )   )
                 ( (  ( (  (  )     (_  )  ) )  _)   ) _( ( )
                  ((  (   )(    (     _    )   _) _(_ (  (_ )
                   (_((__(_(__(( ( ( |  ) ) ) )_))__))_)___)
                   ((__)        \\||lll|l||///          \_))
                            (   /(/ (  )  ) )\   )
                          (    ( ( ( | | ) ) )\   )
                           (   /(| / ( )) ) ) )) )
                         (     ( ((((_(|)_)))))     )
                          (      ||\(|(|)|/||     )
                        (        |(||(||)||||        )
                          (     //|/l|||)|\\ \     )
                        (/ / //  /|//||||\\  \ \  \ _)
-------------------------------------------------------------------------------
Node: 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/security/decorators.py", line 109, in wraps
    return f(self, *args, **kwargs)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/views.py", line 553, in list
    self.list_template, title=self.list_title, widgets=widgets
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/baseviews.py", line 281, in render_template
    template, **dict(list(kwargs.items()) + list(self.extra_args.items()))
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/templating.py", line 140, in render_template
    ctx.app,
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask/templating.py", line 120, in _render
    rv = template.render(context)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 76, in render
    return original_render(self, *args, **kwargs)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/environment.py", line 1008, in render
    return self.environment.handle_exception(exc_info, True)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/environment.py", line 780, in handle_exception
    reraise(exc_type, exc_value, tb)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/_compat.py", line 37, in reraise
    raise value.with_traceback(tb)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/general/model/list.html", line 2, in top-level template code
    {% import 'appbuilder/general/lib.html' as lib %}
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/environment.py", line 1005, in render
    return concat(self.root_render_func(self.new_context(vars)))
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/general/model/list.html", line 17, in root
    {% endblock %}
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/base.html", line 15, in root
  File "/Users/kaxilnaik/Documents/Github/apache/airflow/airflow/www_rbac/templates/airflow/master.html", line 16, in root
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html", line 17, in root
    {% include 'appbuilder/flash.html' %}
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/init.html", line 32, in root
    <link href="{{url_for('appbuilder.static',filename='select2/select2.css')}}" rel="stylesheet">
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html", line 37, in block_body
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/general/model/list.html", line 29, in block_content
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/general/model/list.html", line 56, in block_list_search
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/general/lib.html", line 783, in macro
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/templates/appbuilder/general/model/list.html", line 51, in macro
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/widgets.py", line 115, in __call__
    form_fields[col] = self.template_args["form"][col]()
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/wtforms/fields/core.py", line 155, in __call__
    return self.meta.render_field(self, kwargs)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/wtforms/meta.py", line 56, in render_field
    return field.widget(field, **render_kw)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/fieldwidgets.py", line 176, in __call__
    return super(Select2ManyWidget, self).__call__(field, **kwargs)
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/wtforms/widgets/core.py", line 323, in __call__
    for val, label, selected in field.iter_choices():
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/fields.py", line 208, in iter_choices
    for pk, obj in self._get_object_list():
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/fields.py", line 128, in _get_object_list
    objs = self.query_func()
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/forms.py", line 139, in <lambda>
    return lambda: self.datamodel.get_related_interface(col_name).query()[1]
  File "/Users/kaxilnaik/.virtualenvs/airflow_test_upgrade/lib/python3.7/site-packages/flask_appbuilder/models/sqla/interface.py", line 536, in get_related_interface
    return self.__class__(self.get_related_model(col_name), self.session)
TypeError: __init__() takes 2 positional arguments but 3 were given
```





Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/python_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_python_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/sbom_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/update_quarantined_test_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/hooks/test_package_index.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py']
Ground Truth : ['a/airflow/www/utils.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: S3ToSnowflakeTransfer enforces usage of schema parameter
Unlike all other Snowflake operators, S3ToSnowflakeTransfer requires the usage of schema instead of letting it be an optional variable that when passed would be used, and if not, then connection metadata is used. 

I can be assigned on this if needed. Keeping this up here so this doesn't go unnoticed. 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/context.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/sensors/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/gcs_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/operators/beam.py']
Ground Truth : ['a/airflow/providers/snowflake/transfers/s3_to_snowflake.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: docker container still running while dag run failed
### Apache Airflow version

2.1.4

### What happened

 I have operator run with docker . 

When dag run failed , docker.py try to remove container but remove failed and got the following error:

`2022-04-20 00:03:50,381] {taskinstance.py:1463} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 301, in _run_image_with_mounts
    for line in lines:
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/types/daemon.py", line 32, in __next__
    return next(self._stream)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 412, in <genexpr>
    gen = (data for (_, data) in gen)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/utils/socket.py", line 92, in frames_iter_no_tty
    (stream, n) = next_frame_header(socket)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/utils/socket.py", line 64, in next_frame_header
    data = read_exactly(socket, 8)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/utils/socket.py", line 49, in read_exactly
    next_data = read(socket, n - len(data))
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/utils/socket.py", line 29, in read
    select.select([socket], [], [])
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1238, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 268, in _raise_for_status
    response.raise_for_status()
  File "/home/airflow/.local/lib/python3.8/site-packages/requests/models.py", line 953, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 409 Client Error: Conflict for url: http+docker://localhost/v1.35/containers/de4cd812f8b0dcc448d591d1bd28fa736b1712237c8c8848919be512938bd515?v=False&link=False&force=False

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1165, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1283, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1313, in _execute_task
    result = task_copy.execute(context=context)
  File "/usr/local/airflow/dags/operators/byx_base_operator.py", line 611, in execute
    raise e
  File "/usr/local/airflow/dags/operators/byx_base_operator.py", line 591, in execute
    self.execute_job(context)
  File "/usr/local/airflow/dags/operators/byx_datax_operator.py", line 93, in execute_job
    result = call_datax.execute(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 343, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 265, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 317, in _run_image_with_mounts
    self.cli.remove_container(self.container['Id'])
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/container.py", line 1010, in remove_container
    self._raise_for_status(res)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/api/client.py", line 270, in _raise_for_status
    raise create_api_error_from_http_exception(e)
  File "/home/airflow/.local/lib/python3.8/site-packages/docker/errors.py", line 31, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation)
docker.errors.APIError: 409 Client Error for http+docker://localhost/v1.35/containers/de4cd812f8b0dcc448d591d1bd28fa736b1712237c8c8848919be512938bd515?v=False&link=False&force=False: Conflict ("You cannot remove a running container de4cd812f8b0dcc448d591d1bd28fa736b1712237c8c8848919be512938bd515. Stop the container before attempting removal or force remove")
`

### What you think should happen instead

the container should removed successful when dag run failed

### How to reproduce

step 1: create a dag with execute DockerOperator operation

step 2: trigger dag 

step 3: mark dag run to failed simulate dag run failed, and the remove container failed error will appear and the docker container still running.



### Operating System

NAME="Amazon Linux" VERSION="2" ID="amzn" ID_LIKE="centos rhel fedora" VERSION_ID="2" PRETTY_NAME="Amazon Linux 2"

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/testing_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py']
Ground Truth : ['a/airflow/providers/docker/operators/docker_swarm.py', 'a/airflow/providers/docker/operators/docker.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: Auto-refresh doesn't take into account the selected date
### Apache Airflow version

2.1.3

### Operating System

Debian GNU/Linux 9 (stretch)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

In the DAG tree view, if I select a custom date in the date filter (top left corner) and press "update", DAG runs are correctly filtered to the selected date and number of runs.
 
However, if the "auto-refresh" toggle is on, when the next tick refresh happens, the date filter is no longer taken into account and the UI displays the actual **latest** x DAG runs status. However, neither the header dates (45 angle) nor the date filter reflect this new time window 

I investigated in the network inspector and it seems that the xhr request that fetches dag runs status doesn't contain a date parameter and thus always fetch the latest DAG run data

### What you expected to happen

I expect that the auto-refresh feature preserves the selected time window

### How to reproduce

Open a DAG with at least 20 dag runs
Make sure autorefresh is disabled
Select a filter date earlier than the 10th dag run start date and a "number of runs" value of 10, press "update"
The DAG tree view should now be focused on the 10 first dag runs
Now toggle autorefresh and wait for next tick
The DAG tree view will now display status of the latest 10 runs but the header dates will still reflect the old start dates

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: Webserver does not exit upon gunicorn master crash
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 1.10.14 and 2.0.0

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): N/A

**Environment**:

- **Cloud provider or hardware configuration**: AWS, custom Docker image based on Debian buster
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`): Linux 9b57b2a952e3 4.14.193-149.317.amzn2.x86_64
- **Install tools**:
- **Others**:

**What happened**:

When gunicorn master process dies with all the workers, webserver fails to exit - instead it keeps logging the following message about every 10 seconds:

```
webserver_1  | [2021-01-04 22:32:47 +0000] [31] [INFO] Handling signal: ttou
webserver_1  | [2021-01-04 22:32:47 +0000] [82] [INFO] Worker exiting (pid: 82)
webserver_1  | [2021-01-04 22:32:57 +0000] [31] [INFO] Handling signal: term
webserver_1  | [2021-01-04 22:32:57 +0000] [95] [INFO] Worker exiting (pid: 95)
webserver_1  | [2021-01-04 22:32:57 +0000] [116] [INFO] Worker exiting (pid: 116)
webserver_1  | [2021-01-04 22:32:58 +0000] [31] [INFO] Shutting down: Master
webserver_1  | [2021-01-04 22:32:58,228] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:33:09,239] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:33:20,252] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:33:31,263] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:33:42,275] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:33:53,288] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:34:04,301] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:34:15,313] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:34:26,320] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:34:37,332] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:34:48,344] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:34:59,357] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:35:10,367] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:35:21,379] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:35:32,392] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:35:43,404] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
webserver_1  | [2021-01-04 22:35:54,414] {cli.py:1082} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
```

In the example above, I've killed the gunicorn master process intentionally: `kill 31`. In the real-world scenarios I've observed, the master process and the workers would crash due to some transient issue, such as temporary failure to fetch a secret.

**What you expected to happen**:

Airflow webserver should exit, so it can be restarted via systemd, docker deamon, or whatever else is managing the running services.


**How to reproduce it**:

Send KILL signal to the gunicorn master process.

**Anything else we need to know**:

I'll be providing a PR shortly with a fix for this issue.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/process_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/serve_logs.py']
Ground Truth : ['a/airflow/bin/cli.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: More natural sorting of DAG runs in the grid view 
### Apache Airflow version

2.3.2

### What happened

Dag with schedule to run once every hour.
Dag was started manually at 12:44, lets call this run 1
At 13:00 the scheduled run started, lets call this run 2. It appears before run 1 in the grid view.

See attached screenshot 
![image](https://user-images.githubusercontent.com/89977373/179212616-4113a1d5-ea61-4e0b-9c3f-3e4eba8318bc.png)


### What you think should happen instead

Dags in grid view should appear in the order they are started. 

### How to reproduce

_No response_

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

apache-airflow==2.3.2
apache-airflow-client==2.1.0
apache-airflow-providers-celery==3.0.0
apache-airflow-providers-cncf-kubernetes==4.0.2
apache-airflow-providers-docker==3.0.0
apache-airflow-providers-ftp==2.1.2
apache-airflow-providers-http==2.1.2
apache-airflow-providers-imap==2.2.3
apache-airflow-providers-postgres==5.0.0
apache-airflow-providers-sqlite==2.1.3

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/models/dag.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/timetables/simple.py', 'a/airflow/timetables/base.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: Unable to clear Failed task with retries

**Apache Airflow version**: 2.0.1


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): NA

**Environment**: Windows WSL2 (Ubuntu) Local

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): Ubuntu 18.04
- **Kernel** (e.g. `uname -a`): Linux d255bce4dcd5 5.4.72-microsoft-standard-WSL2
- **Install tools**: Docker -compose
- **Others**:

**What happened**:
I have a dag with tasks:
Task1 - Get Date
Task 2 - Get data from Api call (Have set retires to 3)
Task 3 - Load Data

Task 2 had failed after three attempts. I am unable to clear the task Instance and get the below error in UI. 

[Dag Code](https://github.com/anilkulkarni87/airflow-docker/blob/master/dags/covidNyDaily.py)

```
Python version: 3.8.7
Airflow version: 2.0.1rc2
Node: d255bce4dcd5
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/auth.py", line 34, in decorated
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/decorators.py", line 60, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/views.py", line 1547, in clear
    return self._clear_dag_tis(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/views.py", line 1475, in _clear_dag_tis
    count = dag.clear(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 65, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dag.py", line 1324, in clear
    clear_task_instances(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 160, in clear_task_instances
    ti.max_tries = ti.try_number + task_retries - 1
TypeError: unsupported operand type(s) for +: 'int' and 'str'
```

**What you expected to happen**:

I expected to clear the Task Instance so that the task could be scheduled again.

**How to reproduce it**:
1) Clone the repo link shared above
2) Follow instructions to setup cluster.
3) Change code to enforce error in Task 2 
4) Execute and try to clear task instance after three attempts.

![Error pops up when clicked on Clear](https://user-images.githubusercontent.com/10644132/107998258-8e1ee180-6f99-11eb-8442-0c0be5b23478.png)








Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py']
Ground Truth : ['a/airflow/models/baseoperator.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: Missing operator - CloudVisionReferenceImageCreateOperator
Hello,

We miss the operator for method delete_reference_image to hook CloudVisionHook. All other methods probably have matching operators.
This is important to me because the hook method refers to this operator in docstring.
https://github.com/apache/airflow/blob/1d36b0303b8632fce6de78ca4e782ae26ee06fea/airflow/providers/google/cloud/hooks/vision.py#L428

Best regards,
Kamil

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vision.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/pubsub.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/compute.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/compute.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/spanner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/functions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/datacatalog.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/cloud_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/transfers/copy_into_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_storage_transfer_service.py']
Ground Truth : ['a/airflow/providers/google/cloud/example_dags/example_vision.py', 'a/airflow/providers/google/cloud/hooks/vision.py', 'a/airflow/providers/google/cloud/operators/vision.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: serve_logs.py should respect the logging config's task handler's base_log_folder value
### Apache Airflow version

2.2.5

### What happened

in a worker container, a flask app is spun up to serve task log files that is read by the webserver and rendered to the user in the UI.  The log files cannot be read if you overwrite the task handler's base_log_folder value. (404 error)
 ie. in the airflow.cfg, the base_log_folder = `foo/bar/logs`, and the task handler uses `{base_log_folder}/dags`



### What you think should happen instead

in https://github.com/apache/airflow/blob/main/airflow/utils/serve_logs.py#L33, it should read the logging config's task handler log location.

### How to reproduce

use a custom logging config, override the task's base log folder. 
Run a dag and try to view logs in the ui, you will get a 404

```
LOGGING_CONFIG["handlers"].update(
    {
        "task": {
            "class":  "airflow.utils.log.file_task_handler.FileTaskHandler",
            "formatter": "airflow",
            "base_log_folder": f"{BASE_LOG_FOLDER}/dags",
            "filename_template": FILENAME_TEMPLATE,
            "filters": ["mask_secrets"],
        },
  }
```

### Operating System

ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/airflow_local_settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/serve_logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job_logging.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_processor_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/alibaba/cloud/log/oss_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/s3_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_log_endpoint.py']
Ground Truth : ['a/airflow/utils/serve_logs.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: DataprocCreateBatchOperator in deferrable mode doesn't reattach with deferment.  
### Apache Airflow version

main (development)

### What happened

The DataprocCreateBatchOperator (Google provider) handles the case when a batch_id already exists in the Dataproc API by 'reattaching' to a potentially running job.  

Current reattachment logic uses the non-deferrable method even when the operator is in deferrable mode.  

### What you think should happen instead

The operator should reattach in deferrable mode. 

### How to reproduce

Create a DAG with a task of DataprocCreateBatchOperator that is long running.  Make DataprocCreateBatchOperator deferrable in the constructor.  

Restart local Airflow to simulate having to 'reattach' to a running job in Google Cloud Dataproc. 

The operator resumes using the running job but in the code path for the non-derferrable logic.  

### Operating System

macOS 13.4.1 (22F82)

### Versions of Apache Airflow Providers

Current main.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/dbt/cloud/operators/dbt.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/dataproc/example_dataproc_batch_deferrable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py']
Ground Truth : ['a/airflow/providers/google/cloud/operators/dataproc.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: Add an AWS operator for Create RDS Database
### Description

@eladkal suggested we add the operator and then incorporate it into https://github.com/apache/airflow/pull/23681.  I have a little bit of a backlog right now trying to get the System Tests up and running for AWS, but if someone wants to get to it before me, it should be a pretty easy first contribution.

The required API call is documented [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.html#RDS.Client.create_db_instance) and I'm happy to help with any questions and./or help review it if someone wants to take a stab at it before I get the time.

### Use case/motivation

_No response_

### Related issues

Could be used to simplify https://github.com/apache/airflow/pull/23681

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/rds.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/rds.py', 'a/airflow/providers/amazon/aws/example_dags/example_dms.py']
Current Recall: 0.009100642398286937

=========================================================

ISSUE: Can't pass --without-mingle and --without-gossip to celery worker
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: ANY


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:

I've an issue with airflow celery worker performance, and I think there are a place for optimization if airflow enables providing --without-gossip while creating celery worker https://github.com/apache/airflow/blob/87038ae42aff3ff81cba1111e418a1f411a0c7b1/airflow/bin/cli.py#L1271 
https://docs.celeryproject.org/en/4.4.3/reference/celery.bin.worker.html?highlight=--without-gossip#cmdoption-celery-worker-without-gossip. In my case, I've a 200 vm and every vm running 2 queues, so 400 **workers exchanging useless message every second** which result in airflow worker is taking about 10% of cpu time (in 4 cores) and to compare, I've a standalone celery app that consume 0.1% of cpu time **after enabling --without-gossip --without-mingle --without-heartbeat**.

**What you expected to happen**:

celery worker should not take more than 2% of cpu time.

**How to reproduce it**:
Using 200 vm and every vm 2 celery workers running at same time, you will see that the celery worker process is taking ~10-15% of cpu time.

PLEASE NOTE:
Due to https://github.com/celery/celery/issues/2566, we can't provide --without-mingle and --without-gossip through configuration (config file).
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py']
Ground Truth : ['a/airflow/cli/commands/celery_command.py', 'a/airflow/cli/cli_parser.py']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: Building backport packages can be done inside Breeze container
**Description**

Currently, backport packages are built on the host, but we might easily switch to building them in the container. 

What would be needed:

- [ ] Change the script to run using docker run and breeze image rather than locally (Similarly to docs build or generate-requirements)
- [ ] Make sure MOUNT_SOURCE_DIR_FOR_STATIC_CHECKS is set to "true" and exported to mount all sources to inside the container
- [ ] Make sure the packages are generated in /dist folder rather than in ${AIRFLOW_SOURCES}/dist (local dist folder should be mapped there - check that indeed it is mapped when MOUNT_SOURCE_DIR_FOR_STATIC_CHECKS is "true"
- [ ] Make sure `bowler`, `wheel`, `sdist` is part of the pip-installed dependencies in the container 

**Use case / motivation**

It's not obvious that you need some requirements (bowler, wheel, sdist) and it would be great if you do not have to set up your local virtualenv with it to make it runs.

CC: @feluelle @turbaszek 


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_prepare_airflow_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/install_airflow_and_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py']
Ground Truth : ['a/setup.py', 'a/backport_packages/setup_backport_packages.py']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: expand_kwargs.map(func) gives unhelpful error message if func returns list
### Apache Airflow version

main (development)

### What happened

Here's a DAG:

```python3

with DAG(
    dag_id="expand_list",
    doc_md="try to get kwargs from a list",
    schedule_interval=None,
    start_date=datetime(2001, 1, 1),
) as expand_list:

    @expand_list.task
    def do_this():
        return [
            ("echo hello $USER", "USER", "foo"),
            ("echo hello $USER", "USER", "bar"),
        ]

    def mapper(tuple):
        if tuple[2] == "bar":
            return [1, 2, 3]
        else:
            return {"bash_command": tuple[0], "env": {tuple[1]: tuple[2]}}

    BashOperator.partial(task_id="one_cmd").expand_kwargs(do_this().map(mapper))
```

The `foo` task instance succeeds  as expected, and the `bar` task fails as expected.  But the error message that it gives isn't particularly helpful to a user who doesn't know what they did wrong:

```
ERROR - Failed to execute task: resolve() takes 3 positional arguments but 4 were given.
Traceback (most recent call last):
  File "/home/matt/src/airflow/airflow/executors/debug_executor.py", line 78, in _run_task
    ti.run(job_id=ti.job_id, **params)
  File "/home/matt/src/airflow/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/matt/src/airflow/airflow/models/taskinstance.py", line 1782, in run
    self._run_raw_task(
  File "/home/matt/src/airflow/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/matt/src/airflow/airflow/models/taskinstance.py", line 1445, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/home/matt/src/airflow/airflow/models/taskinstance.py", line 1580, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/home/matt/src/airflow/airflow/models/taskinstance.py", line 2202, in render_templates
    rendered_task = self.task.render_template_fields(context)
  File "/home/matt/src/airflow/airflow/models/mappedoperator.py", line 751, in render_template_fields
    unmapped_task = self.unmap(mapped_kwargs)
  File "/home/matt/src/airflow/airflow/models/mappedoperator.py", line 591, in unmap
    kwargs = self._expand_mapped_kwargs(resolve)
  File "/home/matt/src/airflow/airflow/models/mappedoperator.py", line 546, in _expand_mapped_kwargs
    return expand_input.resolve(*resolve)
TypeError: resolve() takes 3 positional arguments but 4 were given
```

### What you think should happen instead

Whatever checks the return value for mappability should do more to point the user to their error.  Perhaps something like:

> UnmappableDataError: Expected a dict with keys that BashOperator accepts, got `[1, 2, 3]` instead

### How to reproduce

Run the dag above

### Operating System

Linux 5.10.101 #1-NixOS SMP Wed Feb 16 11:54:31 UTC 2022 x86_64 GNU/Linux

### Versions of Apache Airflow Providers

n/a

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py']
Ground Truth : ['a/airflow/models/expandinput.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/decorators/base.py']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: Masking passwords with empty connection passwords make some logs unreadable in 2.1.0
Discovered in this [Slack conversation](https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1621752408213700).

When you have connections with empty passwords masking logs masks all the character breaks:
```
[2021-05-23 04:00:23,309] {{logging_mixin.py:104}} WARNING - ***-***-***-*** ***L***o***g***g***i***n***g*** ***e***r***r***o***r*** ***-***-***-***
[2021-05-23 04:00:23,309] {{logging_mixin.py:104}} WARNING - ***T***r***a***c***e***b***a***c***k*** ***(***m***o***s***t*** ***r***e***c***e***n***t*** ***c***a***l***l*** ***l***a***s***t***)***:***
[2021-05-23 04:00:23,309] {{logging_mixin.py:104}} WARNING - *** *** ***F***i***l***e*** ***"***/***u***s***r***/***l***o***c***a***l***/***l***i***b***/***p***y***t***h***o***n***3***.***8***/***l***o***g***g***i***n***g***/***_***_***i***n***i***t***_***_***.***p***y***"***,*** ***l***i***n***e*** ***1***0***8***1***,*** ***i***n*** ***e***m***i***t***
*** *** *** *** ***m***s***g*** ***=*** ***s***e***l***f***.***f***o***r***m***a***t***(***r***e***c***o***r***d***)***
```

Until this is fixed, an easy workaround is to disable masking via disabling sensitive connection masking in configuration:

```
[core]
hide_sensitive_var_conn_fields = False
```

or vial env variable:

```
AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS="False"
```

This is only happening if the task accesses the connection that has empty password. However there are a number of cases where such an empty password might be "legitimate" - for example in `google` provider you might authenticate using env variable or workload identity and connection will contain an empty password then.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dags/test_dagrun_fast_follow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_cli_util.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/parallel.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_impersonation_tests.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/cli_commands/definition.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/pinot/hooks/test_pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serde.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_serde.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_create_user_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py']
Ground Truth : ['a/airflow/utils/log/secrets_masker.py']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: AIP-45 breaks follow-on mini scheduler for mapped tasks
I've just noticed that this causes a problem for the follow-on mini scheduler for mapped tasks. I guess that code path wasn't sufficiently unit tested.

DAG

```python
import csv
import io
import os
import json
from datetime import datetime

from airflow import DAG
from airflow.decorators import task
from airflow.models.xcom_arg import XComArg
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator


with DAG(dag_id='mapped_s3', start_date=datetime(2022, 5, 19)) as dag:
    files = S3ListOperator(
        task_id="get_inputs",
        bucket="airflow-summit-2022",
        prefix="data_provider_a/{{ data_interval_end | ds }}/",
        delimiter='/',
        do_xcom_push=True,
    )

    @task
    def csv_to_json(aws_conn_id, input_bucket, key, output_bucket):
        hook = S3Hook(aws_conn_id=aws_conn_id)

        csv_data = hook.read_key(key, input_bucket)
        reader = csv.DictReader(io.StringIO(csv_data))

        output = io.BytesIO()

        for row in reader:
            output.write(json.dumps(row, indent=None).encode('utf-8'))
            output.write(b"\n")

        output.seek(0, os.SEEK_SET)
        hook.load_file_obj(output, key=key.replace(".csv", ".json"), bucket_name=output_bucket)

    csv_to_json.partial(
        aws_conn_id="aws_default", input_bucket=files.bucket, output_bucket="airflow-summit-2022-processed"
    ).expand(key=XComArg(files))
```

Error:

```
  File "/home/ash/code/airflow/airflow/airflow/jobs/local_task_job.py", line 253, in _run_mini_scheduler_on_child_tasks
    info = dag_run.task_instance_scheduling_decisions(session)
  File "/home/ash/code/airflow/airflow/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/home/ash/code/airflow/airflow/airflow/models/dagrun.py", line 658, in task_instance_scheduling_decisions
    schedulable_tis, changed_tis, expansion_happened = self._get_ready_tis(
  File "/home/ash/code/airflow/airflow/airflow/models/dagrun.py", line 714, in _get_ready_tis
    expanded_tis, _ = schedulable.task.expand_mapped_task(self.run_id, session=session)
  File "/home/ash/code/airflow/airflow/airflow/models/mappedoperator.py", line 609, in expand_mapped_task
    operator.mul, self._resolve_map_lengths(run_id, session=session).values()
  File "/home/ash/code/airflow/airflow/airflow/models/mappedoperator.py", line 591, in _resolve_map_lengths
    expansion_kwargs = self._get_expansion_kwargs()
  File "/home/ash/code/airflow/airflow/airflow/models/mappedoperator.py", line 526, in _get_expansion_kwargs
    return getattr(self, self._expansion_kwargs_attr)
AttributeError: 'MappedOperator' object has no attribute 'mapped_op_kwargs'
```

_Originally posted by @ashb in https://github.com/apache/airflow/issues/21877#issuecomment-1133409500_

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/transfers/test_dynamodb_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/sql_queries.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_s3.py']
Ground Truth : ['a/airflow/models/mappedoperator.py']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: Support DAGS folder being in different location on scheduler and worker.
**Description**

Dags folder in celery workers can be kept at different location as kept in Airflow scheduler node. Airflow task run can use dag full file path based on airflow home configuration on their node given dag file relative path to dags folder.

**Use case / motivation**

Currently scheduler uses absolute path for dags file to schedule task in task run command (`-sd` arg). This command makes hard to keep dags folder at different locations in celery workers which may be running in different environment. So using Airflow with celery with cross environment setup is difficult. 
This feature will help to keep dags folder at any location at worker nodes and tasks command can be configured for dag file path using home configuration.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/gcs/example_gcs_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_cli_util.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/visuals.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/models/dagbag.py', 'a/airflow/www/views.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/models/serialized_dag.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dag.py']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: Provider discovery based on entry_points is rather brittle
The tests we run in CI had shown that provider discovery based  on entry_points is rather brittle. 

Example here:

https://github.com/apache/airflow/pull/12466/checks?check_run_id=1467792592#step:9:4452

This is not a problem with Airflow, but wth PIP which might silently upgrade some packages and cause "version conflict" totally independently from Airflow configuration and totally out-of-our-control. 

Simple installing a whl package on top of the existing airflow installation (as it happened in the case above) might cause inconsistent requirements (in the case above installing .whl packages with all providers on top of existing Airflow installation caused the requests package to be upgraded to 2.25.0, even if airflow has the right requirements set. In this case it was (correct and it is from the "install_requires" section of airflow's setup.cfg):

```
Requirement.parse('requests<2.24.0,>=2.20.0'), {'apache-airflow'}
```

In case you have a version conflict in your env, running entry_point.load() from a package that has this version conflicts results with `pkg_resources.VersionConflict` error or `pkg_resources.ContextualVersionConflict) rather than returning the entry_point. Or at least that's what I observed so far. It's rather easy to reproduce. Simply install requests > 2.24.0 in the current airflow and see what happens.

So far I could not find a way to mitigate this problem, but @ashb - since you have more experience with it, maybe you can find a workaround for this?

I think we have a few options:

1) We fail 'airflow' hard if there is any Version Conflict. We have a way now after I've implemented ##10854  (and after @ephraimbuddy finishes the #12188 ) - we have a good, maintainable list of non-conflicting dependencies for Airflow and it's providers and we can keep that in the future thanks to pip-check. But I am afraid that will give a hard time to people who would like to install airflow with some custom dependencies (Tensorflow for example, depending on versions is notoriously difficult to sync with Airflow when it comes to dependencies). However, this is the most "Proper" (TM) solution.

2)  We find a workaround for the entry_point.load() VersionConflict exception. However, I think that might not be possible or easy looking for example at this SO thread: https://stackoverflow.com/questions/52982603/python-entry-point-fails-for-dependency-conflict . The most upvoted (=1) answer there starts with "Welcome to the world of dependencies hell! I know no clean way to solve this" - which is not very encouraging. I tried also to find it out from docs and code of the entry_point.load() but to no avail. @ashb - maybe you can help here.

3) We go back to the original implementation of mine where I read provider info from provider.yaml embedded into the package. This has disadvantage of being non-standard, but it works independently of version conflicts.


WDYT?




Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/docker_tests_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py']
Ground Truth : ['a/airflow/providers_manager.py', 'a/airflow/plugins_manager.py']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: Use Hash / SHA to compare Serialized DAGs
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

**Description**

<!-- A short description of your feature -->
Currently, we compare entire Serialized blobs to determine if the DAG changed or not before we write the Serialized DAGs to the Database which can be expensive.
Instead, we will store the HASH / SHA of the Serialized JSON and store that in a separate column in the serialized_dag table.
And before writing the Serialized Blob, we will compare the SHA to decide if DAG changed.



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/serialized_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py']
Ground Truth : ['a/airflow/models/serialized_dag.py', '/dev/null']
Current Recall: 0.010171306209850106

=========================================================

ISSUE: Task mapping against 'params'
### Apache Airflow version

2.3.1 (latest released)

### What happened

Importing a DAG using `PostgresOperator` with `expand(params=[...])` fails, claiming `params` was already specified as a partial argument, even though it wasn't.

### What you think should happen instead

The DAG imports successfully.

### How to reproduce

```python
from pathlib import Path
import pendulum
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.decorators import dag, task


@dag(
    start_date=pendulum.datetime(2021, 11, 19, tz="UTC"),
    schedule_interval="@daily",
    catchup=False,
)
def test():
    query_values = [{"a": 1}, {"a": 2}]

    PostgresOperator.partial(
        task_id="simple_select",
        sql="SELECT {{ params.a }}",
    ).expand(params=query_values)

test_dag = test()
```

Exception during import:
```
Broken DAG: [/usr/local/airflow/dags/test_dag.py] Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 199, in expand
    prevent_duplicates(self.kwargs, mapped_kwargs, fail_reason="mapping already partial")
  File "/usr/local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 139, in prevent_duplicates
    raise TypeError(f"{fail_reason} argument: {duplicated_keys.pop()}")
TypeError: mapping already partial argument: params
```

### Operating System

macOS 12.4

### Versions of Apache Airflow Providers

```apache-airflow-providers-postgres==4.1.0```

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py']
Ground Truth : ['a/airflow/models/param.py', 'a/airflow/models/taskinstance.py', 'a/airflow/decorators/base.py', 'a/airflow/models/abstractoperator.py', 'a/airflow/models/baseoperator.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/utils/context.py']
Current Recall: 0.010438972162740898

=========================================================

ISSUE: DruidOperator failing to submit ingestion tasks : Getting 500 error code from Druid
Issue : When trying to submit an ingestion task using DruidOperator, getting 500 error code in response from Druid. And can see no task submitted in Druid console.
In Airflow 1.10.x, everything is working fine. But when upgraded to 2.0.1, it is failing to submit the task. There is absolutely no change in the code/files except the import statements.

Resolution : I compared DruidOperator code for both Airflow 1.10.x & 2.0.1 and found one line causing the issue.
In Airflow 2.0.x, before submitting the indexing job json string is converted to python object. But it should be json string only.
In Airflow 1.10.x there is no conversion happening and hence it is working fine. (Please see below code snippets.)

I have already tried this change in my setup and re-ran the ingestion tasks. It is all working fine.

~~hook.submit_indexing_job(json.loads(self.json_index_file))~~
**hook.submit_indexing_job(self.json_index_file)**


Airflow 1.10.x - airflow/contrib/operators/druid_operator.py
```
def execute(self, context):
    hook = DruidHook(
        druid_ingest_conn_id=self.conn_id,
        max_ingestion_time=self.max_ingestion_time
    )
    self.log.info("Submitting %s", self.index_spec_str)
    hook.submit_indexing_job(self.index_spec_str)
```

Airflow 2.0.1 - airflow/providers/apache/druid/operators/druid.py
```
def execute(self, context: Dict[Any, Any]) -> None:
    hook = DruidHook(druid_ingest_conn_id=self.conn_id, max_ingestion_time=self.max_ingestion_time)
    self.log.info("Submitting %s", self.json_index_file)
    hook.submit_indexing_job(json.loads(self.json_index_file))
```


**Apache Airflow version**: 2.0.x

**Error Logs**:

```
[2021-02-24 06:42:24,287] {{connectionpool.py:452}} DEBUG - http://druid-master:8081 "POST /druid/indexer/v1/task HTTP/1.1" 500 15714
[2021-02-24 06:42:24,287] {{taskinstance.py:570}} DEBUG - Refreshing TaskInstance <TaskInstance: druid_compact_daily 2021-02-23T01:20:00+00:00 [running]> from DB
[2021-02-24 06:42:24,296] {{taskinstance.py:605}} DEBUG - Refreshed TaskInstance <TaskInstance: druid_compact_daily 2021-02-23T01:20:00+00:00 [running]>
[2021-02-24 06:42:24,298] {{taskinstance.py:1455}} ERROR - Did not get 200 when submitting the Druid job to http://druid-master:8081/druid/indexer/v1/task
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/druid/operators/druid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/druid/hooks/druid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/druid/hooks/test_druid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/apache/druid/example_druid_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/druid/transfers/hive_to_druid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py']
Ground Truth : ['a/airflow/providers/apache/druid/operators/druid.py']
Current Recall: 0.012580299785867237

=========================================================

ISSUE: WebUI: Action on selection in task instance list yields an error
**Apache Airflow version**: v2.0.0.dev0 (latest master)

**Environment**:
- **OS**: Ubuntu 18.04.4 LTS
- **Others**: Python 3.6.9

**What happened**:

Selecting a task in the the **task instance list** (*http:localhost:8080/taskinstance/list/*) and **performing an Action** on it (e.g. *Set state to 'failed'*) yields an error.

Error message:

```
Something bad has happened.
Please consider letting us know by creating a bug report using Github.

Python version: 3.6.12
Airflow version: 2.0.0.dev0
Node: emma
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/airflow/.local/lib/python3.6/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/home/airflow/.local/lib/python3.6/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/airflow/.local/lib/python3.6/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/views.py", line 686, in action_post
    self.datamodel.get(self._deserialize_pk_if_composite(pk)) for pk in pks
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/views.py", line 686, in <listcomp>
    self.datamodel.get(self._deserialize_pk_if_composite(pk)) for pk in pks
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/models/sqla/interface.py", line 870, in get
    query, _filters, select_columns=select_columns
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/models/sqla/interface.py", line 324, in apply_all
    select_columns,
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/models/sqla/interface.py", line 272, in _apply_inner_all
    query = self.apply_filters(query, inner_filters)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/models/sqla/interface.py", line 162, in apply_filters
    return filters.apply_all(query)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/models/filters.py", line 295, in apply_all
    query = flt.apply(query, value)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/models/sqla/filters.py", line 137, in apply
    query, field = get_field_setup_query(query, self.model, self.column_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_appbuilder/models/sqla/filters.py", line 40, in get_field_setup_query
    if not hasattr(model, column_name):
TypeError: hasattr(): attribute name must be string
```

**How to reproduce it**:

I wanted to take an Action on a task instance of a DAG with `schedule_interval=None`. I am attaching a minimal DAG file used for reproducing this error. 

<details>
<summary>DAG file</summary>

```
from airflow import DAG
from datetime import timedelta, datetime
from airflow.operators.bash_operator import BashOperator

dag = DAG(
    'simple_dag',
    default_args= {
        'owner': 'airflow',
        'depends_on_past': False,
        'retries' : 0,
        'start_date': datetime(1970, 1, 1),
        'retry_delay': timedelta(seconds=30),
    },
    description='',
    schedule_interval=None,
    catchup=False,
)

t1 = BashOperator(
    task_id='task1',
    bash_command='echo 1',
    dag=dag
)
```

</details>

**Anything else we need to know**:

Taking the same Action on the DagRun list *(http://localhost:8080/dagrun/list)* works.

Great project btw . Really enjoying using it.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/beam/hooks/test_beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py']
Ground Truth : ['a/airflow/www/utils.py']
Current Recall: 0.012580299785867237

=========================================================

ISSUE: filesensor wildcard matching does not recognize directories
**Apache Airflow version**: 2.1.0


**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:  FileSensor does not recognize directories with wildcard glob matching.

**What you expected to happen**: FileSensor would sense a directory that contains files if it matches with the wild card option.

**How to reproduce it**: Create a directory with a pattern that matches a wild card using glob



**Anything else we need to know**: Code from FileSensor source that I believe to cause the issue:

```
for path in glob(full_path):
            if os.path.isfile(path):
                mod_time = os.path.getmtime(path)
                mod_time = datetime.datetime.fromtimestamp(mod_time).strftime('%Y%m%d%H%M%S')
                self.log.info('Found File %s last modified: %s', str(path), str(mod_time))
                return True

            for _, _, files in os.walk(full_path):
                if len(files) > 0:
                    return True
        return False

```
I believe to resolve the issue `full_path` in os.walk should be `path` instead.



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/filesystem.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/triggers/file.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/hooks/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/sensors/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/triggers/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/file.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/system_tests_class.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/sftp/hooks/test_sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ftp/sensors/ftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/gcs_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py']
Ground Truth : ['a/airflow/sensors/filesystem.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: Airflow 2.3.0: can't filter by owner if selected from dropdown
### Apache Airflow version

2.3.0 (latest released)

### What happened

On a clean install of 2.3.0, whenever I try to filter by owner, if I select it from the dropdown (which correctly detects the owner's name) it returns the following error:

`DAG "ecodina" seems to be missing from DagBag.`
Webserver's log:
```
127.0.0.1 - - [12/May/2022:12:27:47 +0000] "GET /dagmodel/autocomplete?query=ecodin&status=all HTTP/1.1" 200 17 "http://localhost/home?search=ecodina" "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"
127.0.0.1 - - [12/May/2022:12:27:50 +0000] "GET /dags/ecodina/grid?search=ecodina HTTP/1.1" 302 217 "http://localhost/home?search=ecodina" "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"
127.0.0.1 - - [12/May/2022:12:27:50 +0000] "GET /home HTTP/1.1" 200 35774 "http://localhost/home?search=ecodina" "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"
127.0.0.1 - - [12/May/2022:12:27:50 +0000] "POST /blocked HTTP/1.1" 200 2 "http://localhost/home" "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"
127.0.0.1 - - [12/May/2022:12:27:50 +0000] "POST /last_dagruns HTTP/1.1" 200 402 "http://localhost/home" "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"
127.0.0.1 - - [12/May/2022:12:27:50 +0000] "POST /dag_stats HTTP/1.1" 200 333 "http://localhost/home" "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"
127.0.0.1 - - [12/May/2022:12:27:50 +0000] "POST /task_stats HTTP/1.1" 200 1194 "http://localhost/home" "Mozilla/5.0 (X11; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0"
```

Instead, if I write the owner's name fully and avoid selecting it from the dropdown, it works as expected since it constructs the correct URL:

`my.airflow.com/home?search=ecodina`

### What you think should happen instead

The DAGs table should only show the selected owner's DAGs.

### How to reproduce

- Start the Airflow Webserver
- Connect to the Airflow webpage
- Type an owner name in the _Search DAGs_ textbox and select it from the dropdown

### Operating System

CentOS Linux 8

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

Installed on a conda environment, as if it was a virtualenv:

- `conda create -c conda-forge -n airflow python=3.9`
- `conda activate airflow`
- `pip install "apache-airflow[postgres]==2.3.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.3.0/constraints-3.9.txt"`

Database: PostgreSQL 13


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/main_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_trigger_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_trigger_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_ec2.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_acl.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: dynamic dataset ref breaks when viewed in UI or when triggered (dagbag.py:_add_dag_from_db)
### Apache Airflow version

2.4.0b1

### What happened

Here's a file which defines three dags.  "source" uses `Operator.partial` to reference either "sink".  I'm not sure if it's supported to do so, but airlflow should at least fail more gracefully than it does.

```python3
from datetime import datetime, timedelta
from time import sleep

from airflow import Dataset
from airflow.decorators import dag
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator

ps = Dataset("partial_static")
p1 = Dataset("partial_dynamic_1")
p2 = Dataset("partial_dynamic_2")
p3 = Dataset("partial_dynamic_3")

def sleep_n(n):
    sleep(n)

@dag(start_date=datetime(1970, 1, 1), schedule=timedelta(days=365 * 30))
def two_kinds_dynamic_source():

    # dataset ref is not dynamic
    PythonOperator.partial(
        task_id="partial_static", python_callable=sleep_n, outlets=[ps]
    ).expand(op_args=[[1], [20], [40]])

    # dataset ref is dynamic
    PythonOperator.partial(
        task_id="partial_dynamic", python_callable=sleep_n
    ).expand_kwargs(
        [
            {"op_args": [1], "outlets": [p1]},
            {"op_args": [20], "outlets": [p2]},
            {"op_args": [40], "outlets": [p3]},
        ]
    )

two_kinds_dynamic_source()


@dag(schedule=[ps], start_date=datetime(1970, 1, 1))
def two_kinds_static_sink():
    DummyOperator(task_id="dummy")

two_kinds_static_sink()


@dag(schedule=[p1, p2, p3], start_date=datetime(1970, 1, 1))
def two_kinds_dynamic_sink():
    DummyOperator(task_id="dummy")

two_kinds_dynamic_sink()

```

Tried to trigger the dag in the browser, saw this traceback instead:

```
Python version: 3.9.13
Airflow version: 2.4.0.dev1640+astro.1
Node: airflow-webserver-6b969cbd87-4q5kh
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 2525, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1822, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1820, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1796, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/usr/local/lib/python3.9/site-packages/airflow/www/auth.py", line 46, in decorated
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/www/decorators.py", line 117, in view_func
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/www/decorators.py", line 80, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/www/views.py", line 2532, in grid
    dag = get_airflow_app().dag_bag.get_dag(dag_id, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 176, in get_dag
    self._add_dag_from_db(dag_id=dag_id, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 251, in _add_dag_from_db
    dag = row.dag
  File "/usr/local/lib/python3.9/site-packages/airflow/models/serialized_dag.py", line 223, in dag
    dag = SerializedDAG.from_dict(self.data)  # type: Any
  File "/usr/local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 1220, in from_dict
    return cls.deserialize_dag(serialized_obj['dag'])
  File "/usr/local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 1197, in deserialize_dag
    setattr(task, k, kwargs_ref.deref(dag))
  File "/usr/local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 224, in deref
    value = {k: v.deref(dag) if isinstance(v, _XComRef) else v for k, v in self.value.items()}
AttributeError: 'list' object has no attribute 'items'
```

I can also summon a similar traceback by just trying to view the dag in the grid view, or when running `airflow dags trigger`

### What you think should happen instead

If there's something invalid about this dag, it should fail to parse--rather than successfully parsing and then breaking the UI.

I'm a bit uncertain about what should happen in the dag dependency graph when the source dag runs.  The dynamic outlets are not known until runtime, so it's reasonable that they don't show up in the graph.  But what about after the dag runs?
- do they still trigger the "sink" dag even though we didn't know about the dependency up front?
- do we update the dependency graph now that we know about the dynamic dependency?

Because of this error, we don't get far enough to find out.

### How to reproduce

Include the dag above, try to display it in the grid view.

### Operating System

kubernetes-in-docker on MacOS via helm

### Versions of Apache Airflow Providers

n/a

### Deployment

Other 3rd-party Helm chart

### Deployment details

Deployed using [the astronomer helm chart ](https://github.com/astronomer/airflow-chart)and these values:
```yaml
airflow:
  airflowHome: /usr/local/airflow
  airflowVersion: $VERSION
  defaultAirflowRepository: img
  defaultAirflowTag: $TAG
  executor: KubernetesExecutor
  gid: 50000
  images:
    airflow:
      repository: img
  logs:
    persistence:
      enabled: true
      size: 2Gi
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py']
Ground Truth : ['a/airflow/serialization/serialized_objects.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: mini-scheduler raises AttributeError: 'NoneType' object has no attribute 'keys'
### Apache Airflow version

2.3.2 (latest released)

### What happened

The mini-scheduler run after a task finishes sometimes fails with an error "AttributeError: 'NoneType' object has no attribute 'keys'"; see full traceback below.

### What you think should happen instead

_No response_

### How to reproduce

The minimal reproducing example I could find is this:

```python
import pendulum
from airflow.models import BaseOperator
from airflow.utils.task_group import TaskGroup
from airflow.decorators import task
from airflow import DAG

@task
def task0():
    pass

class Op0(BaseOperator):
    template_fields = ["some_input"]

    def __init__(self, some_input, **kwargs):
        super().__init__(**kwargs)
        self.some_input = some_input

if __name__ == "__main__":
    with DAG("dag0", start_date=pendulum.now()) as dag:
        with TaskGroup(group_id="tg1"):
            Op0(task_id="task1", some_input=task0())
    dag.partial_subset("tg1.task1")
```

Running this script with airflow 2.3.2 produces this traceback:

```
Traceback (most recent call last):
  File "/app/airflow-bug-minimal.py", line 22, in <module>
    dag.partial_subset("tg1.task1")
  File "/venv/lib/python3.10/site-packages/airflow/models/dag.py", line 2013, in partial_subset
    dag.task_dict = {
  File "/venv/lib/python3.10/site-packages/airflow/models/dag.py", line 2014, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/venv/lib/python3.10/site-packages/airflow/models/dag.py", line 2011, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/local/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1156, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1000, in __setattr__
    self.set_xcomargs_dependencies()
  File "/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1107, in set_xcomargs_dependencies
    XComArg.apply_upstream_relationship(self, arg)
  File "/venv/lib/python3.10/site-packages/airflow/models/xcom_arg.py", line 186, in apply_upstream_relationship
    op.set_upstream(ref.operator)
  File "/venv/lib/python3.10/site-packages/airflow/models/taskmixin.py", line 241, in set_upstream
    self._set_relatives(task_or_task_list, upstream=True, edge_modifier=edge_modifier)
  File "/venv/lib/python3.10/site-packages/airflow/models/taskmixin.py", line 185, in _set_relatives
    dags: Set["DAG"] = {task.dag for task in [*self.roots, *task_list] if task.has_dag() and task.dag}
  File "/venv/lib/python3.10/site-packages/airflow/models/taskmixin.py", line 185, in <setcomp>
    dags: Set["DAG"] = {task.dag for task in [*self.roots, *task_list] if task.has_dag() and task.dag}
  File "/venv/lib/python3.10/site-packages/airflow/models/dag.py", line 508, in __hash__
    val = tuple(self.task_dict.keys())
AttributeError: 'NoneType' object has no attribute 'keys'
```

Note that the call to `dag.partial_subset` usually happens in the mini-scheduler: https://github.com/apache/airflow/blob/2.3.2/airflow/jobs/local_task_job.py#L253


### Operating System

Linux (Debian 9)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_python_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskmixin.py']
Ground Truth : ['a/airflow/models/baseoperator.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: CLI `airflow scheduler -D --pid <PIDFile>` fails silently if PIDFile given is a relative path


<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 2.0.0

**Environment**: Linux & MacOS, venv

- **OS** (e.g. from /etc/os-release): Ubuntu 18.04.3 LTS / MacOS 10.15.7 
- **Kernel** (e.g. `uname -a`):
  - Linux *** 5.4.0-1029-aws #30~18.04.1-Ubuntu SMP Tue Oct 20 11:09:25 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
  - Darwin *** 19.6.0 Darwin Kernel Version 19.6.0: Thu Oct 29 22:56:45 PDT 2020; root:xnu-6153.141.2.2~1/RELEASE_X86_64 x86_64

**What happened**:

Say I'm in my home dir, running command `airflow scheduler -D --pid test.pid` (`test.pid` is a relative path) is supposed to start the scheduler in daemon mode, and the PID will be stored in the file `test.pid` (if it doesn't exist, it should be created).

However, the scheduler is NOT started. This can be validated by running `ps aux | grep airflow | grep scheduler` (no process is shown). In the whole process, I don't see any error message.

However, if I change the pid file path to an absolute path, i.e. `airflow scheduler -D --pid ${PWD}/test.pid`, it successfully start the scheduler in daemon mode (can be validated via the method above).

**What you expected to happen**:

Even if the PID file path provided is a relative path, the scheduler should be started properly as well.

<!-- What do you think went wrong? -->

**How to reproduce it**:

Described above

<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py']
Ground Truth : ['a/airflow/utils/cli.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: Add ssl extra arguments to postgres hook
This enables one to specify items such as ssl certificate paths in the extra field of a postgres connection object. Rather than simply pass-through everything from the extra field, I followed the existing code pattern of selecting for specific arguments, as demonstrated in other hooks. I tested this in Python 2.7.6 and Python 2.7.10.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/docker/hooks/test_docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/smtp/hooks/smtp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mongo/hooks/mongo.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/http_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/psrp/hooks/psrp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/teradata/hooks/teradata.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/s3_to_redshift.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/sql_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/postgres/hooks/postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/azure_blob_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/redis/hooks/redis.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/hooks/postgres_hook.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: Create NoAdditionalArgsInOperatorsRule to ease upgrade to Airflow 2.0

This issue is part of #8765

## Rule
Create `NoAdditionalArgsInOperatorsRule` which corresponds to
> Additional arguments passed to BaseOperator cause an exception

entry in UPDATING.md. This rule should allow users to check if their current configuration needs any adjusting
before migration to Airflow 2.0.

## How to guide
To implement a new rule, create a class that inherits from `airflow.upgrade.rules.base_rule.BaseRule`.
It will be auto-registered and used by `airflow upgrade-check` command. The custom rule class has to have `title`,
`description` properties and should implement `check` method which returns a list of error messages in case of
incompatibility.

For example:
https://github.com/apache/airflow/blob/ea36166961ca35fc51ddc262ec245590c3e236fb/airflow/upgrade/rules/conn_type_is_not_nullable.py#L25-L42

**Remember** to open the PR against `v1-10-test` branch.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/file.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py']
Ground Truth : ['/dev/null']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: Move Kubernetes tests and kind setup out of the container to host
**Description**

We should move kind setup out of the container for CI to the host environment

**Use case / motivation**

Makes CI image smaller and setup easier for local testing.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/main_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/configure_rich_click.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_impersonation_tests.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/docker/operators/test_docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/visuals.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py']
Ground Truth : ['a/airflow/kubernetes/volume_mount.py', 'a/airflow/kubernetes/refresh_config.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: Add detailed documentation for scheduler "tuning" parameters
https://github.com/apache/airflow/pull/12139 added some new params to the scheduler -- we have "cursory" docs for them in the comments in the config file, but we should have a more details docs in the scheduler.rst page.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_ec2.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/ads/hooks/ads.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/weaviate/hooks/weaviate.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: Custom Timetable error with CeleryExecutor
### Apache Airflow version

2.2.3 (latest released)

### What happened

i'm really new to airflow, and i have an error when using Custom Timetable w/ CeleryExecutor

### What you expected to happen

For the custom timetable to be implemented and used by DAG.

error log as follows:

```
[2021-12-23 05:44:30,843: WARNING/ForkPoolWorker-2] Running <TaskInstance: example_cron_trivial_dag.dummy scheduled__2021-12-23T05:44:00+00:00 [queued]> on host 310bbd362d25
[2021-12-23 05:44:30,897: ERROR/ForkPoolWorker-2] Failed to execute task Not a valid timetable: <cron_trivial_timetable.CronTrivialTimetable object at 0x7fc15a0a4100>.
Traceback (most recent call last):
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/executors/celery_executor.py", line 121, in _execute_in_fork
    args.func(args)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 105, in _run_task_by_selected_method
    _run_task_by_local_task_job(args, ti)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 163, in _run_task_by_local_task_job
    run_job.run()
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/jobs/base_job.py", line 245, in run
    self._execute()
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/jobs/local_task_job.py", line 78, in _execute
    self.task_runner = get_task_runner(self)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/task/task_runner/__init__.py", line 63, in get_task_runner
    task_runner = task_runner_class(local_task_job)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 35, in __init__
    super().__init__(local_task_job)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/task/task_runner/base_task_runner.py", line 48, in __init__
    super().__init__(local_task_job.task_instance)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 40, in __init__
    self._set_context(context)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 54, in _set_context
    set_context(self.log, context)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/utils/log/logging_mixin.py", line 178, in set_context
    handler.set_context(value)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/utils/log/file_task_handler.py", line 59, in set_context
    local_loc = self._init_file(ti)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/utils/log/file_task_handler.py", line 264, in _init_file
    relative_path = self._render_filename(ti, ti.try_number)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/utils/log/file_task_handler.py", line 80, in _render_filename
    context = ti.get_template_context()
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1912, in get_template_context
    'next_ds': get_next_ds(),
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1868, in get_next_ds
    execution_date = get_next_execution_date()
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1862, in get_next_execution_date
    next_execution_date = dag.following_schedule(self.execution_date)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/models/dag.py", line 595, in following_schedule
    data_interval = self.infer_automated_data_interval(timezone.coerce_datetime(dttm))
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/models/dag.py", line 678, in infer_automated_data_interval
    raise ValueError(f"Not a valid timetable: {self.timetable!r}")
ValueError: Not a valid timetable: <cron_trivial_timetable.CronTrivialTimetable object at 0x7fc15a0a4100>
[2021-12-23 05:44:30,936: ERROR/ForkPoolWorker-2] Task airflow.executors.celery_executor.execute_command[7c76904d-1b61-4441-b5f0-96ef2ba7c3b7] raised unexpected: AirflowException('Celery command failed on host: 310bbd362d25')
Traceback (most recent call last):
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/celery/app/trace.py", line 451, in trace_task
    R = retval = fun(*args, **kwargs)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/celery/app/trace.py", line 734, in __protected_call__
    return self.run(*args, **kwargs)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/executors/celery_executor.py", line 90, in execute_command
    _execute_in_fork(command_to_exec, celery_task_id)
  File "/opt/rh/rh-python38/root/usr/local/lib/python3.8/site-packages/airflow/executors/celery_executor.py", line 101, in _execute_in_fork
    raise AirflowException('Celery command failed on host: ' + get_hostname())
airflow.exceptions.AirflowException: Celery command failed on host: 310bbd362d25
```

seems like the same problem mentioned here, but by different reasons that i really don't understand
https://github.com/apache/airflow/issues/19578#issuecomment-974654209

### How to reproduce

before 2.2.3 release, i'm having DAG import error w/ custom timetables, and since i only want Next Dag Run on UI be exact time, i will just change the code of CronDataIntervalTimetable by returning DataInterval.exact(end) in next_dagrun_info and infer_manual_data_interval.

here's the new timetable plugin file that i created (simply copied and tweaked from CronDataIntervalTimetable):

```
import datetime
from typing import Any, Dict, Optional, Union

from croniter import CroniterBadCronError, CroniterBadDateError, croniter
from dateutil.relativedelta import relativedelta
from pendulum import DateTime
from pendulum.tz.timezone import Timezone

from airflow.plugins_manager import AirflowPlugin
from airflow.compat.functools import cached_property
from airflow.exceptions import AirflowTimetableInvalid
from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable
from airflow.utils.dates import cron_presets
from airflow.utils.timezone import convert_to_utc, make_aware, make_naive

Delta = Union[datetime.timedelta, relativedelta]

class _TrivialTimetable(Timetable):
    """Basis for timetable implementations that schedule data intervals.

    This kind of timetable classes create periodic data intervals (exact times) from an
    underlying schedule representation (e.g. a cron expression, or a timedelta
    instance), and schedule a DagRun at the end of each interval (exact time).
    """

    def _skip_to_latest(self, earliest: Optional[DateTime]) -> DateTime:
        """Bound the earliest time a run can be scheduled.

        This is called when ``catchup=False``.
        """
        raise NotImplementedError()

    def _align(self, current: DateTime) -> DateTime:
        """Align given time to the scheduled.

        For fixed schedules (e.g. every midnight); this finds the next time that
        aligns to the declared time, if the given time does not align. If the
        schedule is not fixed (e.g. every hour), the given time is returned.
        """
        raise NotImplementedError()

    def _get_next(self, current: DateTime) -> DateTime:
        """Get the first schedule after the current time."""
        raise NotImplementedError()

    def _get_prev(self, current: DateTime) -> DateTime:
        """Get the last schedule before the current time."""
        raise NotImplementedError()

    def next_dagrun_info(
        self,
        *,
        last_automated_data_interval: Optional[DataInterval],
        restriction: TimeRestriction,
    ) -> Optional[DagRunInfo]:
        earliest = restriction.earliest
        if not restriction.catchup:
            earliest = self._skip_to_latest(earliest)
        if last_automated_data_interval is None:
            # First run; schedule the run at the first available time matching
            # the schedule, and retrospectively create a data interval for it.
            if earliest is None:
                return None
            start = self._align(earliest)
        else:
            # There's a previous run. Create a data interval starting from when
            # the end of the previous interval.
            start = last_automated_data_interval.end
        if restriction.latest is not None and start > restriction.latest:
            return None
        end = self._get_next(start)
        return DagRunInfo.exact(end)

def _is_schedule_fixed(expression: str) -> bool:
    """Figures out if the schedule has a fixed time (e.g. 3 AM every day).

    :return: True if the schedule has a fixed time, False if not.

    Detection is done by "peeking" the next two cron trigger time; if the
    two times have the same minute and hour value, the schedule is fixed,
    and we *don't* need to perform the DST fix.

    This assumes DST happens on whole minute changes (e.g. 12:59 -> 12:00).
    """
    cron = croniter(expression)
    next_a = cron.get_next(datetime.datetime)
    next_b = cron.get_next(datetime.datetime)
    return next_b.minute == next_a.minute and next_b.hour == next_a.hour

class CronTrivialTimetable(_TrivialTimetable):
    """Timetable that schedules data intervals with a cron expression.

    This corresponds to ``schedule_interval=<cron>``, where ``<cron>`` is either
    a five/six-segment representation, or one of ``cron_presets``.

    The implementation extends on croniter to add timezone awareness. This is
    because crontier works only with naive timestamps, and cannot consider DST
    when determining the next/previous time.

    Don't pass ``@once`` in here; use ``OnceTimetable`` instead.
    """

    def __init__(self, cron: str, timezone: Timezone) -> None:
        self._expression = cron_presets.get(cron, cron)
        self._timezone = timezone

    @classmethod
    def deserialize(cls, data: Dict[str, Any]) -> "Timetable":
        from airflow.serialization.serialized_objects import decode_timezone

        return cls(data["expression"], decode_timezone(data["timezone"]))

    def __eq__(self, other: Any) -> bool:
        """Both expression and timezone should match.

        This is only for testing purposes and should not be relied on otherwise.
        """
        if not isinstance(other, CronTrivialTimetable):
            return NotImplemented
        return self._expression == other._expression and self._timezone == other._timezone

    @property
    def summary(self) -> str:
        return self._expression
   
    def serialize(self) -> Dict[str, Any]:
        from airflow.serialization.serialized_objects import encode_timezone

        return {"expression": self._expression, "timezone": encode_timezone(self._timezone)}
   
    def validate(self) -> None:
        try:
            croniter(self._expression)
        except (CroniterBadCronError, CroniterBadDateError) as e:
            raise AirflowTimetableInvalid(str(e))
   
    @cached_property
    def _should_fix_dst(self) -> bool:
        # This is lazy so instantiating a schedule does not immediately raise
        # an exception. Validity is checked with validate() during DAG-bagging.
        return not _is_schedule_fixed(self._expression)
   
    def _get_next(self, current: DateTime) -> DateTime:
        """Get the first schedule after specified time, with DST fixed."""
        naive = make_naive(current, self._timezone)
        cron = croniter(self._expression, start_time=naive)
        scheduled = cron.get_next(datetime.datetime)
        if not self._should_fix_dst:
            return convert_to_utc(make_aware(scheduled, self._timezone))
        delta = scheduled - naive
        return convert_to_utc(current.in_timezone(self._timezone) + delta)

    def _get_prev(self, current: DateTime) -> DateTime:
        """Get the first schedule before specified time, with DST fixed."""
        naive = make_naive(current, self._timezone)
        cron = croniter(self._expression, start_time=naive)
        scheduled = cron.get_prev(datetime.datetime)
        if not self._should_fix_dst:
            return convert_to_utc(make_aware(scheduled, self._timezone))
        delta = naive - scheduled
        return convert_to_utc(current.in_timezone(self._timezone) - delta)

    def _align(self, current: DateTime) -> DateTime:
        """Get the next scheduled time.

        This is ``current + interval``, unless ``current`` is first interval,
        then ``current`` is returned.
        """
        next_time = self._get_next(current)
        if self._get_prev(next_time) != current:
            return next_time
        return current

    def _skip_to_latest(self, earliest: Optional[DateTime]) -> DateTime:
        """Bound the earliest time a run can be scheduled.

        The logic is that we move start_date up until one period before, so the
        current time is AFTER the period end, and the job can be created...
        """
        current_time = DateTime.utcnow()
        next_start = self._get_next(current_time)
        last_start = self._get_prev(current_time)
        if next_start == current_time:
            new_start = last_start
        elif next_start > current_time:
            new_start = self._get_prev(last_start)
        else:
            raise AssertionError("next schedule shouldn't be earlier")
        if earliest is None:
            return new_start
        return max(new_start, earliest)

    def infer_manual_data_interval(self, *, run_after: DateTime) -> DataInterval:
        # Get the last complete period before run_after, e.g. if a DAG run is
        # scheduled at each midnight, the data interval of a manually triggered
        # run at 1am 25th is between 0am 24th and 0am 25th.
        end = self._get_prev(self._align(run_after))
        return DataInterval.exact(end)

class CronTrivialTimetablePlugin(AirflowPlugin):
    name = "cron_trivial_timetable_plugin"
    timetables = [CronTrivialTimetable]
```

the dag file i'm using/testing:

```
import pendulum
from datetime import datetime, timedelta
from typing import Dict

from airflow.decorators import task
from airflow.models import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator

from cron_trivial_timetable import CronTrivialTimetable

with DAG(
    dag_id="example_cron_trivial_dag",
    start_date=datetime(2021,11,14,12,0,tzinfo=pendulum.timezone('Asia/Tokyo')),
    max_active_runs=1,
    timetable=CronTrivialTimetable('*/2 * * * *', pendulum.timezone('Asia/Tokyo')),
    default_args={
        'owner': '********',
        'depends_on_past': False,
        'email': [**************],
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 0,
        'retry_delay': timedelta(seconds=1),
        'end_date': datetime(2101, 1, 1),
    },
    tags=['testing'],
    catchup=False
) as dag:

    dummy = BashOperator(task_id='dummy', queue='daybatch', bash_command="date")
```

### Operating System

CentOS-7

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

my scheduler & workers are in different dockers, hence i'm using CeleryExecutor

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/interval.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/_cron.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/events.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/simple.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.014721627408993576

=========================================================

ISSUE: API Endpoint - CRUD - Pool
Hello 

We need to create several endpoints that perform basic CRUD operations on **Pools**. We need the following endpoints:

POST /pools
DELETE /pools/{pool_name}
PATCH /pools/{pool_name}

Detailed information is available in the issue:
https://github.com/apache/airflow/issues/8118

LOVE,

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/client/json_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/pool_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_pool_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/pool.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/validators.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/pool_slots_available_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_pool.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataprep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/client/local_client.py']
Ground Truth : ['a/airflow/api_connexion/schemas/pool_schema.py', 'a/airflow/api_connexion/endpoints/pool_endpoint.py']
Current Recall: 0.015792291220556746

=========================================================

ISSUE: Import error when using custom backend and sql_alchemy_conn_secret
**Apache Airflow version**: 2.0.0

**Environment**: 

- **Cloud provider or hardware configuration**: N/A
- **OS** (e.g. from /etc/os-release): custom Docker image (`FROM python:3.6`) and macOS Big Sur (11.0.1)
- **Kernel** (e.g. `uname -a`): 
  - `Linux xxx 4.14.174+ #1 SMP x86_64 GNU/Linux`
  - `Darwin xxx 20.1.0 Darwin Kernel Version 20.1.0 rRELEASE_X86_64 x86_64`
- **Install tools**:
- **Others**:

**What happened**:

I may have mixed 2 different issues here, but this is what happened to me.

I'm trying to use Airflow with the `airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend` and a `sql_alchemy_conn_secret` too, however, I have a `NameError` exception when attempting to run either `airflow scheduler` or `airflow webserver`:
```
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/lib/python3.6/site-packages/airflow/__init__.py", line 34, in <module>
    from airflow import settings
  File "/usr/local/lib/python3.6/site-packages/airflow/settings.py", line 35, in <module>
    from airflow.configuration import AIRFLOW_HOME, WEBSERVER_CONFIG, conf  # NOQA F401
  File "/usr/local/lib/python3.6/site-packages/airflow/configuration.py", line 786, in <module>
    conf.read(AIRFLOW_CONFIG)
  File "/usr/local/lib/python3.6/site-packages/airflow/configuration.py", line 447, in read
    self._validate()
  File "/usr/local/lib/python3.6/site-packages/airflow/configuration.py", line 196, in _validate
    self._validate_config_dependencies()
  File "/usr/local/lib/python3.6/site-packages/airflow/configuration.py", line 224, in _validate_config_dependencies
    is_sqlite = "sqlite" in self.get('core', 'sql_alchemy_conn')
  File "/usr/local/lib/python3.6/site-packages/airflow/configuration.py", line 324, in get
    option = self._get_option_from_secrets(deprecated_key, deprecated_section, key, section)
  File "/usr/local/lib/python3.6/site-packages/airflow/configuration.py", line 342, in _get_option_from_secrets
    option = self._get_secret_option(section, key)
  File "/usr/local/lib/python3.6/site-packages/airflow/configuration.py", line 303, in _get_secret_option
    return _get_config_value_from_secret_backend(secrets_path)
NameError: name '_get_config_value_from_secret_backend' is not defined
```

**What you expected to happen**:

A proper import and configuration creation.

**How to reproduce it**:

`airflow.cfg`:
```ini
[core]

# ...

sql_alchemy_conn_secret = some-key

# ...

[secrets]

backend = airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend

backend_kwargs = { ... }

# ...

```

**Anything else we need to know**:

Here is the workaround I have for the moment, not sure it works all the way, and probably doesn't cover all edge cases, tho it kinda works for my setup:

Move `get_custom_secret_backend` before (for me it's actually below `_get_config_value_from_secret_backend`): https://github.com/apache/airflow/blob/cc87caa0ce0b31aa29df7bbe90bdcc2426d80ff1/airflow/configuration.py#L794

Then comment: https://github.com/apache/airflow/blob/cc87caa0ce0b31aa29df7bbe90bdcc2426d80ff1/airflow/configuration.py#L232-L236


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/beam/hooks/test_beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.015792291220556746

=========================================================

ISSUE: AWS Hooks fail when assuming role and connection id contains forward slashes
**Apache Airflow version**: 2.1.0


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): Client Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.3", GitCommit:"1e11e4a2108024935ecfcb2912226cedeafd99df", GitTreeState:"clean", BuildDate:"2020-10-14T18:49:28Z", GoVersion:"go1.15.2", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"18+", GitVersion:"v1.18.16-eks-7737de", GitCommit:"7737de131e58a68dda49cdd0ad821b4cb3665ae8", GitTreeState:"clean", BuildDate:"2021-03-10T21:33:25Z", GoVersion:"go1.13.15", Compiler:"gc", Platform:"linux/amd64"}

**Environment**: Local/Development

- **Cloud provider or hardware configuration**: Docker container
- **OS** (e.g. from /etc/os-release): Debian GNU/Linux 10 (buster) 
- **Kernel** (e.g. `uname -a`): Linux 243e98509628 5.10.25-linuxkit #1 SMP Tue Mar 23 09:27:39 UTC 2021 x86_64 GNU/Linux
- **Install tools**:
- **Others**:

**What happened**:

* Using AWS Secrets Manager secrets backend
* Using S3Hook with aws_conn_id="foo/bar/baz" (example, but the slashes are important)
* Secret value is: `aws://?role_arn=arn%3Aaws%3Aiam%3A%3A<account_id>%3Arole%2F<role_name>&region_name=us-east-1`
* Get the following error: `botocore.exceptions.ClientError: An error occurred (ValidationError) when calling the AssumeRole operation: 1 validation error detected: Value 'Airflow_data/foo/bar/baz' at 'roleSessionName' failed to satisfy constraint: Member must satisfy regular expression pattern: [\w+=,.@-]*`


**What you expected to happen**:

No error and for boto to attempt to assume the role in the connection URI.

The _SessionFactory._assume_role class method is setting the role session name to `f"Airflow_{self.conn.conn_id}"` with no encoding.

**How to reproduce it**:

* Create an AWS connection with forward slashes in the name/id
** Use a role_arn in the connection string (e.g. `aws://?role_arn=...`)
* Create a test DAG using an AWS hook.  Example below:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta


with DAG(
    dag_id='test_assume_role',
    start_date=datetime(2021, 6, 1),
    schedule_interval=None, # no schedule, triggered manually/ad-hoc
    tags=['test'],
) as dag:
    def write_to_s3(**kwargs):
        s3_hook = S3Hook(aws_conn_id='aws/test')
        s3_hook.load_string(string_data='test', bucket_name='test_bucket', key='test/{{ execution_date }}')
    write_test_object = PythonOperator(task_id='write_test_object', python_callable=write_to_s3)
```

**Anything else we need to know**:

This is a redacted log from my actual test while using AWS Secrets Manager. Should get a similar result *without* Secrets Manager though.

<details>
<summary>1.log</summary>
[2021-07-13 12:38:10,271] {taskinstance.py:876} INFO - Dependencies all met for <TaskInstance: test_assume_role.write_test_object 2021-07-13T12:35:02.576772+00:00 [queued]>
[2021-07-13 12:38:10,288] {taskinstance.py:876} INFO - Dependencies all met for <TaskInstance: test_assume_role.write_test_object 2021-07-13T12:35:02.576772+00:00 [queued]>
[2021-07-13 12:38:10,288] {taskinstance.py:1067} INFO - 
--------------------------------------------------------------------------------
[2021-07-13 12:38:10,289] {taskinstance.py:1068} INFO - Starting attempt 1 of 1
[2021-07-13 12:38:10,289] {taskinstance.py:1069} INFO - 
--------------------------------------------------------------------------------
[2021-07-13 12:38:10,299] {taskinstance.py:1087} INFO - Executing <Task(PythonOperator): write_test_object> on 2021-07-13T12:35:02.576772+00:00
[2021-07-13 12:38:10,305] {standard_task_runner.py:52} INFO - Started process 38974 to run task
[2021-07-13 12:38:10,309] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'test_assume_role', 'write_test_object', '2021-07-13T12:35:02.576772+00:00', '--job-id', '2376', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/test_assume_role.py', '--cfg-path', '/tmp/tmprusuo0ys', '--error-file', '/tmp/tmp8ytd9bk8']
[2021-07-13 12:38:10,311] {standard_task_runner.py:77} INFO - Job 2376: Subtask write_test_object
[2021-07-13 12:38:10,331] {logging_mixin.py:104} INFO - Running <TaskInstance: test_assume_role.write_test_object 2021-07-13T12:35:02.576772+00:00 [running]> on host 243e98509628
[2021-07-13 12:38:10,392] {taskinstance.py:1282} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=test_assume_role
AIRFLOW_CTX_TASK_ID=write_test_object
AIRFLOW_CTX_EXECUTION_DATE=2021-07-13T12:35:02.576772+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2021-07-13T12:35:02.576772+00:00
[2021-07-13 12:38:10,419] {base_aws.py:362} INFO - Airflow Connection: aws_conn_id=foo/bar/baz
[2021-07-13 12:38:10,444] {credentials.py:1087} INFO - Found credentials in environment variables.
[2021-07-13 12:38:11,079] {base_aws.py:173} INFO - No credentials retrieved from Connection
[2021-07-13 12:38:11,079] {base_aws.py:76} INFO - Retrieving region_name from Connection.extra_config['region_name']
[2021-07-13 12:38:11,079] {base_aws.py:81} INFO - Creating session with aws_access_key_id=None region_name=us-east-1
[2021-07-13 12:38:11,096] {base_aws.py:151} INFO - role_arn is arn:aws:iam::<account_id>:role/<role_name>
[2021-07-13 12:38:11,096] {base_aws.py:97} INFO - assume_role_method=None
[2021-07-13 12:38:11,098] {credentials.py:1087} INFO - Found credentials in environment variables.
[2021-07-13 12:38:11,119] {base_aws.py:185} INFO - Doing sts_client.assume_role to role_arn=arn:aws:iam::<account_id>:role/<role_name> (role_session_name=Airflow_data/foo/bar/baz)
[2021-07-13 12:38:11,407] {taskinstance.py:1481} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1137, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/python.py", line 150, in execute
    return_value = self.execute_callable()
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/python.py", line 161, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/usr/local/airflow/dags/test_assume_role.py", line 49, in write_to_s3
    key='test/{{ execution_date }}'
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 61, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 90, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 571, in load_string
    self._upload_file_obj(file_obj, key, bucket_name, replace, encrypt, acl_policy)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 652, in _upload_file_obj
    if not replace and self.check_for_key(key, bucket_name):
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 61, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 90, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 328, in check_for_key
    raise e
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 322, in check_for_key
    self.get_conn().head_object(Bucket=bucket_name, Key=key)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 455, in get_conn
    return self.conn
  File "/usr/local/lib/python3.7/site-packages/cached_property.py", line 36, in __get__
    value = obj.__dict__[self.func.__name__] = self.func(obj)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 437, in conn
    return self.get_client_type(self.client_type, region_name=self.region_name)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 403, in get_client_type
    session, endpoint_url = self._get_credentials(region_name)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 379, in _get_credentials
    conn=connection_object, region_name=region_name, config=self.config
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 69, in create_session
    return self._impersonate_to_role(role_arn=role_arn, session=session, session_kwargs=session_kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 101, in _impersonate_to_role
    sts_client=sts_client, role_arn=role_arn, assume_role_kwargs=assume_role_kwargs
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 188, in _assume_role
    RoleArn=role_arn, RoleSessionName=role_session_name, **assume_role_kwargs
  File "/usr/local/lib/python3.7/site-packages/botocore/client.py", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.7/site-packages/botocore/client.py", line 676, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (ValidationError) when calling the AssumeRole operation: 1 validation error detected: Value 'Airflow_data/foo/bar/baz' at 'roleSessionName' failed to satisfy constraint: Member must satisfy regular expression pattern: [\w+=,.@-]*
[2021-07-13 12:38:11,417] {taskinstance.py:1531} INFO - Marking task as FAILED. dag_id=test_assume_role, task_id=write_test_object, execution_date=20210713T123502, start_date=20210713T123810, end_date=20210713T123811
[2021-07-13 12:38:11,486] {local_task_job.py:151} INFO - Task exited with return code 1
</details>

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/connection_wrapper.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/utils/test_connection_wrapper.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_response.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_emr_eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py']
Ground Truth : ['a/airflow/providers/amazon/aws/hooks/base_aws.py']
Current Recall: 0.015792291220556746

=========================================================

ISSUE: Add unittests for GCSTaskHandler
**Description**

The `GCSTaskHandler` from `airflow.utils.log.gcs_task_handler` does not have a unit test. 

**Use case / motivation**

The test is necessary so that we can be able to move the `GCSTaskHandler` to its providers package(`google`)

**Related Issues**
#9386 


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/log/test_gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/docs_build/lint_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py']
Ground Truth : ['a/airflow/providers/google/cloud/log/gcs_task_handler.py']
Current Recall: 0.017933618843683084

=========================================================

ISSUE: Dataset list remembers old datasets
### Apache Airflow version

main (development)

### What happened

Here's a simple pair of dags linked by a dataset:

```python3
from datetime import datetime
from airflow import Dataset
from airflow.operators.empty import EmptyOperator
from airflow.decorators import dag

dataset = Dataset("dataset")

@dag(start_date=datetime(1970, 1, 1))
def upstream():
    EmptyOperator(task_id="empty", outlets=[dataset])

upstream()


@dag(start_date=datetime(1970, 1, 1), schedule=[dataset])
def downstream():
    EmptyOperator(task_id="empty")

downstream()
```

I let airflow parse this, then I made the following edit:
```python3
dataset = Dataset("dataset")
dataset = Dataset("dataset1")
```

I let airflow parse it again, then I made another edit:
```python3
dataset = Dataset("dataset1")
dataset = Dataset("dataset2")
```

Then I viewed the datasets page:
<img width="675" alt="Screen Shot 2022-09-14 at 12 07 08 AM" src="https://user-images.githubusercontent.com/5834582/190073533-ebd4cb54-e080-4de5-9d95-9adf3c6c0a74.png">

Notice that instead of updating the name of the dataset, as I intended, I ended up creating three datasets.




### What you think should happen instead

If there are no dags which reference a dataset, it should not be shown in the dataset list

### How to reproduce

update a dataset URI

### Operating System

docker/debian

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

`astro dev start`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/datasets/test_dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dataset_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_dataset_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/openlineage/extractors/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_serialized_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_serde.py']
Ground Truth : ['a/airflow/dag_processing/manager.py', 'a/airflow/models/dataset.py', 'a/airflow/configuration.py', 'a/airflow/www/views.py', '/dev/null', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dag.py']
Current Recall: 0.017933618843683084

=========================================================

ISSUE: alembic Logging
### Apache Airflow version

2.5.3

### What happened

When I call the airflow initdb function, it outputs these lines to the log

INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.

### What you think should happen instead

There should be a mechanism to disable these logs, or they should just be set to WARN by default

### How to reproduce

Set up a new postgres connection and call:

from airflow.utils.db import initdb
initdb()

### Operating System

MacOS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py']
Ground Truth : ['a/airflow/utils/db.py']
Current Recall: 0.017933618843683084

=========================================================

ISSUE: Need an REST API or/and Airflow CLI to fetch last parsed time of a given DAG
### Description


We need to access the time at which a given DAG was parsed last.

Airflow Version : 2.2.2 and above.

### Use case/motivation

End users want to run a given DAG post applying the changes they have done on them. This would mean that the DAG should be parsed post the edits done to it. Right now the last parsed time is available by accessing the airflow database only. Querying the database directly is not the best solution to the problem. Ideally airflow should be exposing APIs that end users can consume that can help provide the last parsed time for a given DAG.

### Related issues

Not Aware.

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py']
Ground Truth : ['a/airflow/cli/cli_config.py', 'a/airflow/cli/commands/dag_command.py']
Current Recall: 0.017933618843683084

=========================================================

ISSUE: Question: Can I set airflow configuration parameters from the environment?
I am running airflow under a heroku-like PaaS.  Can I somehow use environment variables to specify configuration like the `sql_alchemy_conn` and `broker_url` variables?


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/default_celery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/dags/elastic_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/systems_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/secrets/lockbox.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.017933618843683084

=========================================================

ISSUE: Airflow logging secrets masker assumes dict_key is type `str`
**Apache Airflow version**: 2.1.0

**What happened**:
Airflow logging assume dict_key is type `str`
```
    logging.info("Dictionary where key is int type: %s", modified_table_mapping)
  File "/usr/lib64/python3.6/logging/__init__.py", line 1902, in info
    root.info(msg, *args, **kwargs)
  File "/usr/lib64/python3.6/logging/__init__.py", line 1308, in info
    self._log(INFO, msg, args, **kwargs)
  File "/usr/lib64/python3.6/logging/__init__.py", line 1444, in _log
    self.handle(record)
  File "/usr/lib64/python3.6/logging/__init__.py", line 1453, in handle
    if (not self.disabled) and self.filter(record):
  File "/usr/lib64/python3.6/logging/__init__.py", line 720, in filter
    result = f.filter(record)
  File "/bb/bin/airflow_env/lib/python3.6/site-packages/airflow/utils/log/secrets_masker.py", line 157, in filter
    record.__dict__[k] = self.redact(v)
  File "/bb/bin/airflow_env/lib/python3.6/site-packages/airflow/utils/log/secrets_masker.py", line 193, in redact
    return {dict_key: self.redact(subval, dict_key) for dict_key, subval in item.items()}
  File "/bb/bin/airflow_env/lib/python3.6/site-packages/airflow/utils/log/secrets_masker.py", line 193, in <dictcomp>
    return {dict_key: self.redact(subval, dict_key) for dict_key, subval in item.items()}
  File "/bb/bin/airflow_env/lib/python3.6/site-packages/airflow/utils/log/secrets_masker.py", line 189, in redact
    if name and should_hide_value_for_key(name):
  File "/bb/bin/airflow_env/lib/python3.6/site-packages/airflow/utils/log/secrets_masker.py", line 74, in should_hide_value_for_key
    name = name.strip().lower()
AttributeError: 'int' object has no attribute 'strip'
```

**How to reproduce it**:
Define a dictionary where the type of keys is `int` and print it in any Airflow tasks.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/openlineage/utils/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/beam/hooks/test_beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/operators_and_hooks_ref.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py']
Ground Truth : ['a/airflow/utils/log/secrets_masker.py']
Current Recall: 0.02007494646680942

=========================================================

ISSUE: reverting json setting to use json_encoder despite warnings
closes: [26527](https://github.com/apache/airflow/issues/26527)

The `flask_app.json_provider_class` does not take a JsonEncoder as an argument. Reverting the setting to `flask_app.json_encoder` despite deprecation warnings until a provider can be written. 



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_cli_util.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/postgres/hooks/postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/app.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/weekday.py']
Ground Truth : ['a/airflow/www/utils.py', 'a/airflow/www/views.py', 'a/airflow/www/app.py', 'a/airflow/utils/json.py']
Current Recall: 0.02007494646680942

=========================================================

ISSUE: Remove or limit table of content at the main Airflow doc page
Table of content here : http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/index.html at the main page of "apache-airflow" documentation is huge and useless (especially that we have it in directory on the left). 


We should remove it or limit heavily (to 1st level only)


![Screenshot from 2020-11-22 20-34-28](https://user-images.githubusercontent.com/595491/99915147-27738a00-2d02-11eb-8e6b-1e2717180f26.png)




Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/add_back_references.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/docs_build/fetch_inventories.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_migration_reference.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py']
Ground Truth : ['a/setup.py', 'a/docs/conf.py', 'a/airflow/serialization/serialized_objects.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Create UndefinedJinjaVariablesRule to ease upgrade to 2.0
This issue is part of #8765

## Rule
Create `UndefinedJinjaVariablesRule` which corresponds to
> Prior to Airflow 2.0 Jinja Templates would permit the use of undefined variables. They would render as an
empty string, with no indication to the user an undefined variable was used. With this release, any template
rendering involving undefined variables will fail the task, as well as displaying an error in the UI when
rendering.

entry in UPDATING.md. Introduced in #11016 
This rule should check if any of the jinja templates use undefined variables. This might not be always possible, as this might involve the dynamic generation of some context variables, but likely we can at least print some warning or "double-check this" statements.

## How to guide
To implement a new rule, create a class that inherits from `airflow.upgrade.rules.base_rule.BaseRule`.
It will be auto-registered and used by `airflow upgrade-check` command. The custom rule class has to have `title`,
`description` properties and should implement `check` method which returns a list of error messages in case of
incompatibility.

For example:
https://github.com/apache/airflow/blob/ea36166961ca35fc51ddc262ec245590c3e236fb/airflow/upgrade/rules/conn_type_is_not_nullable.py#L25-L42

**Remember** to open the PR against `v1-10-test` branch.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_bulk_issues.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/sql_to_slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py']
Ground Truth : ['/dev/null', 'a/airflow/models/dag.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Rendered Template raise general error if TaskInstance not exists
### Apache Airflow version

2.2.0b1 (beta snapshot)

### Operating System

Linux

### Versions of Apache Airflow Providers

n/a

### Deployment

Docker-Compose

### Deployment details

Docker Image: apache/airflow:2.2.0b1-python3.8

```shell
$ docker version
Client:
 Version:           20.10.8
 API version:       1.41
 Go version:        go1.16.6
 Git commit:        3967b7d28e
 Built:             Wed Aug  4 10:59:01 2021
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server:
 Engine:
  Version:          20.10.8
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.16.6
  Git commit:       75249d88bc
  Built:            Wed Aug  4 10:58:48 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          v1.5.5
  GitCommit:        72cec4be58a9eb6b2910f5d10f1c01ca47d231c0.m
 runc:
  Version:          1.0.2
  GitCommit:        v1.0.2-0-g52b36a2d
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
```

```shell
$ docker-compose version
docker-compose version 1.29.2, build unknown
docker-py version: 5.0.2
CPython version: 3.9.6
OpenSSL version: OpenSSL 1.1.1l  24 Aug 2021
```

### What happened

Rendered Template endpoint raise error if not Task Instances exists (e.g. fresh DAG)

```
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/auth.py", line 51, in decorated
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/decorators.py", line 72, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/views.py", line 1055, in rendered_templates
    ti = dag_run.get_task_instance(task_id=task.task_id, session=session)
AttributeError: 'NoneType' object has no attribute 'get_task_instance'
```

### What you expected to happen

I understand that probably no way to render this value anymore by airflow webserver.

So basically it can be replaced by the same warning/error which throw when we tried to access to XCom for TaskInstance which not exists (Task Instance -> XCom -> redirect to /home -> In top of the page Task Not exists)

![image](https://user-images.githubusercontent.com/85952209/133264002-21c3ff4a-8273-4e24-b00e-cf91639c58b4.png)

However it would be nice if we have same as we have in 2.1 (and probably earlier in 2.x)


### How to reproduce

1. Create new DAG
2. Go to Graph View
3. Try to access Rendered Tab
4. Get an error

![image](https://user-images.githubusercontent.com/85952209/133263628-80e4a7f9-ac82-48d5-9aea-84df1c64a240.png)


### Anything else

Every time.

Sample url: `http://127.0.0.1:8081/rendered-templates?dag_id=sample_dag&task_id=sample_task&execution_date=2021-09-14T10%3A10%3A23.484252%2B00%3A00`

Discuss with @uranusjr 

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

NoResultFound raises in UI when access to Task Instance Details
### Apache Airflow version

2.2.0b1 (beta snapshot)

### Operating System

Linux (`5.13.13-zen1-1-zen #1 ZEN SMP PREEMPT Thu, 26 Aug 2021 19:14:35 +0000 x86_64 GNU/Linux`)

### Versions of Apache Airflow Providers

<details><summary> pip freeze | grep apache-airflow-providers</summary> 
apache-airflow-providers-amazon==2.1.0<br>
apache-airflow-providers-celery==2.0.0<br>
apache-airflow-providers-cncf-kubernetes==2.0.1<br>
apache-airflow-providers-docker==2.1.0<br>
apache-airflow-providers-elasticsearch==2.0.2<br>
apache-airflow-providers-ftp==2.0.0<br>
apache-airflow-providers-google==5.0.0<br>
apache-airflow-providers-grpc==2.0.0<br>
apache-airflow-providers-hashicorp==2.0.0<br>
apache-airflow-providers-http==2.0.0<br>
apache-airflow-providers-imap==2.0.0<br>
apache-airflow-providers-microsoft-azure==3.1.0<br>
apache-airflow-providers-mysql==2.1.0<br>
apache-airflow-providers-postgres==2.0.0<br>
apache-airflow-providers-redis==2.0.0<br>
apache-airflow-providers-sendgrid==2.0.0<br>
apache-airflow-providers-sftp==2.1.0<br>
apache-airflow-providers-slack==4.0.0<br>
apache-airflow-providers-sqlite==2.0.0<br>
apache-airflow-providers-ssh==2.1.0<br>
</details>

### Deployment

Docker-Compose

### Deployment details

Docker Image: apache/airflow:2.2.0b1-python3.8

```shell
$ docker version
Client:
 Version:           20.10.8
 API version:       1.41
 Go version:        go1.16.6
 Git commit:        3967b7d28e
 Built:             Wed Aug  4 10:59:01 2021
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server:
 Engine:
  Version:          20.10.8
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.16.6
  Git commit:       75249d88bc
  Built:            Wed Aug  4 10:58:48 2021
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          v1.5.5
  GitCommit:        72cec4be58a9eb6b2910f5d10f1c01ca47d231c0.m
 runc:
  Version:          1.0.2
  GitCommit:        v1.0.2-0-g52b36a2d
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
```

```shell
$ docker-compose version
docker-compose version 1.29.2, build unknown
docker-py version: 5.0.2
CPython version: 3.9.6
OpenSSL version: OpenSSL 1.1.1l  24 Aug 2021
```

### What happened

I've tested current project for compatibility to migrate to 2.2.x in the future.

As soon as i tried to access from UI to Task Instance Details or Rendered Template for DAG _which never started before_ I've got this error

```
Python version: 3.8.11
Airflow version: 2.2.0b1
Node: e70bca1d41d3
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/airflow/.local/lib/python3.8/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/auth.py", line 51, in decorated
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/decorators.py", line 72, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/www/views.py", line 1368, in task
    session.query(TaskInstance)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 3500, in one
    raise orm_exc.NoResultFound("No row was found for one()")
sqlalchemy.orm.exc.NoResultFound: No row was found for one()
```

### What you expected to happen

On previous version (Apache Airflow 2.1.2) it show information even if DAG never started.
If it's new behavior of Airflow for Task Instances in UI it would be nice get information specific to this error rather than generic error

### How to reproduce

1. Use Apache Airflow: 2.2.0b1
2. Create new DAG
3. In web server try to access to Task Instance Details (`/task` entrypoint) or Rendered Template (`rendered-templates` entrypoint)

### Anything else

As soon as DAG started at least once this kind of errors gone when access Task Instance Details or Rendered Template for this DAG Tasks

Seems like this kind of errors happen after #17719

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: DockerOperator fails to pull an image
**Apache Airflow version**: 2.0

**Environment**:

- **OS** (from /etc/os-release): Debian GNU/Linux 10 (buster)
- **Kernel** (`uname -a`): Linux 37365fa0b59b 5.4.0-47-generic #51-Ubuntu SMP Fri Sep 4 19:50:52 UTC 2020 x86_64 GNU/Linux
- **Others**: running inside a docker container, forked puckel/docker-airflow

**What happened**:

`DockerOperator` does not attempt to pull an image unless force_pull is set to True, instead displaying a misleading 404 error.

**What you expected to happen**:

`DockerOperator` should attempt to pull an image when it is not present locally.

**How to reproduce it**:

Make sure you don't have an image tagged `debian:buster-slim` present locally.
```
DockerOperator(
        task_id=f'try_to_pull_debian',
        image='debian:buster-slim',
        command=f'''echo hello''',
        force_pull=False
    )
```
prints: `{taskinstance.py:1396} ERROR - 404 Client Error: Not Found ("No such image: ubuntu:latest")`
This, on the other hand:
```
DockerOperator(
        task_id=f'try_to_pull_debian',
        image='debian:buster-slim',
        command=f'''echo hello''',
        force_pull=True
    )
```
pulls the image and prints `{docker.py:263} INFO - hello`

**Anything else we need to know**:

I overrode `DockerOperator` to track down what I was doing wrong and found the following:

When trying to run an image that's not present locally, `self.cli.images(name=self.image)` in the line:
https://github.com/apache/airflow/blob/8723b1feb82339d7a4ba5b40a6c4d4bbb995a4f9/airflow/providers/docker/operators/docker.py#L286
returns a non-empty array even when the image has been deleted from the local machine. 

In fact, `self.cli.images` appears to return non-empty arrays even when supplied with nonsense image names.

<details><summary>force_pull_false.log</summary> 
[2021-01-27 06:15:28,987] {__init__.py:124} DEBUG - Preparing lineage inlets and outlets
[2021-01-27 06:15:28,987] {__init__.py:168} DEBUG - inlets: [], outlets: []
[2021-01-27 06:15:28,987] {config.py:21} DEBUG - Trying paths: ['/usr/local/airflow/.docker/config.json', '/usr/local/airflow/.dockercfg']
[2021-01-27 06:15:28,987] {config.py:25} DEBUG - Found file at path: /usr/local/airflow/.docker/config.json
[2021-01-27 06:15:28,987] {auth.py:182} DEBUG - Found 'auths' section
[2021-01-27 06:15:28,988] {auth.py:142} DEBUG - Found entry (registry='https://index.docker.io/v1/', username='xxxxxxx')
[2021-01-27 06:15:29,015] {connectionpool.py:433} DEBUG - http://localhost:None "GET /version HTTP/1.1" 200 851
[2021-01-27 06:15:29,060] {connectionpool.py:433} DEBUG - http://localhost:None "GET /v1.41/images/json?filter=debian%3Abuster-slim&only_ids=0&all=0 HTTP/1.1" 200 None
[2021-01-27 06:15:29,060] {docker.py:224} INFO - Starting docker container from image debian:buster-slim
[2021-01-27 06:15:29,063] {connectionpool.py:433} DEBUG - http://localhost:None "POST /v1.41/containers/create HTTP/1.1" 404 48
[2021-01-27 06:15:29,063] {taskinstance.py:1396} ERROR - 404 Client Error: Not Found ("No such image: debian:buster-slim")
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/docker/api/client.py", line 261, in _raise_for_status
    response.raise_for_status()
  File "/usr/local/lib/python3.8/site-packages/requests/models.py", line 941, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http+docker://localhost/v1.41/containers/create

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1086, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/usr/local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1260, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/usr/local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1300, in _execute_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 305, in execute
    return self._run_image()
  File "/usr/local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 231, in _run_image
    self.container = self.cli.create_container(
  File "/usr/local/lib/python3.8/site-packages/docker/api/container.py", line 427, in create_container
    return self.create_container_from_config(config, name)
  File "/usr/local/lib/python3.8/site-packages/docker/api/container.py", line 438, in create_container_from_config
    return self._result(res, True)
  File "/usr/local/lib/python3.8/site-packages/docker/api/client.py", line 267, in _result
    self._raise_for_status(response)
  File "/usr/local/lib/python3.8/site-packages/docker/api/client.py", line 263, in _raise_for_status
    raise create_api_error_from_http_exception(e)
  File "/usr/local/lib/python3.8/site-packages/docker/errors.py", line 31, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation)
docker.errors.ImageNotFound: 404 Client Error: Not Found ("No such image: debian:buster-slim")
</details>

<details><summary>force_pull_true.log</summary>
[2021-01-27 06:17:01,811] {__init__.py:124} DEBUG - Preparing lineage inlets and outlets
[2021-01-27 06:17:01,811] {__init__.py:168} DEBUG - inlets: [], outlets: []
[2021-01-27 06:17:01,811] {config.py:21} DEBUG - Trying paths: ['/usr/local/airflow/.docker/config.json', '/usr/local/airflow/.dockercfg']
[2021-01-27 06:17:01,811] {config.py:25} DEBUG - Found file at path: /usr/local/airflow/.docker/config.json
[2021-01-27 06:17:01,811] {auth.py:182} DEBUG - Found 'auths' section
[2021-01-27 06:17:01,812] {auth.py:142} DEBUG - Found entry (registry='https://index.docker.io/v1/', username='xxxxxxxxx')
[2021-01-27 06:17:01,825] {connectionpool.py:433} DEBUG - http://localhost:None "GET /version HTTP/1.1" 200 851
[2021-01-27 06:17:01,826] {docker.py:287} INFO - Pulling docker image debian:buster-slim
[2021-01-27 06:17:01,826] {auth.py:41} DEBUG - Looking for auth config
[2021-01-27 06:17:01,826] {auth.py:242} DEBUG - Looking for auth entry for 'docker.io'
[2021-01-27 06:17:01,826] {auth.py:250} DEBUG - Found 'https://index.docker.io/v1/'
[2021-01-27 06:17:01,826] {auth.py:54} DEBUG - Found auth config
[2021-01-27 06:17:04,399] {connectionpool.py:433} DEBUG - http://localhost:None "POST /v1.41/images/create?tag=buster-slim&fromImage=debian HTTP/1.1" 200 None
[2021-01-27 06:17:04,400] {docker.py:301} INFO - buster-slim: Pulling from library/debian
[2021-01-27 06:17:04,982] {docker.py:301} INFO - a076a628af6f: Pulling fs layer
[2021-01-27 06:17:05,884] {docker.py:301} INFO - a076a628af6f: Downloading
[2021-01-27 06:17:11,429] {docker.py:301} INFO - a076a628af6f: Verifying Checksum
[2021-01-27 06:17:11,429] {docker.py:301} INFO - a076a628af6f: Download complete
[2021-01-27 06:17:11,480] {docker.py:301} INFO - a076a628af6f: Extracting
</details>

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_supported_versions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/flink/sensors/test_flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Partial of a KubernetesPodOperator does not allow for limit_cpu and limit_memory in the resources argument
### Apache Airflow version

2.3.0 (latest released)

### What happened

When performing dynamic task mapping and providing Kubernetes limits to the `resources` argument, the DAG raises an import error: 

```
Broken DAG: [/opt/airflow/dags/bulk_image_processing.py] Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 287, in partial
    partial_kwargs["resources"] = coerce_resources(partial_kwargs["resources"])
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/baseoperator.py", line 133, in coerce_resources
    return Resources(**resources)
TypeError: __init__() got an unexpected keyword argument 'limit_cpu'
```

The offending code is:

```
KubernetesPodOperator.partial(
            get_logs: True,
            in_cluster: True,
            is_delete_operator_pod: True,
            namespace: settings.namespace,
            resources={'limit_cpu': settings.IMAGE_PROCESSING_OPERATOR_CPU, 'limit_memory': settings.IMAGE_PROCESSING_OPERATOR_MEMORY},
            service_account_name: settings.SERVICE_ACCOUNT_NAME,
            startup_timeout_seconds: 600,
            **kwargs,
    )
```

But you can see this in any DAG utilizing a `KubernetesPodOperator.partial` where the `partial` contains the `resources` argument.



### What you think should happen instead

The `resources` argument should be taken at face value and applied to the `OperatorPartial` and subsequently the `MappedOperator`.

### How to reproduce

Try to import this DAG using Airflow 2.3.0:

```
from datetime import datetime
from airflow import XComArg
from airflow.models import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator


def make_operator(
    **kwargs
):
    return KubernetesPodOperator(
        **{
            'get_logs': True,
            'in_cluster': True,
            'is_delete_operator_pod': True,
            'namespace': 'default',
            'startup_timeout_seconds': 600,
            **kwargs,
        }
    )


def make_partial_operator(
    **kwargs
):
    return KubernetesPodOperator.partial(
        **{
            'get_logs': True,
            'in_cluster': True,
            'is_delete_operator_pod': True,
            'namespace': 'default',
            'startup_timeout_seconds': 600,
            **kwargs,
        }
    )


with DAG(dag_id='bulk_image_processing',
         schedule_interval=None,
         start_date=datetime(2020, 1, 1),
         max_active_tasks=20) as dag:

    op1 = make_operator(
        arguments=['--bucket-name', f'{{{{ dag_run.conf.get("bucket", "some-fake-default") }}}}'],
        cmds=['python3', 'some_entrypoint'],
        image='some-image',
        name='airflow-private-image-pod-1',
        task_id='some-task',
        do_xcom_push=True
    )

    op2 = make_partial_operator(
        image='another-image',
        name=f'airflow-private-image-pod-2',
        resources={'limit_cpu': '2000m', 'limit_memory': '16Gi'},
        task_id='another-task',
        cmds=[
            'some',
            'set',
            'of',
            'cmds'
        ]
    ).expand(arguments=XComArg(op1))

```

### Operating System

MacOS 11.6.5

### Versions of Apache Airflow Providers

Relevant:
```
apache-airflow-providers-cncf-kubernetes==4.0.1
```

Full:
```
apache-airflow-providers-amazon==3.3.0
apache-airflow-providers-celery==2.1.4
apache-airflow-providers-cncf-kubernetes==4.0.1
apache-airflow-providers-docker==2.6.0
apache-airflow-providers-elasticsearch==3.0.3
apache-airflow-providers-ftp==2.1.2
apache-airflow-providers-google==6.8.0
apache-airflow-providers-grpc==2.0.4
apache-airflow-providers-hashicorp==2.2.0
apache-airflow-providers-http==2.1.2
apache-airflow-providers-imap==2.2.3
apache-airflow-providers-microsoft-azure==3.8.0
apache-airflow-providers-mysql==2.2.3
apache-airflow-providers-odbc==2.0.4
apache-airflow-providers-postgres==4.1.0
apache-airflow-providers-redis==2.0.4
apache-airflow-providers-sendgrid==2.0.4
apache-airflow-providers-sftp==2.6.0
apache-airflow-providers-slack==4.2.3
apache-airflow-providers-sqlite==2.1.3
apache-airflow-providers-ssh==2.4.3
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Docker (Docker Desktop)
- Server Version: 20.10.13
- API Version: 1.41
- Builder: 2

Kubernetes (Docker Desktop)
- Env: docker-desktop
- Context: docker-desktop
- Cluster Name: docker-desktop
- Namespace: default
- Container Runtime: docker
- Version: v1.22.5

Helm:
- version.BuildInfo{Version:"v3.6.3", GitCommit:"d506314abfb5d21419df8c7e7e68012379db2354", GitTreeState:"dirty", GoVersion:"go1.16.5"}

### Anything else

You can get around this by creating the `partial` first without calling `expand` on it, setting the resources via the `kwargs` parameter, then calling `expand`. Example:

```
from datetime import datetime
from airflow import XComArg
from airflow.models import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator


def make_operator(
    **kwargs
):
    return KubernetesPodOperator(
        **{
            'get_logs': True,
            'in_cluster': True,
            'is_delete_operator_pod': True,
            'namespace': 'default',
            'startup_timeout_seconds': 600,
            **kwargs,
        }
    )


def make_partial_operator(
    **kwargs
):
    return KubernetesPodOperator.partial(
        **{
            'get_logs': True,
            'in_cluster': True,
            'is_delete_operator_pod': True,
            'namespace': 'default',
            'startup_timeout_seconds': 600,
            **kwargs,
        }
    )


with DAG(dag_id='bulk_image_processing',
         schedule_interval=None,
         start_date=datetime(2020, 1, 1),
         max_active_tasks=20) as dag:

    op1 = make_operator(
        arguments=['--bucket-name', f'{{{{ dag_run.conf.get("bucket", "some-fake-default") }}}}'],
        cmds=['python3', 'some_entrypoint'],
        image='some-image',
        name='airflow-private-image-pod-1',
        task_id='some-task',
        do_xcom_push=True
    )

    op2 = make_partial_operator(
        image='another-image',
        name=f'airflow-private-image-pod-2',
        task_id='another-task',
        cmds=[
            'some',
            'set',
            'of',
            'cmds'
        ]
    )
    
    op2.kwargs['resources'] = {'limit_cpu': '2000m', 'limit_memory': '16Gi'} 

    op2.expand(arguments=XComArg(op1))
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/cncf/kubernetes/example_kubernetes_async.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/cncf/kubernetes/example_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/test_pod_generator.py']
Ground Truth : ['a/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: The "ldap" extra misses libldap dependency
The 'ldap' provider misses 'ldap' extra dep (which adds ldap3 pip dependency).

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_image_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/remove_arm_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/default_webserver_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/send_email.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/hatch_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Unable to use nested lists in DAG markdown documentation
**Apache Airflow version**: 2.0.2

**What happened**:

Tried to use the following markdown as a `doc_md` string passed to a DAG

```markdown
- Example
    -  Nested List
```

It was rendered in the web UI as a single list with no nesting or indentation.

**What you expected to happen**:

I expected the list to display as a nested list with visible indentation.

**How to reproduce it**:

Try and pass a DAG a `doc_md` string of the above nested list.
I think the bug will affect any markdown that relies on meaningful indentation (tabs or spaces)

doc_md code block collapsing lines
**Apache Airflow version**: 2.0.0 - 2.1.0

**Kubernetes version**: N/A

**Environment**:

- **Cloud provider or hardware configuration**: Docker on MacOS (but also AWS ECS deployed)
- **OS** (e.g. from /etc/os-release): MacOS Big Sur 11.3.1
- **Kernel** (e.g. `uname -a`): Darwin Kernel Version 20.4.0
- **Install tools**:
- **Others**:

**What happened**:

When a code block is a part of a DAG's `doc_md`, it does not render correctly in the Web UI, but collapses all the lines into one line instead.

**What you expected to happen**:

The multi line code block be rendered with line breaks preserved.

**How to reproduce it**:
Create a DAG with `doc_md` containing a code block:

````python
from airflow import DAG

DOC_MD = """\
# Markdown code block

Inline `code` works well.

```
Code block
does not
respect
newlines
```
"""

dag = DAG(
    dag_id='test',
    doc_md=DOC_MD
)
````

The rendered documentation looks like this:
<img src="https://user-images.githubusercontent.com/11132999/119981579-19a70600-bfbe-11eb-8036-7d981ae1f232.png" width="50%"/>

**Anything else we need to know**: N/A



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_trigger_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py']
Ground Truth : ['a/airflow/www/utils.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Migrate from 2.1.4 to 2.2.0
### Apache Airflow version

2.2.0

### Operating System

Linux

### Versions of Apache Airflow Providers

default.

### Deployment

Docker-Compose

### Deployment details

Using airflow-2.2.0python3.7

### What happened

Upgrading image from apache/airflow:2.1.4-python3.7
to apache/airflow:2.2.0-python3.7
Cause this inside scheduler, which is not starting:

```
Python version: 3.7.12
Airflow version: 2.2.0
Node: 6dd55b0a5dd7
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column dag.max_active_tasks does not exist
LINE 1: ..., dag.schedule_interval AS dag_schedule_interval, dag.max_ac...
                                                             ^

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/airflow/.local/lib/python3.7/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/airflow/.local/lib/python3.7/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/home/airflow/.local/lib/python3.7/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/home/airflow/.local/lib/python3.7/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/airflow/.local/lib/python3.7/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/www/auth.py", line 51, in decorated
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/www/views.py", line 588, in index
    filter_dag_ids = current_app.appbuilder.sm.get_accessible_dag_ids(g.user)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/www/security.py", line 377, in get_accessible_dag_ids
    return {dag.dag_id for dag in accessible_dags}
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 3535, in __iter__
    return self._execute_and_instances(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 3560, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement
    distilled_params,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column dag.max_active_tasks does not exist
LINE 1: ..., dag.schedule_interval AS dag_schedule_interval, dag.max_ac...
                                                             ^

[SQL: SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_parsed_time AS dag_last_parsed_time, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval, dag.max_active_tasks AS dag_max_active_tasks, dag.max_active_runs AS dag_max_active_runs, dag.has_task_concurrency_limits AS dag_has_task_concurrency_limits, dag.next_dagrun AS dag_next_dagrun, dag.next_dagrun_data_interval_start AS dag_next_dagrun_data_interval_start, dag.next_dagrun_data_interval_end AS dag_next_dagrun_data_interval_end, dag.next_dagrun_create_after AS dag_next_dagrun_create_after 
FROM dag]
(Background on this error at: http://sqlalche.me/e/13/f405)
```

### What you expected to happen

Automatic database migration and properly working scheduler.

### How to reproduce

Ugrade from 2.1.4 to 2.2.0 with some dags history.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/setup_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py']
Ground Truth : ['a/airflow/utils/db.py', 'a/airflow/www/views.py', 'a/airflow/settings.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: API Endpoints - Read-only - XCOM
**Description**

Hello 

We need to create several endpoints that perform basic read-only operations on **XCOM** . We need the following endpoints:

- GET /dags/{dag_id}/taskInstances/{task_id}/{execution_date}/xcomValues
- GET /dags/{dag_id}/taskInstances/{task_id}/{execution_date}/xcomValues/{key}

For now, we focus only on read-only operations, but the others will also be implemented as the next step.

Detailed information is available in the issue:
https://github.com/apache/airflow/issues/8118

**Use case / motivation**
N/A

**Related Issues**
N/A

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/skipmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py']
Ground Truth : ['/dev/null', 'a/airflow/api_connexion/endpoints/xcom_endpoint.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Generalize MLEngineStartTrainingJobOperator to custom images
**Description**

The operator is arguably unnecessarily limited to AI Platforms standard images. The only change that is required to lift this constraint is making `package_uris` and `training_python_module` optional with default values `[]` and `None`, respectively. Then, using `master_config`, one can supply `imageUri` and run any image of choice.

**Use case / motivation**

This will open up for running arbitrary images on AI Platform.

**Are you willing to submit a PR?**

If the above sounds reasonable, I can open pull requests.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/operators/test_mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/pinecone/example_pinecone_openai.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/pagerduty/hooks/pagerduty.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/pagerduty/hooks/pagerduty_events.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py']
Ground Truth : ['a/airflow/providers/google/cloud/operators/mlengine.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Moving of the transfer operators to new packages
Following the discussion
https://lists.apache.org/thread.html/rbf69247f8d4702a1b3770ab4891d90daed1ef12f385f2040f3ebe92d%40%3Cdev.airflow.apache.org%3E
and voting
https://lists.apache.org/thread.html/r3514ef575b437b9eb368111b1e4b03ad7455e63d64c359c22fd6ea9a%40%3Cdev.airflow.apache.org%3E

We move transfer operators to the new package - airflow.providers.*.transfer. 

As a result, we will have the following provider packages structure.

`airflow.providers.*.example_dags`
`airflow.providers.*.hooks`
`airflow.providers.*.operators`
`airflow.providers.*.secrets`
`airflow.providers.*.sensors`
`airflow.providers.*.transfer`
`airflow.providers.*.utils`



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/contrib/secrets/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/contrib/sensors/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/contrib/hooks/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/contrib/operators/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/operators_and_hooks_ref.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_check_provider_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/docs_build/lint_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/operators/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery_dts.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_storage_transfer_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ftp/operators/ftp.py']
Ground Truth : ['/dev/null', 'a/airflow/contrib/operators/oracle_to_oracle_transfer.py', 'a/airflow/contrib/operators/cassandra_to_gcs.py', 'a/airflow/operators/google_api_to_s3_transfer.py', 'a/airflow/providers/microsoft/azure/operators/oracle_to_azure_data_lake_transfer.py', 'a/airflow/providers/google/ads/example_dags/example_ads.py', 'a/airflow/providers/amazon/aws/operators/google_api_to_s3_transfer.py', 'a/airflow/contrib/operators/gcp_transfer_operator.py', 'a/airflow/providers/google/ads/operators/ads.py', 'a/airflow/providers/google/cloud/example_dags/example_gcs_to_sftp.py', 'a/airflow/providers/google/cloud/example_dags/example_local_to_gcs.py', 'a/airflow/contrib/operators/vertica_to_mysql.py', 'a/airflow/contrib/operators/mysql_to_gcs.py', 'a/airflow/operators/mssql_to_hive.py', 'a/airflow/providers/snowflake/operators/s3_to_snowflake.py', 'a/airflow/contrib/operators/adls_to_gcs.py', 'a/airflow/contrib/operators/gcs_download_operator.py', 'a/airflow/contrib/operators/file_to_gcs.py', 'a/airflow/providers/apache/hive/operators/vertica_to_hive.py', 'a/airflow/operators/presto_to_mysql.py', 'a/airflow/contrib/operators/gcs_to_bq.py', 'a/airflow/providers/amazon/aws/operators/s3_to_redshift.py', 'a/airflow/contrib/operators/gcs_to_gdrive_operator.py', 'a/airflow/providers/google/cloud/example_dags/example_bigquery_to_gcs.py', 'a/airflow/providers/google/cloud/example_dags/example_dataflow.py', 'a/airflow/providers/amazon/aws/example_dags/example_google_api_to_s3_transfer_advanced.py', 'a/airflow/providers/google/cloud/example_dags/example_bigquery_transfer.py', 'a/airflow/providers/apache/hive/operators/hive_to_samba.py', 'a/airflow/providers/google/cloud/example_dags/example_bigquery_to_bigquery.py', 'a/airflow/providers/google/cloud/example_dags/example_gcs_to_bigquery.py', 'a/airflow/providers/mysql/operators/vertica_to_mysql.py', 'a/backport_packages/refactor_backport_packages.py', 'a/airflow/providers/apache/hive/operators/s3_to_hive.py', 'a/airflow/contrib/operators/bigquery_to_gcs.py', 'a/airflow/contrib/operators/postgres_to_gcs_operator.py', 'a/airflow/operators/redshift_to_s3_operator.py', 'a/airflow/contrib/operators/bigquery_to_mysql_operator.py', 'a/airflow/providers/apache/hive/operators/mysql_to_hive.py', 'a/airflow/contrib/operators/vertica_to_hive.py', 'a/airflow/providers/amazon/aws/operators/redshift_to_s3.py', 'a/airflow/providers/google/cloud/example_dags/example_gcs.py', 'a/airflow/providers/google/marketing_platform/example_dags/example_display_video.py', 'a/airflow/providers/google/cloud/example_dags/example_sftp_to_gcs.py', 'a/airflow/providers/google/cloud/example_dags/example_presto_to_gcs.py', 'a/airflow/operators/mysql_to_hive.py', 'a/airflow/operators/gcs_to_s3.py', 'a/airflow/providers/apache/hive/operators/mssql_to_hive.py', 'a/airflow/providers/snowflake/example_dags/example_snowflake.py', 'a/airflow/providers/mysql/operators/s3_to_mysql.py', 'a/airflow/providers/google/cloud/example_dags/example_facebook_ads_to_gcs.py', 'a/airflow/contrib/operators/file_to_wasb.py', 'a/airflow/providers/google/cloud/operators/gcs.py', 'a/airflow/providers/oracle/operators/oracle_to_oracle_transfer.py', 'a/airflow/contrib/operators/oracle_to_azure_data_lake_transfer.py', 'a/airflow/providers/google/cloud/operators/gcs_to_gcs.py', 'a/airflow/providers/google/cloud/example_dags/example_postgres_to_gcs.py', 'a/airflow/providers/google/suite/example_dags/example_gcs_to_sheets.py', 'a/airflow/operators/hive_to_samba_operator.py', 'a/airflow/contrib/operators/s3_to_sftp_operator.py', 'a/airflow/contrib/operators/sql_to_gcs.py', 'a/airflow/providers/amazon/aws/example_dags/example_imap_attachment_to_s3.py', 'a/backport_packages/setup_backport_packages.py', 'a/airflow/providers/apache/hive/operators/hive_to_mysql.py', 'a/airflow/providers/amazon/aws/example_dags/example_s3_to_redshift.py', 'a/airflow/providers/apache/druid/operators/hive_to_druid.py', 'a/airflow/contrib/operators/dynamodb_to_s3.py', 'a/airflow/contrib/operators/mssql_to_gcs.py', 'a/airflow/contrib/operators/gcs_to_s3.py', 'a/airflow/contrib/operators/imap_attachment_to_s3_operator.py', 'a/airflow/contrib/operators/s3_to_gcs_operator.py', 'a/airflow/providers/google/cloud/operators/presto_to_gcs.py', 'a/airflow/contrib/operators/hive_to_dynamodb.py', 'a/airflow/providers/google/cloud/operators/mysql_to_gcs.py', 'a/airflow/providers/mysql/operators/presto_to_mysql.py', 'a/airflow/providers/google/cloud/example_dags/example_sheets_to_gcs.py', 'a/airflow/providers/google/cloud/operators/mssql_to_gcs.py', 'a/airflow/providers/amazon/aws/example_dags/example_google_api_to_s3_transfer_basic.py', 'a/airflow/operators/hive_to_mysql.py', 'a/airflow/providers/google/cloud/operators/postgres_to_gcs.py', 'a/airflow/contrib/operators/gcs_to_gcs.py', 'a/airflow/contrib/operators/bigquery_to_bigquery.py', 'a/airflow/operators/hive_to_druid.py', 'a/airflow/contrib/operators/sftp_to_s3_operator.py', 'a/airflow/providers/google/suite/example_dags/example_gcs_to_gdrive.py', 'a/airflow/operators/s3_to_redshift_operator.py', 'a/airflow/providers/google/suite/example_dags/example_sheets.py', 'a/airflow/operators/s3_to_hive_operator.py', 'a/airflow/providers/amazon/aws/hooks/batch_client.py', 'a/airflow/contrib/operators/mongo_to_s3.py', 'a/airflow/providers/google/cloud/example_dags/example_gcs_to_gcs.py', 'a/airflow/providers/amazon/aws/operators/hive_to_dynamodb.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Empty `expand()` crashes the scheduler
### Apache Airflow version

2.3.0 (latest released)

### What happened

I've found a DAG that will crash the scheduler:

```
@task
    def hello():
        return "hello"

    hello.expand()
```

```
[2022-05-03 03:41:23,779] {scheduler_job.py:753} ERROR - Exception when executing SchedulerJob._run_scheduler_loop                 
Traceback (most recent call last):                                                                                                 
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py", line 736, in _execute                     
    self._run_scheduler_loop()                                                                                                     
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py", line 824, in _run_scheduler_loop          
    num_queued_tis = self._do_scheduling(session)                                                                                  
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py", line 906, in _do_scheduling               
    callback_to_run = self._schedule_dag_run(dag_run, session)                                                                     
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py", line 1148, in _schedule_dag_run           
    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)                              
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper                            
    return func(*args, **kwargs)                                                                                                   
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagrun.py", line 522, in update_state                      
    info = self.task_instance_scheduling_decisions(session)                                                                        
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 68, in wrapper                            
    return func(*args, **kwargs)                                                                                                   
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagrun.py", line 661, in task_instance_scheduling_decisions
    session=session,                                                                                                               
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagrun.py", line 714, in _get_ready_tis                    
    expanded_tis, _ = schedulable.task.expand_mapped_task(self.run_id, session=session)                                            
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/mappedoperator.py", line 609, in expand_mapped_task        
    operator.mul, self._resolve_map_lengths(run_id, session=session).values()                                                      
TypeError: reduce() of empty sequence with no initial value                                                                        
```

### What you think should happen instead

A user DAG shouldn't crash the scheduler. This specific case could likely be an ImportError at parse time, but it makes me think we might be missing some exception handling?

### How to reproduce

_No response_

### Operating System

Debian

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py']
Ground Truth : ['a/airflow/models/mappedoperator.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/decorators/base.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: DuplicateTaskIdFound when reusing tasks with task_group decorator
### Apache Airflow version

2.2.2 (latest released)

### Operating System

MacOS 11.6.1

### Versions of Apache Airflow Providers

$ pip freeze | grep airflow
apache-airflow==2.2.2
apache-airflow-providers-celery==2.1.0
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-sqlite==2.0.1

### Deployment

Other

### Deployment details

`airflow standalone`

### What happened

Exception raised :     

```
raise DuplicateTaskIdFound(f"Task id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'do_all_things.do_thing__1' has already been added to the DAG
```

### What you expected to happen

_No response_

### How to reproduce

* Add the following python file to the `dags` folder:

```python
from datetime import datetime

from airflow.decorators import dag, task, task_group


@dag(start_date=datetime(2023, 1, 1), schedule_interval="@once")
def test_dag_1():
    @task
    def start():
        pass

    @task
    def do_thing(x):
        print(x)

    @task_group
    def do_all_things():
        for i in range(5):
            do_thing(i)

    @task
    def end():
        pass

    start() >> do_all_things() >> end()


test_dag_1()
```

* Start airflow by running `airflow standalone`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_emr_eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py']
Ground Truth : ['a/airflow/decorators/base.py']
Current Recall: 0.020788722341184868

=========================================================

ISSUE: Unable to specify Python version for AwsGlueJobOperator
### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

2.0.2

### Operating System

Amazon Linux

### Deployment

MWAA

### Deployment details

_No response_

### What happened

When a new Glue job is created using the AwsGlueJobOperator, the job is defaulting to Python2. Setting the version in create_job_kwargs fails with key error.

### What you expected to happen

Expected the Glue job to be created with a Python3 runtime. create_job_kwargs are passed to the boto3 glue client create_job method which includes a "Command" parameter that is a dictionary containing the Python version.



### How to reproduce

Create a dag with an AwsGlueJobOperator and pass a "Command" parameter in the create_job_kwargs argument.

```
    create_glue_job_args = {
        "Command": {
            "Name": "abalone-preprocess",
            "ScriptLocation": f"s3://{output_bucket}/code/preprocess.py",
            "PythonVersion": "3"
        }
    }
    glue_etl = AwsGlueJobOperator(  
        task_id="glue_etl",  
        s3_bucket=output_bucket,
        script_args={
                '--S3_INPUT_BUCKET': data_bucket,
                '--S3_INPUT_KEY_PREFIX': 'input/raw',
                '--S3_UPLOADS_KEY_PREFIX': 'input/uploads',
                '--S3_OUTPUT_BUCKET': output_bucket,
                '--S3_OUTPUT_KEY_PREFIX': str(determine_dataset_id.output) +'/input/data' 
            },
        iam_role_name="MLOps",  
        retry_limit=2,
        concurrent_run_limit=3,
        create_job_kwargs=create_glue_job_args,
        dag=dag) 
```

```
[2022-01-04 16:43:42,053] {{logging_mixin.py:104}} INFO - [2022-01-04 16:43:42,053] {{glue.py:190}} ERROR - Failed to create aws glue job, error: 'Command'
[2022-01-04 16:43:42,081] {{logging_mixin.py:104}} INFO - [2022-01-04 16:43:42,081] {{glue.py:112}} ERROR - Failed to run aws glue job, error: 'Command'
[2022-01-04 16:43:42,101] {{taskinstance.py:1482}} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/glue.py", line 166, in get_or_create_glue_job
    get_job_response = glue_client.get_job(JobName=self.job_name)
  File "/usr/local/lib/python3.7/site-packages/botocore/client.py", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.7/site-packages/botocore/client.py", line 676, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.EntityNotFoundException: An error occurred (EntityNotFoundException) when calling the GetJob operation: Job with name: abalone-preprocess not found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1138, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/operators/glue.py", line 121, in execute
    glue_job_run = glue_job.initialize_job(self.script_args)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/glue.py", line 108, in initialize_job
    job_name = self.get_or_create_glue_job()
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/glue.py", line 186, in get_or_create_glue_job
    **self.create_job_kwargs,
KeyError: 'Command'
```

### Anything else

When a new job is being created.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/lambda_function.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py']
Ground Truth : ['a/airflow/providers/amazon/aws/hooks/glue.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: exception when root account goes to http://airflow.ordercapital.com/dag-dependencies
Happens every time 


Python version: 3.8.10
Airflow version: 2.1.0
Node: airflow-web-55974db849-5bdxq
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/airflow/www/auth.py", line 34, in decorated
    return func(*args, **kwargs)
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/airflow/www/decorators.py", line 97, in view_func
    return f(*args, **kwargs)
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/airflow/www/decorators.py", line 60, in wrapper
    return f(*args, **kwargs)
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/airflow/www/views.py", line 4004, in list
    self._calculate_graph()
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/airflow/www/views.py", line 4023, in _calculate_graph
    for dag, dependencies in SerializedDagModel.get_dag_dependencies().items():
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/opt/bitnami/airflow/venv/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 321, in get_dag_dependencies
    dependencies[row[0]] = [DagDependency(**d) for d in row[1]]
TypeError: 'NoneType' object is not iterable





Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_python_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_check_license.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/setup_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/sbom_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py']
Ground Truth : ['a/airflow/models/serialized_dag.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: TimeSensor triggers immediately when used over midnight (UTC)
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 1.10.10 (issue exists in current master as well)

**Environment**: does not seem relevant

**What happened**:

The TimeSensor does trigger if the current time is later than the defined trigger time. Looking at the [source code](https://github.com/apache/airflow/blob/master/airflow/sensors/time_sensor.py), the trigger rule is defined as
```
return timezone.utcnow().time() > self.target_time
```
This leads to problems when the DAG runs over midnight UTC. For example, suppose the following DAG:

```
with DAG('foo', 
    default_args={'start_date': datetime(2020, 7, 1, tzinfo=pendulum.timezone("Europe/Berlin"))}, 
    schedule_interval="0 0 * * *") as dag:

    # in summer, Europe/Berlin is two hours after UTC, hence: 
    time_04h00_local = TimeSensor(task_id="time_01h30", target_time=time(hour=2, minute=00))
```

This DAG will be triggered at 22:00 UTC. Then, according to the trigger rule:
```
22:00 UTC > 2:00 UTC
```
Hence, the TimeSensor will be triggered immediately. 

**What you expected to happen**:

The TimeSensor should trigger at the following day if `target_time < next_execution_date.time()`

**Possible workarounds**:

One can always use the TimeDeltaSensor to archive similar effects. This does result in code that is not as readable, though. 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/date_time.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/_cron.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/sensors/time_sensor.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: API Endpoint - Config
**Description**

Hello,

We need an endpoint that allows us to **read** the current Airflow configuration.

- GET /config

More details about API Endpoints:
https://github.com/apache/airflow/issues/8118

**Use case / motivation**

N/A

**Related Issues**

N/A

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/secrets/lockbox.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/airbyte/hooks/airbyte.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataprep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/dbt/cloud/hooks/dbt.py']
Ground Truth : ['/dev/null', 'a/airflow/api_connexion/endpoints/config_endpoint.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: Allow to set execution_timeout default value in airflow.cfg
### Discussed in https://github.com/apache/airflow/discussions/18411

<div type='discussions-op-text'>

<sup>Originally posted by **alexInhert** September 21, 2021</sup>
### Description

Currently the default value of execution_timeout in base operator is None
https://github.com/apache/airflow/blob/c686241f4ceb62d52e9bfa607822e4b7a3c76222/airflow/models/baseoperator.py#L502

This means that a task will run without limit till finished.


### Use case/motivation

The problem is that there is no way to overwrite this default for all dags. This causes problems where we find that tasks run sometimes for 1 week!!! for no reason. They are just stuck. This mostly happens with tasks that submit work to some 3rd party resource. The 3rd party had some error and terminated but the Airflow task for some reason did not.

This can be handled in cluster policy however it feels that cluster policy is more to enforce something but less about setting defaults values.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py']
Ground Truth : ['a/airflow/models/baseoperator.py', 'a/airflow/models/abstractoperator.py', 'a/airflow/configuration.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: Invalid livenessProbe for Standalone DAG Processor
### Official Helm Chart version

1.7.0 (latest released)

### Apache Airflow version

2.3.4

### Kubernetes Version

1.22.12-gke.1200	

### Helm Chart configuration

```yaml
  dagProcessor:
    enabled: true
```

### Docker Image customisations

```dockerfile
FROM apache/airflow:2.3.4-python3.9

USER root
RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
RUN apt-get update && apt-get install -y google-cloud-cli
RUN curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
RUN sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
USER airflow
```

### What happened

Current DAG Processor livenessProbe is the following:
```
CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
    airflow jobs check --hostname $(hostname)
```
This command checks the metadata DB searching for an active job whose hostname is the current pod's one (_airflow-dag-processor-xxxx_). 
However, after running the dag-processor pod for more than 1 hour, there are no jobs with the processor hostname in the jobs table.
![image](https://user-images.githubusercontent.com/28935464/196711859-98dadb8f-3273-42ec-a4db-958890db34b7.png)
![image](https://user-images.githubusercontent.com/28935464/196711947-5a0fc5d7-4b91-4e82-9ff0-c721e6a4c1cd.png)

As a consequence, the livenessProbe fails and the pod is constantly restarting.

After investigating the code, I found out that DagFileProcessorManager is not creating jobs in the metadata DB, so the livenessProbe is not valid.

### What you think should happen instead

A new job should be created for the Standalone DAG Processor.
By doing that, the _airflow jobs check --hostname <hostname>_ command would work correctly and the livenessProbe wouldn't fail

### How to reproduce

1. Deploy airflow with a standalone dag-processor.
2. Wait for ~ 5 minutes
3. Check that the livenessProbe has been failing for 5 minutes and the pod has been restarted.

### Anything else

I think this behavior is inherited from the NOT standalone dag-processor mode (the livenessProbe checks for a SchedulerJob, that in fact contains the "DagProcessorJob")

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_core/test_dag_processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_core/test_scheduler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py']
Ground Truth : ['/dev/null', 'a/airflow/cli/commands/dag_processor_command.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: BackfillJob eventually fails tasks when dependencies aren't met
See https://github.com/airbnb/airflow/issues/1225#issuecomment-203153213

When dependencies are unmet, the command exists successfully (as in, without error or return code). The executor marks it as a success. But the task has no status, because it never ran. This triggers the "airflow run command failed to report an error" which eventually fails the task.

This issue will be fixed as part of the resolution for #1225, this issue is for tracking it.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dags/test_issue_1225.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/integration/executors/test_celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py']
Ground Truth : ['a/airflow/executors/celery_executor.py', 'a/airflow/models.py', 'a/airflow/bin/cli.py', 'a/airflow/contrib/operators/ssh_execute_operator.py', 'a/airflow/executors/base_executor.py', 'a/airflow/jobs.py', 'a/airflow/utils/state.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: JSON notation in Airflow DAG comments causing KeyError
-->
**Apache Airflow version**: 1.10.11 (code is working fine in 1.10.10)


**Environment**: Python 3.7

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): Catalina 10.15.5
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**: 

**What happened**:

KeyError: '"randomKey"'

**What you expected to happen**:

Expected the block comment on the top of the DAG to be copied as is into the DAG description box in the UI

**How to reproduce it**:
Add the following code to the top of your DAG

"""
This is a test dag. this is the description of it. This is how you can trigger it with CLI.

airflow trigger_dag --conf '{"randomKey":30}' airflow-test-dag

--conf options:
    randomKey:<INT> - Optional

"""



**Anything else we need to know**: Works fine with Airflow 1.10.10 and Python 3.7.

** UI Logs: **
```
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.7/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.7/site-packages/flask_admin/base.py", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/flask_admin/base.py", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/flask_login/utils.py", line 258, in decorated_view
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/utils.py", line 380, in view_func
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/utils.py", line 286, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/views.py", line 1749, in graph
    doc_md = wrapped_markdown(getattr(dag, 'doc_md', None), css_class='dag-doc')
  File "/usr/local/lib/python3.7/site-packages/airflow/www/views.py", line 258, in wrapped_markdown
    ).format(css_class=css_class)
  File "/usr/local/lib/python3.7/site-packages/markupsafe/__init__.py", line 213, in format
    return self.__class__(formatter.vformat(self, args, kwargs))
  File "/usr/local/Cellar/python/3.7.8/Frameworks/Python.framework/Versions/3.7/lib/python3.7/string.py", line 190, in vformat
    result, _ = self._vformat(format_string, args, kwargs, used_args, 2)
  File "/usr/local/Cellar/python/3.7.8/Frameworks/Python.framework/Versions/3.7/lib/python3.7/string.py", line 230, in _vformat
    obj, arg_used = self.get_field(field_name, args, kwargs)
  File "/usr/local/Cellar/python/3.7.8/Frameworks/Python.framework/Versions/3.7/lib/python3.7/string.py", line 295, in get_field
    obj = self.get_value(first, args, kwargs)
  File "/usr/local/Cellar/python/3.7.8/Frameworks/Python.framework/Versions/3.7/lib/python3.7/string.py", line 252, in get_value
    return kwargs[key]
  File "/usr/local/lib/python3.7/site-packages/markupsafe/__init__.py", line 249, in __getitem__
    return self._kwargs[key]
KeyError: '"randomKey"'
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_add_steps.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/sensors/test_emr_step.py']
Ground Truth : ['a/airflow/www/utils.py']
Current Recall: 0.022930049964311205

=========================================================

ISSUE: MySQL hook uses wrong autocommit calls for mysql-connector-python
**Apache Airflow version**: 2.0.1

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): n/a

**Environment**:

* **Cloud provider or hardware configuration**:  WSL2/Docker running `apache/airflow:2.0.1-python3.7` image
* **OS** (e.g. from /etc/os-release): Host: Ubuntu 20.04 LTS, Docker Image: Debian GNU/Linux 10 (buster)
* **Kernel** (e.g. `uname -a`): 5.4.72-microsoft-standard-WSL2 x86_64
* **Others**:  Docker version 19.03.8, build afacb8b7f0

**What happened**:

Received a `'bool' object is not callable` error when attempting to use the mysql-connector-python client for a task:
```
[2021-03-17 10:20:13,247] {{taskinstance.py:1455}} ERROR - 'bool' object is not callable
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1310, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/mysql/operators/mysql.py", line 74, in execute
    hook.run(self.sql, autocommit=self.autocommit, parameters=self.parameters)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/dbapi.py", line 175, in run
    self.set_autocommit(conn, autocommit)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/mysql/hooks/mysql.py", line 55, in set_autocommit
    conn.autocommit(autocommit)
```

**What you expected to happen**:

The task to run without complaints.

**How to reproduce it**:

Create and use a MySQL connection with `{"client": "mysql-connector-python"}` specified in the Extra field.

**Anything else we need to know**:

The MySQL hook seems to be using `conn.get_autocommit()` and `conn.autocommit()` to get/set the autocommit flag for both mysqlclient and mysql-connector-python. These method don't actually exist in mysql-connector-python as it uses autocommit as a property rather than a method.

I was able to work around it by adding an `if not callable(conn.autocommit)` condition to detect when mysql-connector-python is being used, but I'm sure there's probably a more elegant way of detecting which client is being used.

mysql-connector-python documentation:
https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlconnection-autocommit.html

Autocommit calls:
https://github.com/apache/airflow/blob/2a2adb3f94cc165014d746102e12f9620f271391/airflow/providers/mysql/hooks/mysql.py#L55
https://github.com/apache/airflow/blob/2a2adb3f94cc165014d746102e12f9620f271391/airflow/providers/mysql/hooks/mysql.py#L66

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/mysql/hooks/test_mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/mysql/operators/test_mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py']
Ground Truth : ['a/airflow/providers/mysql/hooks/mysql.py']
Current Recall: 0.025071377587437542

=========================================================

ISSUE: [AIRFLOW-4438] Add Gzip compression to S3_hook
 - Added bool parameter gzip to load_file to s3_hook
 - Tested the load_file with load_file_gzip unittest
 - Updated the load_file docstring to reflect the extra parameter

---
Issue link: [AIRFLOW-4438](https://issues.apache.org/jira/browse/AIRFLOW-4438)

Make sure to mark the boxes below before creating PR: [x]

- [x] Description above provides context of the change
- [x] Commit message/PR title starts with `[AIRFLOW-NNNN]`. AIRFLOW-NNNN = JIRA ID<sup>*</sup>
- [x] Unit tests coverage for changes (not needed for documentation changes)
- [x] Commits follow "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)"
- [x] Relevant documentation is updated including usage instructions.
- [x] I will engage committers as explained in [Contribution Workflow Example](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#contribution-workflow-example).

<sup>*</sup> For document-only changes commit message can start with `[AIRFLOW-XXXX]`.

---
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/weaviate/hooks/weaviate.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py']
Ground Truth : ['a/airflow/providers/amazon/aws/hooks/s3.py']
Current Recall: 0.025071377587437542

=========================================================

ISSUE: Using TaskGroup without context manager (Graph view visual bug)
**Apache Airflow version**: 2.0.0

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): n/a

**What happened**:
When I do not use the context manager for the task group and instead call the add function to add the tasks, those tasks show up on the Graph view.
![Screen Shot 2021-03-17 at 2 06 17 PM](https://user-images.githubusercontent.com/5952735/111544849-5939b200-8732-11eb-80dc-89c013aeb083.png) 

However, when I click on the task group item on the Graph UI, it will fix the issue. When I close the task group item, the tasks will not be displayed as expected.
![Screen Shot 2021-03-17 at 2 06 21 PM](https://user-images.githubusercontent.com/5952735/111544848-58a11b80-8732-11eb-928b-3c76207a0107.png)

**What you expected to happen**:
I expected the tasks inside the task group to not display on the Graph view.
![Screen Shot 2021-03-17 at 3 17 34 PM](https://user-images.githubusercontent.com/5952735/111545824-eaf5ef00-8733-11eb-99c2-75b051bfefe1.png)

**How to reproduce it**:
Render this DAG in Airflow

```python
from airflow.models import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime
with DAG(dag_id="example_task_group", start_date=datetime(2021, 1, 1), tags=["example"], catchup=False) as dag:
    start = BashOperator(task_id="start", bash_command='echo 1; sleep 10; echo 2;')
    tg = TaskGroup("section_1", tooltip="Tasks for section_1")
    task_1 = DummyOperator(task_id="task_1")
    task_2 = BashOperator(task_id="task_2", bash_command='echo 1')
    task_3 = DummyOperator(task_id="task_3")
    tg.add(task_1)
    tg.add(task_2)
    tg.add(task_3)
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_dot_renderer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_setup_teardown_taskflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_task_group_decorator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_setup_teardown.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/natural_language/example_natural_language.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_rendered.py']
Ground Truth : ['a/airflow/exceptions.py', 'a/airflow/utils/task_group.py']
Current Recall: 0.025071377587437542

=========================================================

ISSUE: Fix lost runs in `CronTriggerTimetable`
closes: #27399

Fix lost runs in `CronTriggerTimetable` by resetting seconds and microseconds in `start_time_candidates`.
<!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
 -->

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of an existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->



<!-- Please keep an empty line above the dashes. -->
---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/minor_release_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py']
Ground Truth : ['a/airflow/timetables/trigger.py']
Current Recall: 0.025071377587437542

=========================================================

ISSUE: DatabricksHook method get_run_state returns error
### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

apache-airflow-providers-databricks==1!2.0.2


### Apache Airflow version

2.1.4 (latest released)

### Operating System

Debian GNU/Linux 11 (bullseye)

### Deployment

Docker-Compose

### Deployment details

Running local deployment using Astronomer CLI

### What happened

When calling the `get_run_state` method from the `DatabricksHook`, I get the following error:

`TypeError: Object of type RunState is not JSON serializable`

I think this is due to [the method returning a RunState custom class](https://github.com/apache/airflow/blob/main/airflow/providers/databricks/hooks/databricks.py#L275) as opposed to a `str` like the rest of the methods in the databricks hook (i.e. `get_job_id`, `get_run_page_url`, etc.)

### What you expected to happen

When calling the `get_run_state` method, simply return the `result_state` or `state_message` [variables](https://github.com/apache/airflow/blob/main/airflow/providers/databricks/hooks/databricks.py#L287-L288) instead of the `RunState` class.

### How to reproduce

Create a dag that references a databricks deployment and use this task to see the error:

```
    from airflow.providers.databricks.hooks.databricks import DatabricksHook
    run_id = <insert run id from databricks ui here>

    def get_run_state(self, run_id: str):
           return self.hook.get_run_state(run_id=run_id)

    python_get_run_state = PythonOperator(
        task_id="python_get_run_state",
        python_callable=get_run_state,
        op_kwargs={
            "run_id": str(run_id)
        }
    )
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/databricks/hooks/test_databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/databricks/operators/test_databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks_repos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py']
Ground Truth : ['a/airflow/providers/databricks/hooks/databricks.py']
Current Recall: 0.02721270521056388

=========================================================

ISSUE: Add integration with Google Calendar API


**Description**

I ask for operator to create events table from the Google Calendar API
https://developers.google.com/calendar/v3/reference/events/import

**Use case / motivation**

we have calendar where we save events that has business impact and we need this data available for our analysts in tables.

I would also be nice if the hook will be able to insert events into the calendar based on logic from the pipeline.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/suite/hooks/calendar.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/suite/hooks/test_calendar.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/calendar_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/transfers/test_calendar_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_decorators.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/pagerduty/hooks/pagerduty.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py']
Ground Truth : ['/dev/null']
Current Recall: 0.02721270521056388

=========================================================

ISSUE: Clear notification in UI when duplicate dag names are present
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

**Description**

When using decorators to define dags, e.g. dag_1.py:

```python
from airflow.decorators import dag, task
from airflow.utils.dates import days_ago

DEFAULT_ARGS = {
    "owner": "airflow",
}


@task
def some_task():
    pass


@dag(
    default_args=DEFAULT_ARGS,
    schedule_interval=None,
    start_date=days_ago(2),
)
def my_dag():
    some_task()


DAG_1 = my_dag()

```

and

dag_2.py:

```python
from airflow.decorators import dag, task
from airflow.utils.dates import days_ago

DEFAULT_ARGS = {
    "owner": "airflow",
}


@task
def some_other_task():
    pass


@dag(
    default_args=DEFAULT_ARGS,
    schedule_interval=None,
    start_date=days_ago(2),
)
def my_dag():
    some_other_task()


DAG_2 = my_dag()

```

We have two different dags which have been written in isolation, but by sheer bad luck both define `my_dag()`. This seems fine for each file in isolation, but on the airflow UI, we only end up seeing one entry for `my_dag`, where it has picked up `dag_1.py` and ignored `dag_2.py`.

**Use case / motivation**

We currently end up with only one DAG showing up on the UI, and no indication as to why the other one hasn't appeared.

Suggestion: popup similar to 'DAG import error' to highlight what needs changing in one of the DAG files in order for both to show up ("DAG import error: duplicate dag names found - please review {duplicate files} and ensure all dag definitions are unique"?)

**Are you willing to submit a PR?**

No time to spare on this at present

**Related Issues**

I haven't found any related issues with the search function.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py']
Ground Truth : ['a/airflow/exceptions.py', 'a/airflow/providers/papermill/example_dags/example_papermill.py', 'a/airflow/models/dagbag.py', 'a/airflow/example_dags/tutorial_taskflow_api_etl_virtualenv.py']
Current Recall: 0.02721270521056388

=========================================================

ISSUE: Airflow Scheduler may set running task instance state into None state in multiple scheduler deployment
### Apache Airflow version

2.1.4

### Operating System

Office Docker Image on Amazon Linux 2 Host

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

We deploy 2 schedulers, 2 workers,  2 webservers, and use MySQL8 as the Database backend. We have over 2000 small DAGs.

### What happened

We have been using this 2.1.4 for all most one month. In recent days, we found some of the tasks failed because the following error:

```
worker>     [2021-10-28 02:51:29,343] {local_task_job.py:208} WARNING - State of this instance has been externally set to None. Terminating instance.
worker>     [2021-10-28 02:51:29,347] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 25492
worker>     [2021-10-28 02:51:29,347] {taskinstance.py:1236} ERROR - Received SIGTERM. Terminating subprocesses.
worker>     [2021-10-28 02:51:29,348] {custom_task_operator.py:230} INFO - receive kill signal , kill batch result
worker>     [2021-10-28 02:51:29,348] {custom_task_operator.py:232} INFO - start kill batch task :434649
```

This kind of error only happened occasionally once or twice a day for some executation_date for some tasks in some different DAGs.  I checked all the schedulers and worker log related to a failed task. Below are the logs combined and sort by time.

```
scheduler2> [2021-10-28 02:51:23,676] {base_executor.py:82} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'schedule_12238', 'task_schedule_12238', '2021-10-28T02:35:00+00:00', '--local', '--pool', 'livy_pool', '--subdir', '/usr/local/airflow/dags/schedule_12238.py']
scheduler2> [2021-10-28 02:51:25,450] {scheduler_job.py:612} INFO - Executor reports execution of schedule_12238.task_schedule_12238 execution_date=2021-10-28 02:35:00+00:00 exited with status queued for try_number 1
scheduler2> [2021-10-28 02:51:25,483] {scheduler_job.py:639} INFO - Setting external_id for <TaskInstance: schedule_12238.task_schedule_12238 2021-10-28 02:35:00+00:00 [None]> to 28123ae7-5f11-4cca-8db7-8509d7794299

worker>     [2021-10-28 02:51:29,343] {local_task_job.py:208} WARNING - State of this instance has been externally set to None. Terminating instance.
worker>     [2021-10-28 02:51:29,347] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 25492
worker>     [2021-10-28 02:51:29,347] {taskinstance.py:1236} ERROR - Received SIGTERM. Terminating subprocesses.
worker>     [2021-10-28 02:51:29,348] {custom_task_operator.py:230} INFO - receive kill signal , kill batch result
worker>     [2021-10-28 02:51:29,348] {custom_task_operator.py:232} INFO - start kill batch task :434649

scheduler2> [2021-10-28 02:51:52,225] {scheduler_job.py:612} INFO - Executor reports execution of schedule_12238.task_schedule_12238 execution_date=2021-10-28 02:35:00+00:00 exited with status success for try_number 1

scheduler1> [2021-10-28 02:53:01,244] {scheduler_job.py:411} INFO - DAG schedule_12238 has 0/10 running and queued tasks
scheduler1> [2021-10-28 02:53:01,255] {scheduler_job.py:518} INFO - Sending TaskInstanceKey(dag_id='schedule_12238', task_id='task_schedule_12238', execution_date=datetime.datetime(2021, 10, 28, 2, 35, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default
scheduler1> [2021-10-28 02:53:01,255] {base_executor.py:82} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'schedule_12238', 'task_schedule_12238', '2021-10-28T02:35:00+00:00', '--local', '--pool', 'livy_pool', '--subdir', '/usr/local/airflow/dags/schedule_12238.py']
scheduler1> [2021-10-28 02:53:02,272] {scheduler_job.py:612} INFO - Executor reports execution of schedule_12238.task_schedule_12238 execution_date=2021-10-28 02:35:00+00:00 exited with status queued for try_number 2
scheduler1> [2021-10-28 02:53:02,295] {scheduler_job.py:639} INFO - Setting external_id for <TaskInstance: schedule_12238.task_schedule_12238 2021-10-28 02:35:00+00:00 [running]> to d09a34e8-627a-4b47-9cbc-b1bbdbdc1023
scheduler1> [2021-10-28 02:54:53,773] {scheduler_job.py:612} INFO - Executor reports execution of schedule_12238.task_schedule_12238 execution_date=2021-10-28 02:35:00+00:00 exited with status success for try_number 2

scheduler2> [2021-10-28 02:54:53,795] {dagrun.py:477} INFO - Marking run <DagRun schedule_12238 @ 2021-10-28 02:35:00+00:00: scheduled__2021-10-28T02:35:00+00:00, externally triggered: False> successful
```

We can see after scheduler 2 output this line:
```
scheduler2> [2021-10-28 02:51:25,483] {scheduler_job.py:639} INFO - Setting external_id for <TaskInstance: schedule_12238.task_schedule_12238 2021-10-28 02:35:00+00:00 [None]> to 28123ae7-5f11-4cca-8db7-8509d7794299
```
The worker output `State of this instance has been externally set to None`. So I doubt that it was scheduler 2 wrongly setting the None state to the task instance.  I checked the related code in scheduler_job.py, due to my limited understanding of the airflow source code, I could find out the exact cause. Can anyone confirm whether this is an airflow bug?



### What you expected to happen

No `State of this instance has been externally set to None` error happened.

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/parallel.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_branch_datetime_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.02721270521056388

=========================================================

ISSUE: pdb no longer works with airflow test command since 2.3.3
Converted back to issue as I've reproduced it and traced the issue back to https://github.com/apache/airflow/pull/24362

### Discussed in https://github.com/apache/airflow/discussions/26352

<div type='discussions-op-text'>

<sup>Originally posted by **GuruComposer** September 12, 2022</sup>
### Apache Airflow version

2.3.4

### What happened

I used to be able to use ipdb to debug DAGs by running `airflow tasks test <dag_name> <dag_id>`, and hitting an ipdb breakpoint (ipdb.set_trace()).

This no longer works. I get a strange type error:

```  File "/usr/local/lib/python3.9/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/local/lib/python3.9/bdb.py", line 112, in dispatch_line
    self.user_line(frame)
  File "/usr/local/lib/python3.9/pdb.py", line 262, in user_line
    self.interaction(frame, None)
  File "/home/astro/.local/lib/python3.9/site-packages/IPython/core/debugger.py", line 336, in interaction
    OldPdb.interaction(self, frame, traceback)
  File "/usr/local/lib/python3.9/pdb.py", line 357, in interaction
    self._cmdloop()
  File "/usr/local/lib/python3.9/pdb.py", line 322, in _cmdloop
    self.cmdloop()
  File "/usr/local/lib/python3.9/cmd.py", line 126, in cmdloop
    line = input(self.prompt)
TypeError: an integer is required (got type NoneType)```



### What you think should happen instead

I should get the ipdb shell.

### How to reproduce

1. Add ipdb breakpoint anywhere in airflow task.
import ipdb; ipdb.set_trace()

2. Run that task:
Run `airflow tasks test <dag_name> <dag_id>`, and 

### Operating System

Debian GNU/Linux

### Versions of Apache Airflow Providers

2.3.4

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/sftp/operators/test_sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_add_steps.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/example_dags/update_example_dags_paths.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py']
Ground Truth : ['a/airflow/utils/log/secrets_masker.py']
Current Recall: 0.02721270521056388

=========================================================

ISSUE: Many tasks updating dataset at once causes some of them to fail
### Apache Airflow version

main (development)

### What happened

I have 16 dags which all update the same dataset.  They're set to finish at the same time (when the seconds on the clock are 00). About three quarters of them behave as expected, but the other quarter fails with errors like:

```
[2022-07-21, 06:06:00 UTC] {standard_task_runner.py:97} ERROR - Failed to execute job 8 for task increment_source ((psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dataset_dag_run_queue_pkey"
DETAIL:  Key (dataset_id, target_dag_id)=(1, simple_dataset_sink) already exists.

[SQL: INSERT INTO dataset_dag_run_queue (dataset_id, target_dag_id, created_at) VALUES (%(dataset_id)s, %(target_dag_id)s, %(created_at)s)]
[parameters: {'dataset_id': 1, 'target_dag_id': 'simple_dataset_sink', 'created_at': datetime.datetime(2022, 7, 21, 6, 6, 0, 131730, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 375)
```

I've prepaired a gist with the details: https://gist.github.com/MatrixManAtYrService/b5e58be0949eab9180608d0760288d4d

### What you think should happen instead

All dags should succeed

### How to reproduce

See this gist:  https://gist.github.com/MatrixManAtYrService/b5e58be0949eab9180608d0760288d4d

Summary:  Unpause all of the dags which we expect to collide, wait two minutes.  Some will have collided.

### Operating System

docker/debian

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

`astro dev start` targeting commit: cff7d9194f549d801947f47dfce4b5d6870bfaaa

be sure to have `pause` in requirements.txt

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/datasets/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dataset.py']
Ground Truth : ['a/airflow/models/dataset.py', 'a/airflow/datasets/manager.py']
Current Recall: 0.02721270521056388

=========================================================

ISSUE: Support telegram-bot v20+
### Body

Currently our telegram integration uses Telegram v13 telegram-bot library. On 1st of Jan 2023 a new, backwards incompatible version of Telegram-bot has been released : https://pypi.org/project/python-telegram-bot/20.0/#history and at least as reported by MyPy and our test suite test failures, Telegram 20 needs some changes to work:

Here is a transition guide that might be helpful. 

Transition guide is here: https://github.com/python-telegram-bot/python-telegram-bot/wiki/Transition-guide-to-Version-20.0

In the meantime we limit telegram to < 20.0.0

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/telegram/hooks/telegram.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/telegram/operators/telegram.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/telegram/hooks/test_telegram.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/telegram/operators/test_telegram.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/telegram/example_telegram.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/telegram/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/testing_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/providers/telegram/hooks/telegram.py']
Current Recall: 0.029354032833690216

=========================================================

ISSUE: Airflow scheduler crashed with TypeError: '>=' not supported between instances of 'datetime.datetime' and 'NoneType'
### Apache Airflow version

2.1.4

### Operating System

Ubuntu 20.04.3 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

Airflow scheduler crashed with following exception
```
[2021-11-23 22:41:16,528] {scheduler_job.py:662} INFO - Starting the scheduler
[2021-11-23 22:41:16,528] {scheduler_job.py:667} INFO - Processing each file at most -1 times
[2021-11-23 22:41:16,639] {manager.py:254} INFO - Launched DagFileProcessorManager with pid: 19
[2021-11-23 22:41:16,641] {scheduler_job.py:1217} INFO - Resetting orphaned tasks for active dag runs
[2021-11-23 22:41:16,644] {settings.py:51} INFO - Configured default timezone Timezone('Etc/GMT-7')
[2021-11-23 22:41:19,016] {scheduler_job.py:711} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/scheduler_job.py", line 695, in _execute
    self._run_scheduler_loop()
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/scheduler_job.py", line 788, in _run_scheduler_loop
    num_queued_tis = self._do_scheduling(session)
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/scheduler_job.py", line 901, in _do_scheduling
    callback_to_run = self._schedule_dag_run(dag_run, session)
  File "/usr/local/lib/python3.8/dist-packages/airflow/jobs/scheduler_job.py", line 1143, in _schedule_dag_run
    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)
  File "/usr/local/lib/python3.8/dist-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/airflow/models/dagrun.py", line 438, in update_state
    info = self.task_instance_scheduling_decisions(session)
  File "/usr/local/lib/python3.8/dist-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/airflow/models/dagrun.py", line 539, in task_instance_scheduling_decisions
    schedulable_tis, changed_tis = self._get_ready_tis(scheduleable_tasks, finished_tasks, session)
  File "/usr/local/lib/python3.8/dist-packages/airflow/models/dagrun.py", line 565, in _get_ready_tis
    if st.are_dependencies_met(
  File "/usr/local/lib/python3.8/dist-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/airflow/models/taskinstance.py", line 890, in are_dependencies_met
    for dep_status in self.get_failed_dep_statuses(dep_context=dep_context, session=session):
  File "/usr/local/lib/python3.8/dist-packages/airflow/models/taskinstance.py", line 911, in get_failed_dep_statuses
    for dep_status in dep.get_dep_statuses(self, session, dep_context):
  File "/usr/local/lib/python3.8/dist-packages/airflow/ti_deps/deps/base_ti_dep.py", line 101, in get_dep_statuses
    yield from self._get_dep_statuses(ti, session, dep_context)
  File "/usr/local/lib/python3.8/dist-packages/airflow/ti_deps/deps/ready_to_reschedule.py", line 66, in _get_dep_statuses
    if now >= next_reschedule_date:
TypeError: '>=' not supported between instances of 'datetime.datetime' and 'NoneType'
[2021-11-23 22:41:20,020] {process_utils.py:100} INFO - Sending Signals.SIGTERM to GPID 19
```

### What you expected to happen

_No response_

### How to reproduce

Define a `BaseSensorOperator` task with large `poke_interval` with `reschedule` mode

```
BaseSensorOperator(
    task_id='task',
    poke_interval=863998946,
    mode='reschedule',
    dag=dag
)
```

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/install_airflow_and_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_prepare_airflow_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/sensors/base.py']
Current Recall: 0.029354032833690216

=========================================================

ISSUE: Context deprecation warnings when they aren't used
### Apache Airflow version

2.2.3rc1 (release candidate)

### What happened

I ran a DAG that does nothing explicitly with context, so I'm surprised to see deprecation warnings about context being used (trimmed to cut down on noise):

```
baseoperator.py:1107 DeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
baseoperator.py:1107 DeprecationWarning: Accessing 'next_ds' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds }}' instead.
baseoperator.py:1107 DeprecationWarning: Accessing 'next_ds_nodash' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds_nodash }}' instead.
baseoperator.py:1107 DeprecationWarning: Accessing 'next_execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_end' instead.
baseoperator.py:1107 DeprecationWarning: Accessing 'prev_ds' from the template is deprecated and will be removed in a future version.
baseoperator.py:1107 DeprecationWarning: Accessing 'prev_ds_nodash' from the template is deprecated and will be removed in a future version.
baseoperator.py:1107 DeprecationWarning: Accessing 'prev_execution_date' from the template is deprecated and will be removed in a future version.
baseoperator.py:1107 DeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
baseoperator.py:1107 DeprecationWarning: Accessing 'tomorrow_ds' from the template is deprecated and will be removed in a future version.
baseoperator.py:1107 DeprecationWarning: Accessing 'tomorrow_ds_nodash' from the template is deprecated and will be removed in a future version.
baseoperator.py:1107 DeprecationWarning: Accessing 'yesterday_ds' from the template is deprecated and will be removed in a future version.
baseoperator.py:1107 DeprecationWarning: Accessing 'yesterday_ds_nodash' from the template is deprecated and will be removed in a future version.
```

### What you expected to happen

I'd only expect to see deprecation warnings about things my DAG is actually interacting with.

### How to reproduce

Run this DAG:

```python
from datetime import datetime

from airflow import DAG
from airflow.operators.bash import BashOperator

with DAG(
    dag_id="no-context", schedule_interval=None, start_date=datetime(2021, 12, 10)
) as dag:
    BashOperator(task_id="sleep", bash_command="echo 'Hello!'")
```

Note: I also tried a TaskFlow DAG and that wasn't impacted.

### Operating System

MacOS 11.6

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

This was introduced in #19886 (cc @uranusjr).

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/context.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/sql_to_slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/sensors/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/connection_wrapper.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/operators/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py']
Ground Truth : ['a/airflow/models/param.py', 'a/airflow/models/baseoperator.py', 'a/airflow/utils/helpers.py', 'a/airflow/ti_deps/deps/trigger_rule_dep.py', 'a/airflow/utils/log/file_task_handler.py', 'a/airflow/models/xcom_arg.py', 'a/airflow/utils/context.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: error on click in dag-dependencies - airflow 2.1
Python version: 3.7.9

Airflow version: 2.1.0

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.7/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/auth.py", line 34, in decorated
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/decorators.py", line 97, in view_func
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/decorators.py", line 60, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www/views.py", line 4003, in list
    if SerializedDagModel.get_max_last_updated_datetime() > self.last_refresh:
TypeError: '>' not supported between instances of 'NoneType' and 'datetime.datetime'

**What you expected to happen**:
See the dags dependencies

**What do you think went wrong?**
It's happen only if I don't have any dag yet.

**How to reproduce it**:
With any dag created click in 
menu -> browser -> dag-dependencies
<!---


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_mypy_folder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/sbom_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/sensors/test_emr_step.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_add_steps.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py']
Ground Truth : ['a/airflow/models/serialized_dag.py', 'a/airflow/www/views.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: Delete DAG REST API Functionality
### Description

The stable REST API seems to be missing the functionality to delete a DAG. 
This would mirror clicking the "Trash Can" on the UI (and could maybe eventually power it)

![image](https://user-images.githubusercontent.com/80706212/131702280-2a8d0d74-6388-4b84-84c1-efabdf640cde.png)


### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py']
Ground Truth : ['a/airflow/exceptions.py', 'a/airflow/www/views.py', 'a/airflow/api_connexion/endpoints/dag_endpoint.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: Automate language matters check
Please take a look at the last commit only. It is based on #9174 

---
Make sure to mark the boxes below before creating PR: [x]

- [x] Description above provides context of the change
- [x] Unit tests coverage for changes (not needed for documentation changes)
- [x] Target Github ISSUE in description if exists
- [x] Commits follow "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)"
- [x] Relevant documentation is updated including usage instructions.
- [x] I will engage committers as explained in [Contribution Workflow Example](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#contribution-workflow-example).

---
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_command.py']
Ground Truth : ['a/airflow/contrib/plugins/metastore_browser/main.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/config_templates/default_webserver_config.py', 'a/airflow/providers/apache/hive/operators/hive_stats.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: Dataset Next Trigger Modal Not Populating Latest Update
### Apache Airflow version

2.4.1

### What happened

When using dataset scheduling, it isn't obvious which datasets a downstream dataset consumer is awaiting in order for the DAG to be scheduled.

I would assume that this is supposed to be solved by the `Latest Update` column in the modal that opens when selecting `x of y datasets updated`, but it appears that the data isn't being populated.

<img width="601" alt="image" src="https://user-images.githubusercontent.com/5778047/194116186-d582cede-c778-47f7-8341-fc13a69a2358.png">

Although one of the datasets has been produced, there is no data in the `Latest Update` column of the modal.

In the above example, both datasets have been produced > 1 time.

<img width="581" alt="image" src="https://user-images.githubusercontent.com/5778047/194116368-ceff241f-a623-4893-beb7-637b821c4b53.png">

<img width="581" alt="image" src="https://user-images.githubusercontent.com/5778047/194116410-19045f5a-8400-47b0-afcb-9fbbffca26ee.png">


### What you think should happen instead

The `Latest Update` column should be populated with the latest update timestamp for each dataset required to schedule a downstream, dataset consuming DAG.

Ideally there would be some form of highlighting on the "missing" datasets for quick visual feedback when DAGs have a large number of datasets required for scheduling.

### How to reproduce

1. Create a DAG (or 2 individual DAGs) that produces 2 datasets
2. Produce both datasets
3. Then produce _only one_ dataset
4. Check the modal by clicking from the home screen on the `x of y datasets updated` button.

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_datasets.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_serialized_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: SQLTableCheckOperator fails for Postgres
### Apache Airflow version

2.3.3

### What happened

`SQLTableCheckOperator` fails when used with Postgres.

### What you think should happen instead

From the logs:
```
[2022-08-19, 09:28:14 UTC] {taskinstance.py:1910} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/common/sql/operators/sql.py", line 296, in execute
    records = hook.get_first(self.sql)
  File "/usr/local/lib/python3.9/site-packages/airflow/hooks/dbapi.py", line 178, in get_first
    cur.execute(sql)
psycopg2.errors.SyntaxError: subquery in FROM must have an alias
LINE 1: SELECT MIN(row_count_check) FROM (SELECT CASE WHEN COUNT(*) ...
                                         ^
HINT:  For example, FROM (SELECT ...) [AS] foo.
```

### How to reproduce

```python
import pendulum
from datetime import timedelta

from airflow import DAG
from airflow.decorators import task
from airflow.providers.common.sql.operators.sql import SQLTableCheckOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator

_POSTGRES_CONN = "postgresdb"
_TABLE_NAME = "employees"

default_args = {
    "owner": "cs",
    "retries": 3,
    "retry_delay": timedelta(seconds=15),
    }

with DAG(
    dag_id="sql_data_quality",
    start_date=pendulum.datetime(2022, 8, 1, tz="UTC"),
    schedule_interval=None,
) as dag:

    create_table = PostgresOperator(
        task_id="create_table",
        postgres_conn_id=_POSTGRES_CONN,
        sql=f"""
        CREATE TABLE IF NOT EXISTS {_TABLE_NAME} (
            employee_name VARCHAR NOT NULL,
            employment_year INT NOT NULL
        );
        """
    )

    populate_data = PostgresOperator(
        task_id="populate_data",
        postgres_conn_id=_POSTGRES_CONN,
        sql=f"""
            INSERT INTO {_TABLE_NAME} VALUES ('Adam', 2021);
            INSERT INTO {_TABLE_NAME} VALUES ('Chris', 2021);
            INSERT INTO {_TABLE_NAME} VALUES ('Frank', 2021);
            INSERT INTO {_TABLE_NAME} VALUES ('Fritz', 2021);
            INSERT INTO {_TABLE_NAME} VALUES ('Magda', 2022);
            INSERT INTO {_TABLE_NAME} VALUES ('Phil', 2021);
        """,
    )

    check_row_count = SQLTableCheckOperator(
        task_id="check_row_count",
        conn_id=_POSTGRES_CONN,
        table=_TABLE_NAME,
        checks={
            "row_count_check": {"check_statement": "COUNT(*) >= 3"}
        },
    )

    drop_table = PostgresOperator(
        task_id="drop_table",
        trigger_rule="all_done",
        postgres_conn_id=_POSTGRES_CONN,
        sql=f"DROP TABLE {_TABLE_NAME};",
    )

    create_table >> populate_data >> check_row_count >> drop_table
```

### Operating System

macOS

### Versions of Apache Airflow Providers

`apache-airflow-providers-common-sql==1.0.0`

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/common/sql/operators/test_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/postgres/operators/test_postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/postgres/example_postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_s3_to_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_redshift.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/mysql/operators/test_mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/sql_to_sheets/example_sql_to_sheets.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/transfers/test_postgres_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/pgvector/example_pgvector.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/snowflake/operators/test_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/transfers/example_postgres_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/common/sql/example_sql_column_table_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/pgvector/example_pgvector_openai.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/teradata/example_teradata_to_teradata_transfer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks_sql.py']
Ground Truth : ['a/airflow/providers/common/sql/operators/sql.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: BUG: missing status graphics for dag_ids with periods
Based on some experimentation, it looks like status circles on the main web server page don't appear for `dag_id`s that contain periods. Changing a dag_id from `test.dag` to `test_dag` was enough to make the circles reappear. Since I can see the dag_stats json is generated correctly, perhaps this is a problem with how d3 interprets the json? 

These are the circles I mean (note the missing middle row):
![screen shot 2015-09-20 at 11 25 46 pm](https://cloud.githubusercontent.com/assets/153965/9984897/f3660ade-5fee-11e5-8a12-fcc4394b41c2.png)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jenkins/operators/jenkins_job_trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/client/json_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py']
Ground Truth : ['a/airflow/models.py', 'a/airflow/www/app.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: [ldap] section in configuration is not applicable anymore in 2.0
**Apache Airflow version**: 2.0.0b* / master


**What happened**:

`[ldap]` section in `airflow.cfg` is not applicable anymore in 2.0 and `master`, because the LDAP authentication (for webserver and API) is handled by FAB, and the configuration for this is handled by `webserver_config.py` file.

![airflow__default_airflow_cfg](https://user-images.githubusercontent.com/11539188/99289209-ef210700-283c-11eb-948e-16811ba09293.png)


**What you expected to happen**:

The `[ldap]` section should be removed from `airflow/config_templates/default_airflow.cfg` and `airflow/config_templates/config.yml` (and some other applicable files).

Otherwise leaving this section there will be a big confusion for users.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/default_webserver_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/validate_version_added_fields_in_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/app.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/security.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_config_endpoint.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: task_command.task_run log handling is either broken or redundant
**Apache Airflow version**: 2.0.0a2

As of #9363, the `tasks run` command attempts to restore the root logger handlers to the previous state after running the task with the handlers all replaced by those from the `airflow.tasks` logger.

However, because no actual *copy* is created of the root handlers list, what you effectively end up with is either an empty list of handlers and nothing is restored. To make matters worse, the block ends with a `logging.shutdown()` call, which completely closes and releases all handlers.

So, either the code to 'restore' can just be removed, or a proper copy needs to be made and the `logging.shutdown()` call needs to be removed from the command. `logging.shutdown` is going to be called when the interpreter exits, anyway.

In detail, this is the code section:

https://github.com/apache/airflow/blob/bec9f3b29fd42ecd1beae3db75784b9a726caf15/airflow/cli/commands/task_command.py#L194-L222

The first problem here is that `root_logger_handlers = root_logger.handlers` merely creates a reference to the logger `handlers` list. The `root_logger.removeHandler(handler)` in a loop further on remove the handlers from that same list as you iterate, and this causes it to [only remove *every second handler*](https://sopython.com/canon/95/removing-items-from-a-list-while-iterating-over-the-list/). Luckily, this was never a problem because in the standard configuration there is only a single handler on the root. If there are more, there is a bigger problem further down.

Continuing on, the next loop, adding handlers from `airflow_logger_handlers`, causes those same handlers to show up in the `root_logger_handlers` reference, but those very same handlers are removed again in a second loop over the same `airflow_logger_handlers` list.

So when you hit the final `for handler in root_logger_handlers: root_logger.addHandler(handler)` loop, either `root_logger_handlers` is an empty list (no handlers added), or, if you started with more than one root handler and every second handler was left in place, you now are adding handlers to the list that are already in the list.  The `Logger.addHandler()` method uses `self.handler.append()`, appending to a list you are iterating over, and so you have an infinite loop on your hands.

Either just remove all handlers from the root logger (taking care to loop over the list in reverse, e.g. `for h in reversed(root_logger.handlers): root_logger.removeHandler(h)`), or create proper copy of the list with `root_logger_handlers = root_logger.handlers[:]`. The `logging.shutdown()` call has to be removed then too.





Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/process_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/s3_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/logging_mixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job_logging.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/vertica/hooks/vertica.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: Typo in Sensor: GCSObjectsWtihPrefixExistenceSensor (should be GCSObjectsWithPrefixExistenceSensor)
Typo in Google Cloud Storage sensor: airflow/providers/google/cloud/sensors/gcs/GCSObjectsWithPrefixExistenceSensor

The word _With_ is spelled incorrectly. It should be: GCSObjects**With**PrefixExistenceSensor

**Apache Airflow version**: 2.0.0
**Environment**:
- **Cloud provider or hardware configuration**: Google Cloud
- **OS** (e.g. from /etc/os-release): Mac OS BigSur







Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/gcs/example_gcs_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/bigtable/example_bigtable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/triggers/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/local_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/gcs_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/credentials_provider.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_composer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/secrets/secret_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataproc_metastore.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py']
Ground Truth : ['a/airflow/contrib/sensors/gcs_sensor.py', 'a/airflow/providers/google/cloud/sensors/gcs.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: PostgresToGCSOperator fail on empty table and use_server_side_cursor=True
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-google==6.1.0

### Apache Airflow version

2.2.2 (latest released)

### Operating System

Debian GNU/Linux 10 (buster)

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

When I'm execute `PostgresToGCSOperator` on empty table and set `use_server_side_cursor=True` the operator fails with error:
```
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1332, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1458, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1514, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/transfers/sql_to_gcs.py", line 154, in execute
    files_to_upload = self._write_local_data_files(cursor)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/transfers/sql_to_gcs.py", line 213, in _write_local_data_files
    row = self.convert_types(schema, col_type_dict, row)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/google/cloud/transfers/sql_to_gcs.py", line 174, in convert_types
    return [self.convert_type(value, col_type_dict.get(name)) for name, value in zip(schema, row)]
TypeError: 'NoneType' object is not iterable
```

Operator command when I'm using:
```python
task_send = PostgresToGCSOperator(
            task_id=f'send_{table}',
            postgres_conn_id='postgres_raw',
            gcp_conn_id=gcp_conn_id,
            sql=f'SELECT * FROM public.{table}',
            use_server_side_cursor=True,
            bucket=bucket,
            filename=f'{table}.csv',
            export_format='csv',
        )
```



### What you expected to happen

I'm expected, that operator on empty table not creating file and no upload it on Google Cloud.

### How to reproduce

- Create empty postgresql table.
- Create dag with task with PostgresToGCSOperator. that upload this table in Google Cloud.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/sql_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/cassandra_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/postgres_to_gcs.py']
Current Recall: 0.029659936779851123

=========================================================

ISSUE: DbApiHook: add chunksize to get_pandas_df parameters
**Description**

The method "get_pandas_df" should be extended by the parameter "chunksize".

**Use case / motivation**

For the case that very large amounts of data are read, the "chunksize" is a good way to reduce the size of individual queries.

**Links**

https://github.com/apache/airflow/blob/f4faed6eea70e34da60edcb555fffc9099d3214a/airflow/hooks/dbapi_hook.py#L123

https://github.com/pandas-dev/pandas/blob/0.25.x/pandas/io/sql.py#L344

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/exasol/hooks/exasol.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/sql_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/base_sql_to_slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/presto/hooks/presto.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/weaviate/hooks/weaviate.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/slack/transfers/test_base_sql_to_slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/trino/hooks/trino.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/sql_to_slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/connection.py']
Ground Truth : ['a/airflow/providers/presto/hooks/presto.py', 'a/airflow/hooks/dbapi_hook.py', 'a/airflow/providers/exasol/hooks/exasol.py', 'a/airflow/providers/google/cloud/hooks/bigquery.py', 'a/airflow/providers/apache/hive/hooks/hive.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: Graph View is empty when Operator has multiline string in args (v2.0)
Airflow v2.0b3
 Kubernetes v1.19.3

Discovered issue while testing KubernetesPodOperator (haven't tested with other operator).
If I create a multiline string using """ """", add some variables inside (Jinja templating), then use this string as an argument to KubernetesPodOperator:
- In Graph View DAG is not visible (just gray area where it should be a digraph);
- in browser's web console i see the following error:

`Uncaught TypeError: node is undefined
    preProcessGraph http://localhost:8080/static/dist/dagre-d3.min.js:103
    preProcessGraph http://localhost:8080/static/dist/dagre-d3.min.js:103
    fn http://localhost:8080/static/dist/dagre-d3.min.js:103
    call http://localhost:8080/static/dist/d3.min.js:3
    draw http://localhost:8080/graph?dag_id=mydag&execution_date=mydate
    expand_group http://localhost:8080/graph?dag_id=mydag&execution_date=mydate
    <anonymous> http://localhost:8080/graph?dag_id=mydag&execution_date=mydate`

Tree view works without issues in this case. The DAG succeeds.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/smtp/notifications/test_smtp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/http/hooks/test_http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/www.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_app.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/datasets/test_dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_trigger_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/docker/hooks/test_docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py']
Ground Truth : ['a/airflow/models/baseoperator.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: Bring in more resolution to hivestats


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/operators/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_xcomargs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ftp/operators/ftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serde.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py']
Ground Truth : ['a/airflow/providers/amazon/aws/triggers/redshift_cluster.py', 'a/airflow/providers/amazon/aws/operators/redshift_cluster.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: [AIRFLOW-XXX]  airflow.models.DAG docstring mistake correction
Make sure you have checked _all_ steps below.

### Jira

- [ x ] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, "\[AIRFLOW-XXX\] My Airflow PR"
  - https://issues.apache.org/jira/browse/AIRFLOW-XXX
  - In case you are fixing a typo in the documentation you can prepend your commit with \[AIRFLOW-XXX\], code changes always need a Jira issue.

### Description

- [ x ] Here are some details about my PR, including screenshots of any UI changes:

### Tests

- [ x ] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:

### Commits

- [ x ] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)":
  1. Subject is separated from body by a blank line
  1. Subject is limited to 50 characters (not including Jira issue reference)
  1. Subject does not end with a period
  1. Subject uses the imperative mood ("add", not "adding")
  1. Body wraps at 72 characters
  1. Body explains "what" and "why", not "how"

### Documentation

- [ x ] In case of new functionality, my PR adds documentation that describes how to use it.
  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.

### Code Quality

- [ x ] Passes `git diff upstream/master -u -- "*.py" | flake8 --diff`


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/minor_release_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/exampleinclude.py']
Ground Truth : ['/dev/null']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: Custom XCom backends circular import when using cli command like airflow connections list
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 2.0.1


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:

While running `airflow connections list` with a `xcom_backend = xcom.MyXComBackend` in `airflow.cfg` I will get a 
 
    ImportError: cannot import name 'BaseHook' from partially initialized module 'airflow.hooks.base' (most likely due to a circular import) (/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/hooks/base.py)

(see below for complete traceback).


The issue seems to be that `airflow/cli/commands/connection_command.py` imports `BaseHook` directly and `BaseHook -> Connection -> BaseOperator -> TaskInstance -> XCom -> MyXComBackend -> S3Hook -> BaseHook ` (again see complete traceback below)
    
```
airflow connections list
[2021-04-12 10:51:34,020] {configuration.py:459} ERROR - cannot import name 'BaseHook' from partially initialized module 'airflow.hooks.base' (most likely due to a circular import) (/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/hooks/base.py)
Traceback (most recent call last):
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/configuration.py", line 457, in getimport
    return import_string(full_qualified_path)
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/utils/module_loading.py", line 32, in import_string
    module = import_module(module_path)
  File "/Users/rubelagu/.pyenv/versions/3.8.8/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/rubelagu/tmp/airflow-xcom-backend/plugins/xcom.py", line 4, in <module>
    from airflow.providers.amazon.aws.hooks.s3 import S3Hook
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 37, in <module>
    from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 41, in <module>
    from airflow.hooks.base import BaseHook
ImportError: cannot import name 'BaseHook' from partially initialized module 'airflow.hooks.base' (most likely due to a circular import) (/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/hooks/base.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 47, in command
    func = import_string(import_path)
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/utils/module_loading.py", line 32, in import_string
    module = import_module(module_path)
  File "/Users/rubelagu/.pyenv/versions/3.8.8/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/cli/commands/connection_command.py", line 30, in <module>
    from airflow.hooks.base import BaseHook
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/hooks/base.py", line 23, in <module>
    from airflow.models.connection import Connection
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/models/__init__.py", line 20, in <module>
    from airflow.models.baseoperator import BaseOperator, BaseOperatorLink
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 55, in <module>
    from airflow.models.taskinstance import Context, TaskInstance, clear_task_instances
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 58, in <module>
    from airflow.models.xcom import XCOM_RETURN_KEY, XCom
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/models/xcom.py", line 289, in <module>
    XCom = resolve_xcom_backend()
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/models/xcom.py", line 279, in resolve_xcom_backend
    clazz = conf.getimport("core", "xcom_backend", fallback=f"airflow.models.xcom.{BaseXCom.__name__}")
  File "/Users/rubelagu/tmp/airflow-xcom-backend/venv/lib/python3.8/site-packages/airflow/configuration.py", line 460, in getimport
    raise AirflowConfigException(
airflow.exceptions.AirflowConfigException: The object could not be loaded. Please check "xcom_backend" key in "core" section. Current value: "xcom.MyXComBackend".
```

**What you expected to happen**:

I expected to be able to use S3Hook or GCSHook from a cusom XCom backedn following the example in what I though was the canonical example at https://medium.com/apache-airflow/airflow-2-0-dag-authoring-redesigned-651edc397178 



**How to reproduce it**:

* In `airflow.cfg` set `xcom_backend = xcom.MyXComBackend`
* Create file `plugins/xcom.py` with the following contents
```
from airflow.models.xcom import BaseXCom
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

class MyXComBackend(BaseXCom):
    pass
```


**Anything else we need to know**:

This can be workaround by not importing the hooks on the module level but at the method level, but that is
* really ugly
* not evident for anybody trying to create a custom xcom backend for the first time.


```
from airflow.models.xcom import BaseXCom
class MyXComBackend(BaseXCom):


    @staticmethod
    def serialize_value(value):
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        hook = S3Hook()
        pass
    @staticmethod
    def deserialize_value(value):
        from airflow.providers.amazon.aws.hooks.s3 import S3Hook
        hook = S3Hook()
        pass
```



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/list-integrations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py']
Ground Truth : ['a/airflow/hooks/base.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: Resolve warning about renderedtifields query
### Body

This warning is emitted when running a task instance, at least on mysql:

```
[2022-09-21, 05:22:56 UTC] {logging_mixin.py:117} WARNING - 
/home/airflow/.local/lib/python3.8/site-packages/airflow/models/renderedtifields.py:258 
SAWarning: Coercing Subquery object into a select() for use in IN(); 
please pass a select() construct explicitly
```

Need to resolve.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py']
Ground Truth : ['a/airflow/models/renderedtifields.py', 'a/airflow/utils/sqlalchemy.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: Restrict allowed characters in connection ids
### Description

I bumped into a bug where a connection id was suffixed with a whitespace e.g. "myconn ". When referencing the connection id "myconn" (without whitespace), you get a connection not found error.

To avoid such human errors, I suggest restricting the characters allowed for connection ids.

Some suggestions:
- There's an `airflow.utils.helpers.validate_key` function for validating the DAG id. Probably a good idea to reuse this.
- I believe variable ids are also not validated, would be good to check those too.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py']
Ground Truth : ['a/airflow/www/validators.py', 'a/airflow/www/forms.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: Reference to undeclared variable: "local variable 'return_code' referenced before assignment"
### Apache Airflow version

2.2.1

### Operating System

Ubuntu 20.04 LTS

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==2.3.0
apache-airflow-providers-apache-cassandra==2.1.0
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==6.0.0
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-jdbc==2.0.1
apache-airflow-providers-mysql==2.1.1
apache-airflow-providers-postgres==2.3.0
apache-airflow-providers-presto==2.0.1
apache-airflow-providers-slack==4.1.0
apache-airflow-providers-sqlite==2.0.1

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

Incorrect "finally" block invokes "UnboundLocalError: local variable 'return_code' referenced before assignment"
Traceback example:
```python
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 88, in _start_by_fork
    self.log.exception(
  File "/usr/lib/python3.8/logging/__init__.py", line 1481, in exception
    self.error(msg, *args, exc_info=exc_info, **kwargs)
  File "/usr/lib/python3.8/logging/__init__.py", line 1475, in error
    self._log(ERROR, msg, args, **kwargs)
  File "/usr/lib/python3.8/logging/__init__.py", line 1589, in _log
    self.handle(record)
  File "/usr/lib/python3.8/logging/__init__.py", line 1599, in handle
    self.callHandlers(record)
  File "/usr/lib/python3.8/logging/__init__.py", line 1661, in callHandlers
    hdlr.handle(record)
  File "/usr/lib/python3.8/logging/__init__.py", line 950, in handle
    rv = self.filter(record)
  File "/usr/lib/python3.8/logging/__init__.py", line 811, in filter
    result = f.filter(record)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/utils/log/secrets_masker.py", line 167, in filter
    self._redact_exception_with_context(exc)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/utils/log/secrets_masker.py", line 150, in _redact_exception_with_context
    self._redact_exception_with_context(exception.__context__)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/utils/log/secrets_masker.py", line 150, in _redact_exception_with_context
    self._redact_exception_with_context(exception.__context__)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/utils/log/secrets_masker.py", line 148, in _redact_exception_with_context
    exception.args = (self.redact(v) for v in exception.args)
AttributeError: can't set attribute
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/executors/celery_executor.py", line 121, in _execute_in_fork
    args.func(args)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 292, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 105, in _run_task_by_selected_method
    _run_task_by_local_task_job(args, ti)
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 163, in _run_task_by_local_task_job
    run_job.run()
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/jobs/base_job.py", line 245, in run
    self._execute()
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/jobs/local_task_job.py", line 103, in _execute
    self.task_runner.start()
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 41, in start
    self.process = self._start_by_fork()
  File "/var/lib/airflow/.venv/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 98, in _start_by_fork
    os._exit(return_code)
UnboundLocalError: local variable 'return_code' referenced before assignment
```

Bug location:
https://github.com/apache/airflow/blob/2.2.1/airflow/task/task_runner/standard_task_runner.py#L84-L98

Explanation:
Nested exception triggered when we are trying to log exception, so return_code remains undeclared.

### What you expected to happen

return_code variable should be declared 

### How to reproduce

It is probably hard to reproduce because you need to have exception in task execution as well as exception in logging function.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/testing_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_add_steps.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/task/task_runner/standard_task_runner.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: TaskFlow AirflowSkipException causes downstream step to fail
### Apache Airflow version

2.3.2 (latest released)

### What happened

Using TaskFlow API and have 2 tasks that lead to the same downstream task.  These tasks check for new data and when found will set an XCom entry of the new filename for the downstream to handle.  If no data is found the upstream tasks raise a skip exception. 
 The downstream task has the trigger_rule = none_failed_min_one_success.  

Problem is that a task which is set to Skip doesn't set any XCom.  When the downstream task starts it raises the error:
`airflow.exceptions.AirflowException: XComArg result from task2 at airflow_2_3_xcomarg_render_error with key="return_value" is not found!`

### What you think should happen instead

Based on trigger rule of "none_failed_min_one_success", expectation is that an upstream task should be allowed to skip and the downstream task will still run.  While the downstream does try to start based on trigger rules, it never really gets to run since the error is raised when rendering the arguments.

### How to reproduce

Example dag will generate the error if run.

```
from airflow.decorators import dag, task
from airflow.exceptions import AirflowSkipException

@task
def task1():
    return "example.csv"

@task
def task2():
    raise AirflowSkipException()

@task(trigger_rule="none_failed_min_one_success")
def downstream_task(t1, t2):
    print("task ran")

@dag(
    default_args={"owner": "Airflow", "start_date": "2022-06-07"},
    schedule_interval=None,
)
def airflow_2_3_xcomarg_render_error():
    t1 = task1()
    t2 = task2()
    downstream_task(t1, t2)

example_dag = airflow_2_3_xcomarg_render_error()
```

### Operating System

Ubuntu 20.04.4 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/skipmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/exceptions.py', 'a/airflow/models/xcom_arg.py']
Current Recall: 0.03008820230447639

=========================================================

ISSUE: Add `endpoint_id` arg to `vertex_ai.endpoint_service.CreateEndpointOperator`
### Description

Add the optional argument `endpoint_id` to `google.cloud.operators.vertex_ai.endpoint_service.CreateEndpointOperator` class and `google.cloud.hooks.vertex_ai.endpoint_service.EndpointServiceHook.create_endpoint` method.

### Use case/motivation

`google.cloud.operators.vertex_ai.endpoint_service.CreateEndpointOperator` class and `google.cloud.hooks.vertex_ai.endpoint_service.EndpointServiceHook.create_endpoint` method do not have `endpoint_id` argument. They internally use [`CreateEndpointRequest`](https://github.com/googleapis/python-aiplatform/blob/v1.11.0/google/cloud/aiplatform_v1/types/endpoint_service.py#L43), which accepts `endpoint_id`. Hence, I'd like them to accept `endpoint_id` argument and pass it to [`CreateEndpointRequest`](https://github.com/googleapis/python-aiplatform/blob/v1.11.0/google/cloud/aiplatform_v1/types/endpoint_service.py#L43).

If this is satisfied, we can create Vertex Endpoints with a specific Endpoint ID. Then, an Endpoint will be created with the specified Endpoint ID. Without it, an Endpoint will be created with an ID generated randomly.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/endpoint_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/endpoint_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/google_api_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/vertex_ai/example_vertex_ai_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/hooks/vertex_ai/test_endpoint_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/otel_logger.py']
Ground Truth : ['a/airflow/providers/google/cloud/hooks/vertex_ai/endpoint_service.py', 'a/airflow/providers/google/cloud/operators/vertex_ai/endpoint_service.py']
Current Recall: 0.034370857550729064

=========================================================

ISSUE: GenericTransfer and Postgres - ERROR - SET AUTOCOMMIT TO OFF is no longer supported
Trying to implement a generic transfer

``` python
t1 = GenericTransfer(
  task_id = 'copy_small_table',
  sql = "select * from my_schema.my_table",
  destination_table = "my_schema.my_table",
  source_conn_id = "postgres9.1.13",
  destination_conn_id = "postgres9.4.5",
  dag=dag
)
```

I get the following error:

```
--------------------------------------------------------------------------------
New run starting @2015-11-25T11:05:40.673401
--------------------------------------------------------------------------------
[2015-11-25 11:05:40,698] {models.py:951} INFO - Executing <Task(GenericTransfer): copy_my_table_v1> on 2015-11-24 00:00:00
[2015-11-25 11:05:40,711] {base_hook.py:53} INFO - Using connection to: 10.x.x.x
[2015-11-25 11:05:40,711] {generic_transfer.py:53} INFO - Extracting data from my_db
[2015-11-25 11:05:40,711] {generic_transfer.py:54} INFO - Executing: 
select * from my_schema.my_table
[2015-11-25 11:05:40,713] {base_hook.py:53} INFO - Using connection to: 10.x.x.x
[2015-11-25 11:05:40,808] {base_hook.py:53} INFO - Using connection to: 10.x.x.x
[2015-11-25 11:05:45,271] {base_hook.py:53} INFO - Using connection to: 10.x.x.x
[2015-11-25 11:05:45,272] {generic_transfer.py:63} INFO - Inserting rows into 10.x.x.x
[2015-11-25 11:05:45,273] {base_hook.py:53} INFO - Using connection to: 10.x.x.x
[2015-11-25 11:05:45,305] {models.py:1017} ERROR - SET AUTOCOMMIT TO OFF is no longer supported
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/airflow/models.py", line 977, in run
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python2.7/dist-packages/airflow/operators/generic_transfer.py", line 64, in execute
    destination_hook.insert_rows(table=self.destination_table, rows=results)
  File "/usr/local/lib/python2.7/dist-packages/airflow/hooks/dbapi_hook.py", line 136, in insert_rows
    cur.execute('SET autocommit = 0')
NotSupportedError: SET AUTOCOMMIT TO OFF is no longer supported

[2015-11-25 11:05:45,330] {models.py:1053} ERROR - SET AUTOCOMMIT TO OFF is no longer supported
```

Python 2.7
Airflow 1.6.1
psycopg2 2.6 (Also tried 2.6.1)
Postgeres destination 9.4.5

Any idea on what might cause this problem?


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_dates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_dynamodb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_runnable_exec_date_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/postgres/example_postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/databricks/hooks/test_databricks_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0008_1_6_0_task_duration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_generic_transfer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/common/sql/hooks/test_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/transfers/test_mysql_to_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py']
Ground Truth : ['a/airflow/hooks/postgres_hook.py']
Current Recall: 0.034370857550729064

=========================================================

ISSUE: Scheduler crashes with psycopg2.errors.DeadlockDetected exception
### Apache Airflow version

2.2.5 (latest released)

### What happened

Customer has a dag that generates around 2500 tasks dynamically using a task group. While running the dag, a subset of the tasks (~1000) run successfully with no issue and (~1500) of the tasks are getting "skipped", and the dag fails. The same DAG runs successfully in Airflow v2.1.3 with same Airflow configuration.

While investigating the Airflow processes, We found that both the scheduler got restarted with below error during the DAG execution.

```
[2022-04-27 20:42:44,347] {scheduler_job.py:742} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1256, in _execute_context
    self.dialect.do_executemany(
  File "/usr/local/lib/python3.9/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py", line 912, in do_executemany
    cursor.executemany(statement, parameters)
psycopg2.errors.DeadlockDetected: deadlock detected
DETAIL:  Process 1646244 waits for ShareLock on transaction 3915993452; blocked by process 1640692.
Process 1640692 waits for ShareLock on transaction 3915992745; blocked by process 1646244.
HINT:  See server log for query details.
CONTEXT:  while updating tuple (189873,4) in relation "task_instance"
```
This issue seems to be related to #19957

### What you think should happen instead

This issue was observed while running huge number of concurrent task created dynamically by a DAG. Some of the tasks are getting skipped due to restart of scheduler with Deadlock exception.

### How to reproduce

DAG file:

```
from propmix_listings_details import BUCKET, ZIPS_FOLDER, CITIES_ZIP_COL_NAME, DETAILS_DEV_LIMIT, DETAILS_RETRY, DETAILS_CONCURRENCY, get_api_token, get_values, process_listing_ids_based_zip
from airflow.utils.task_group import TaskGroup
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
}

date = '{{ execution_date }}'
email_to = ['example@airflow.com']
# Using a DAG context manager, you don't have to specify the dag property of each task

state = 'Maha'
with DAG('listings_details_generator_{0}'.format(state),
        start_date=datetime(2021, 11, 18),
        schedule_interval=None,
        max_active_runs=1,
        concurrency=DETAILS_CONCURRENCY,
        dagrun_timeout=timedelta(minutes=10),
        catchup=False  # enable if you don't want historical dag runs to run
        ) as dag:
    t0 = DummyOperator(task_id='start')

    with TaskGroup(group_id='group_1') as tg1:
        token = get_api_token()
        zip_list = get_values(BUCKET, ZIPS_FOLDER+state, CITIES_ZIP_COL_NAME)
        for zip in zip_list[0:DETAILS_DEV_LIMIT]:
            details_operator = PythonOperator(
                task_id='details_{0}_{1}'.format(state, zip),  # task id is generated dynamically
                pool='pm_details_pool',
                python_callable=process_listing_ids_based_zip,
                task_concurrency=40,
                retries=3,
                retry_delay=timedelta(seconds=10),
                op_kwargs={'zip': zip, 'date': date, 'token':token, 'state':state}
            )
            
    t0 >> tg1
```

### Operating System

kubernetes cluster running on GCP  linux (amd64)

### Versions of Apache Airflow Providers

pip freeze | grep apache-airflow-providers

apache-airflow-providers-amazon==1!3.2.0
apache-airflow-providers-cncf-kubernetes==1!3.0.0
apache-airflow-providers-elasticsearch==1!2.2.0
apache-airflow-providers-ftp==1!2.1.2
apache-airflow-providers-google==1!6.7.0
apache-airflow-providers-http==1!2.1.2
apache-airflow-providers-imap==1!2.2.3
apache-airflow-providers-microsoft-azure==1!3.7.2
apache-airflow-providers-mysql==1!2.2.3
apache-airflow-providers-postgres==1!4.1.0
apache-airflow-providers-redis==1!2.0.4
apache-airflow-providers-slack==1!4.2.3
apache-airflow-providers-snowflake==2.6.0
apache-airflow-providers-sqlite==1!2.1.3
apache-airflow-providers-ssh==1!2.4.3

### Deployment

Astronomer

### Deployment details

Airflow v2.2.5-2
Scheduler count: 2 
Scheduler resources: 20AU (2CPU and 7.5GB)
Executor used: Celery
Worker count : 2 
Worker resources: 24AU (2.4 CPU and 9GB)
Termination grace period : 2mins

### Anything else

This issue happens in all the dag runs. Some of the tasks are getting skipped and some are getting succeeded and the scheduler fails with the Deadlock exception error.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.034370857550729064

=========================================================

ISSUE: Dynamic Tasks with Depends On Past breaks Scheduler
### Apache Airflow version

2.3.1 (latest released)

### What happened

If you set 
- `'depends_on_past'=True`
on a task that utilises dynamic task mapping and then run that task the scheduler shut downs, logging the following error:

```
AttributeError: 'MappedOperator' object has no attribute 'ignore_first_depends_on_past'
[40] [INFO] Shutting down: Master
```
The scheduler then restarts, logs the same error, restarts again getting stuck in this loop, restarting indefinitely and never actually executing the task.

For the same DAG, if you set:
- `'depends_on_past'=False`
Then it will execute correctly. 

This issue occurs on Airflow versions: 2.3.0-5 and 2.3.1 (Other versions have not been tested)

### What you think should happen instead

Airflow should check the status of all tasks in the previous task mapping collection and then decide whether the current task mapping should be executed or not. 

### How to reproduce

The following code should act as the minimal reproducible case. 

- Task: `first` will execute correctly.
- Task: `second` causes the scheduler to restart

```
from datetime import datetime, timedelta

from airflow import models
from airflow.models.baseoperator import chain
from airflow.decorators import task


default_args = {
    'owner': 'airflow',
    'start_date': datetime(2022, 4, 25),
    'depends_on_past': True,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag_name = 'break-scheduler'

@task
def producer():
    return[1, 2]

@task
def consumer(arg):
    print(arg)

with models.DAG(
    dag_name,
    default_args=default_args,
    schedule_interval='0 * * * *',
    catchup=True,
    max_active_runs=1,
) as dag:
    first = producer()

    second = consumer.expand(arg=first)

    chain(first, second)
```

### Operating System

docker/debian

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

Full stack trace from the scheduler:

```
[2022-05-30 10:55:37,335] {scheduler_job.py:756} ERROR - Exception when executing SchedulerJob._run_scheduler_loop

Traceback (most recent call last):

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 739, in _execute

    self._run_scheduler_loop()

  File "/usr/local/lib/python3.9/site-packages/astronomer/airflow/version_check/plugin.py", line 29, in run_before

    fn(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 827, in _run_scheduler_loop

    num_queued_tis = self._do_scheduling(session)

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 909, in _do_scheduling

    callback_to_run = self._schedule_dag_run(dag_run, session)

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1151, in _schedule_dag_run

    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)

  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 522, in update_state

    info = self.task_instance_scheduling_decisions(session)

  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 658, in task_instance_scheduling_decisions

    schedulable_tis, changed_tis, expansion_happened = self._get_ready_tis(

  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 705, in _get_ready_tis

    if not schedulable.are_dependencies_met(session=session, dep_context=dep_context):

  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1147, in are_dependencies_met

    for dep_status in self.get_failed_dep_statuses(dep_context=dep_context, session=session):

  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1168, in get_failed_dep_statuses

    for dep_status in dep.get_dep_statuses(self, session, dep_context):

Traceback (most recent call last):

  File "/usr/local/bin/airflow", line 8, in <module>

[2022-05-30 10:55:07 +0000] [40] [INFO] Handling signal: term

    sys.exit(main())

  File "/usr/local/lib/python3.9/site-packages/airflow/__main__.py", line 38, in main

    args.func(args)

  File "/usr/local/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 51, in command

    return func(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/utils/cli.py", line 99, in wrapper

[2022-05-30 10:55:07 +0000] [42] [INFO] Worker exiting (pid: 42)

    return f(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 75, in scheduler

    _run_scheduler_job(args=args)

  File "/usr/local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 46, in _run_scheduler_job

    job.run()

[2022-05-30 10:55:07 +0000] [44] [INFO] Worker exiting (pid: 44)

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/base_job.py", line 244, in run

    self._execute()

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 739, in _execute

    self._run_scheduler_loop()

  File "/usr/local/lib/python3.9/site-packages/astronomer/airflow/version_check/plugin.py", line 29, in run_before

    fn(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 827, in _run_scheduler_loop

    num_queued_tis = self._do_scheduling(session)

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 909, in _do_scheduling

    callback_to_run = self._schedule_dag_run(dag_run, session)

  File "/usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1151, in _schedule_dag_run

    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)

  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 522, in update_state

    info = self.task_instance_scheduling_decisions(session)

  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 658, in task_instance_scheduling_decisions

    schedulable_tis, changed_tis, expansion_happened = self._get_ready_tis(

  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 705, in _get_ready_tis

    if not schedulable.are_dependencies_met(session=session, dep_context=dep_context):

  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1147, in are_dependencies_met

    for dep_status in self.get_failed_dep_statuses(dep_context=dep_context, session=session):

  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1168, in get_failed_dep_statuses

    for dep_status in dep.get_dep_statuses(self, session, dep_context):

  File "/usr/local/lib/python3.9/site-packages/airflow/ti_deps/deps/base_ti_dep.py", line 95, in get_dep_statuses

    yield from self._get_dep_statuses(ti, session, dep_context)

  File "/usr/local/lib/python3.9/site-packages/airflow/ti_deps/deps/prev_dagrun_dep.py", line 71, in _get_dep_statuses

    if ti.task.ignore_first_depends_on_past:

AttributeError: 'MappedOperator' object has no attribute 'ignore_first_depends_on_past'

[2022-05-30 10:55:07 +0000] [40] [INFO] Shutting down: Master
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py']
Ground Truth : ['a/airflow/decorators/base.py', 'a/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/dev/null', 'a/airflow/models/abstractoperator.py', 'a/airflow/models/baseoperator.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/ti_deps/deps/prev_dagrun_dep.py']
Current Recall: 0.034370857550729064

=========================================================

ISSUE: Update TaskFlow API example to use `@dag` decorator


**Description**

Update https://github.com/apache/airflow/blob/master/airflow/example_dags/tutorial_taskflow_api_etl.py tp use `@dag` decorator instead of leveraging context manager to make it more TaskFlow-y.



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/docker/example_taskflow_api_docker_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_setup_teardown_taskflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_short_circuit_decorator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_python_decorator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_branch_python_dop_operator_3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial_taskflow_api.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_serialized_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/openai/example_openai.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_branch_operator_decorator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial_taskflow_api_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py']
Ground Truth : ['a/airflow/example_dags/tutorial_taskflow_api_etl.py']
Current Recall: 0.034370857550729064

=========================================================

ISSUE: Production Docker Image AIRFLOW_INSTALL_VERSION does not overwrite AIRFLOW_VERSION environment variable
**Apache Airflow version**: 1.10.10

**What happened**:
When I look at environment variables within my Airflow containers, I see `AIRFLOW_VERSION="2.0.0.dev0"`

**What you expected to happen**:
Since I built the image with `AIRFLOW_INSTALL_VERSION="==1.10.10" `, I would expect any reference to the Airflow version to reflect 1.10.10 and not 2.0.0.

**How to reproduce it**:
Build an image with a specific AIRFLOW_INSTALL_VERSION and then exec into it and printenv or inspect the container and you will see `AIRFLOW_VERSION="2.0.0.dev0"`. It does not appear like this env var impacts anything and it is hard coded to 2.0.0.dev0 at the moment. Most likely, this variable should be replaced with the AIRFLOW_INSTALL_VERSION instead.

```
docker build . \
--build-arg AIRFLOW_INSTALL_SOURCES="apache-airflow" \
--build-arg AIRFLOW_INSTALL_VERSION="==1.10.10" 
```

/label area:production-image

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_image_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/webserver/test_webserver.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py']
Ground Truth : ['a/setup.py', 'a/docs/conf.py', 'a/.github/workflows/delete_old_artifacts.yml']
Current Recall: 0.03508463342510451

=========================================================

ISSUE: google.api_core.exceptions.Unknown: None Stream removed (Snowflake and GCP Secret Manager)
**Apache Airflow version**: 2.1.0


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): N/A

**Environment**:

- **Cloud provider or hardware configuration**: Astronomer-based local setup using Docker `quay.io/astronomer/ap-airflow:2.1.0-2-buster-onbuild`
- **OS** (e.g. from /etc/os-release): `Debian GNU/Linux 10 (buster)`
- **Kernel** (e.g. `uname -a`): `Linux 7a92d1fd4406 5.10.25-linuxkit #1 SMP Tue Mar 23 09:27:39 UTC 2021 x86_64 GNU/Linux`
- **Install tools**: apache-airflow-providers-snowflake
- **Others**:

**What happened**:

Having configured Snowflake connection and pointing to GCP Secret Manager backend `AIRFLOW__SECRETS__BACKEND=airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend` I am getting a pretty consistent error traced all the way down to gRPC

```File "/usr/local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 57, in error_remapped_callable
    return callable_(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/grpc/_channel.py", line 946, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File "/usr/local/lib/python3.7/site-packages/grpc/_channel.py", line 849, in _end_unary_response_blocking
    raise _InactiveRpcError(state)
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.UNKNOWN
	details = "Stream removed"
	debug_error_string = "{"created":"@1624370913.481874500","description":"Error received from peer ipv4:172.xxx.xx.xxx:443","file":"src/core/lib/surface/call.cc","file_line":1067,"grpc_message":"Stream removed","grpc_status":2}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1137, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/python.py", line 150, in execute
    return_value = self.execute_callable()
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/python.py", line 161, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/usr/local/airflow/dags/qe/weekly.py", line 63, in snfk_hook
    df = hook.get_pandas_df(sql)
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/dbapi.py", line 116, in get_pandas_df
    with closing(self.get_conn()) as conn:
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 220, in get_conn
    conn_config = self._get_conn_params()
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 152, in _get_conn_params
    self.snowflake_conn_id  # type: ignore[attr-defined] # pylint: disable=no-member
  File "/usr/local/lib/python3.7/site-packages/airflow/hooks/base.py", line 67, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/connection.py", line 376, in get_connection_from_secrets
    conn = secrets_backend.get_connection(conn_id=conn_id)
  File "/usr/local/lib/python3.7/site-packages/airflow/secrets/base_secrets.py", line 64, in get_connection
    conn_uri = self.get_conn_uri(conn_id=conn_id)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/google/cloud/secrets/secret_manager.py", line 134, in get_conn_uri
    return self._get_secret(self.connections_prefix, conn_id)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/google/cloud/secrets/secret_manager.py", line 170, in _get_secret
    return self.client.get_secret(secret_id=secret_id, project_id=self.project_id)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/google/cloud/_internal_client/secret_manager_client.py", line 86, in get_secret
    response = self.client.access_secret_version(name)
  File "/usr/local/lib/python3.7/site-packages/google/cloud/secretmanager_v1/gapic/secret_manager_service_client.py", line 968, in access_secret_version
    request, retry=retry, timeout=timeout, metadata=metadata
  File "/usr/local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py", line 145, in __call__
    return wrapped_func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/google/api_core/retry.py", line 286, in retry_wrapped_func
    on_error=on_error,
  File "/usr/local/lib/python3.7/site-packages/google/api_core/retry.py", line 184, in retry_target
    return target()
  File "/usr/local/lib/python3.7/site-packages/google/api_core/timeout.py", line 214, in func_with_timeout
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py", line 59, in error_remapped_callable
    six.raise_from(exceptions.from_grpc_error(exc), exc)
  File "<string>", line 3, in raise_from
google.api_core.exceptions.Unknown: None Stream removed
```

**What you expected to happen**:

DAG successfully retrieves a configured connection for Snowflake from GCP Secret Manager and executes a query returning back a result.

**How to reproduce it**:
1. Configure Google Cloud Platform as secrets backend
`AIRFLOW__SECRETS__BACKEND=airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend`
2. Configure a Snowflake connection (`requirements.txt` has `apache-airflow-providers-snowflake`)
3. Create a DAG which uses SnowflakeHook similar to this:

```python
import logging

import airflow
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.contrib.hooks.snowflake_hook import SnowflakeHook
from airflow.contrib.operators.snowflake_operator import SnowflakeOperator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

args = {"owner": "Airflow", "start_date": airflow.utils.dates.days_ago(2)}

dag = DAG(
    dag_id="snowflake_automation", default_args=args, schedule_interval=None
)

snowflake_query = [
    """create table public.test_employee (id number, name string);""",
    """insert into public.test_employee values(1, Sam),(2, Andy),(3, Gill);""",
]


def get_row_count(**context):
    dwh_hook = SnowflakeHook(snowflake_conn_id="snowflake_conn")
    result = dwh_hook.get_first("select count(*) from public.test_employee")
    logging.info("Number of rows in `public.test_employee`  - %s", result[0])

with dag:
    create_insert = SnowflakeOperator(
        task_id="snowfalke_create",
        sql=snowflake_query ,
        snowflake_conn_id="snowflake_conn",
    )

    get_count = PythonOperator(task_id="get_count", python_callable=get_row_count)

create_insert >> get_count
```

**Anything else we need to know**:

I looked around to see if this is an issue with Google's `api-core` and it seems like somebody has done research into it to point out that it might be downstream implementation issue and not the `api-core` issue: https://stackoverflow.com/questions/67374613/why-does-accessing-this-variable-fail-after-it-is-used-in-a-thread


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py']
Ground Truth : ['a/airflow/providers/google/cloud/secrets/secret_manager.py']
Current Recall: 0.03508463342510451

=========================================================

ISSUE: hive ldap with auth=noSasl bug
i have a little troube with CDH 5 cluster:
with 

```
<property>
  <name>hive.server2.authentication</name>
  <value>LDAP</value>
</property>
```

this change useful for properly working:

``` diff
diff --git a/airflow/hooks/hive_hooks.py b/airflow/hooks/hive_hooks.py
index 794c78a..ca15a64 100644
--- a/airflow/hooks/hive_hooks.py
+++ b/airflow/hooks/hive_hooks.py
@@ -86,7 +86,7 @@ class HiveCliHook(BaseHook):
                         jdbc_url = (
                             "jdbc:hive2://"
                             "{0}:{1}/{2}"
-                            ";auth=noSasl"
+                            #";auth=noSasl"
                         ).format(conn.host, conn.port, conn.schema)

                     cmd_extra += ['-u', jdbc_url]
```

i.e. NO sasl parameter need in this case, can u fix it?


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/operators/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/default_webserver_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/postgres/hooks/postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/teradata/hooks/teradata.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/operators/yandexcloud_dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/sql_to_sheets/example_sql_to_sheets.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/weaviate/hooks/weaviate.py']
Ground Truth : ['a/airflow/hooks/hive_hooks.py']
Current Recall: 0.03508463342510451

=========================================================

ISSUE: `airflow tasks states-for-dag-run` has no `map_index` column
### Apache Airflow version

2.3.0b1 (pre-release)

### What happened

I ran:

```
$ airflow tasks states-for-dag-run taskmap_xcom_pull 'manual__2022-04-14T13:27:04.958420+00:00'
dag_id            | execution_date                   | task_id   | state   | start_date                       | end_date
==================+==================================+===========+=========+==================================+=================================
taskmap_xcom_pull | 2022-04-14T13:27:04.958420+00:00 | foo       | success | 2022-04-14T13:27:05.343134+00:00 | 2022-04-14T13:27:05.598641+00:00
taskmap_xcom_pull | 2022-04-14T13:27:04.958420+00:00 | bar       | success | 2022-04-14T13:27:06.256684+00:00 | 2022-04-14T13:27:06.462664+00:00
taskmap_xcom_pull | 2022-04-14T13:27:04.958420+00:00 | identity  | success | 2022-04-14T13:27:07.480364+00:00 | 2022-04-14T13:27:07.713226+00:00
taskmap_xcom_pull | 2022-04-14T13:27:04.958420+00:00 | identity  | success | 2022-04-14T13:27:07.512084+00:00 | 2022-04-14T13:27:07.768716+00:00
taskmap_xcom_pull | 2022-04-14T13:27:04.958420+00:00 | identity  | success | 2022-04-14T13:27:07.546097+00:00 | 2022-04-14T13:27:07.782719+00:00
```

...targeting a dagrun for which `identity` had three expanded tasks.  All three showed up, but the output didn't show me enough to know which one was which.

### What you think should happen instead

There should be a `map_index` column so that I know which one is which.

### How to reproduce

Run a dag with expanded tasks, then try to view their states via the cli

### Operating System

debian (docker)

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_xcom_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/serializers/test_serializers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_xcom_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_appflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/presto/hooks/test_presto.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_openlineage_adapter.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_mapped_task_instance_endpoint.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py']
Current Recall: 0.03508463342510451

=========================================================

ISSUE: Connections import and export should also support ".yml" file extensions
### Apache Airflow version

2.2.4 (latest released)

### What happened

Trying to export or import a yaml formatted connections file with ".yml" extension fails. 

### What you think should happen instead

While the "official recommended extension" for YAML files is .yaml, many pipeline are built around using the .yml file extension. Importing and exporting of .yml files should also be supported. 

### How to reproduce

Running airflow connections import or export with a file having a .yml file extension errors with:
`Unsupported file format. The file must have the extension .env or .json or .yaml`


### Operating System

debian 10 buster

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/secrets/local_filesystem.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_connection_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_json_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/connection_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/flink/operators/flink_kubernetes.py']
Ground Truth : ['a/airflow/secrets/local_filesystem.py']
Current Recall: 0.03508463342510451

=========================================================

ISSUE: DatabricksRunNowOperator missing jar_params as a kwarg
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

**Description**

DatabricksRunNowOperator is missing the option to take in key word arguement _jar_params_ , it already can take the other ones notebook_params,python_params,spark_submit_params. https://docs.databricks.com/dev-tools/api/latest/jobs.html#run-now


**Use case / motivation**

Provide parity with the other options


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/databricks/operators/test_databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py']
Ground Truth : ['a/airflow/providers/databricks/operators/databricks.py']
Current Recall: 0.03722596104823085

=========================================================

ISSUE: Make platform version as independent parameter of ECSOperator
Currently `ECSOperator` propagates `platform_version` parameter either in case `launch_type` is `FARGATE` or there is `capacity_provider_strategy` parameter provided. The case with `capacity_provider_strategy` is wrong. Capacity provider strategy can contain a reference on EC2 capacity provider. If it's an EC2 capacity provider, then `platform_version` should not be propagated to the `boto3` api call. And it's not possible to do so with the current logic of `ECSOperator` because `platform_version` is always propagated in such case and `boto3` doesn't accept `platform_version` as `None`. So in order to fix that `platform_version` should be an independent parameter and propagated only when it's specified, regardless which `launch_type` or `capacity_provider_strategy` is specified. That should also simplify the logic of `ECSOperator`.

I will prepare a PR to fix that.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/_cron.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/ecs.py']
Current Recall: 0.03936728867135719

=========================================================

ISSUE: string templated fields are not rendered literally
**Apache Airflow version**: 2.0.1

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): n/a

**Environment**:

- **Cloud provider or hardware configuration**: 
- **OS** (e.g. from /etc/os-release): redhat 7.9
- **Kernel** (e.g. `uname -a`): 3.10.0-1160.11.1.el7.x86_64
- **Install tools**: pip
- **Others**:

**What happened**:

Given the following code

```
SSHOperator(task_id='dummy', command='''\
for i in 1..100
do
echo $i
echo $i $i
echo $i $i $i
done
''')
```

The `command` field is rendered as:

```
'\n\nfor i in 1..100\ndo\necho $i\necho $i $i\necho $i $i $i\ndone\n'
```

**What you expected to happen**:

I expect to see

```
for i in 1..100
do
echo $i
echo $i $i
echo $i $i $i
done
```

**How to reproduce it**:


**Anything else we need to know**:

This is because the following line renders templated fields using the `pformat` function.

https://github.com/apache/airflow/blob/35c9a902929b79cf7cf53ac5b90c3565dddb97dc/airflow/www/views.py#L926-L928

I suggest we should render strings as is.

```diff
            if renderer in renderers:
                html_dict[template_field] = renderers[renderer](content)
            else:
                html_dict[template_field] = Markup("<pre><code>{}</pre></code>").format(
                     pformat(content)
+                   content if isinstance(content, str) else pformat(content)
                )  # noqa
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/template/templater.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/dags/elastic_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/cncf/kubernetes/example_kubernetes_async.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/lineage/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/charts/helm_template_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_rendered.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py']
Ground Truth : ['a/airflow/providers/mysql/transfers/presto_to_mysql.py', 'a/airflow/providers/singularity/operators/singularity.py', 'a/airflow/providers/mysql/transfers/vertica_to_mysql.py', 'a/airflow/providers/amazon/aws/transfers/mongo_to_s3.py', 'a/airflow/operators/sql.py', 'a/airflow/providers/microsoft/azure/operators/azure_container_instances.py', 'a/airflow/providers/amazon/aws/operators/ecs.py', 'a/airflow/providers/ssh/operators/ssh.py', 'a/airflow/providers/amazon/aws/operators/athena.py', 'a/airflow/providers/microsoft/winrm/operators/winrm.py', 'a/airflow/providers/oracle/transfers/oracle_to_oracle.py', 'a/airflow/providers/amazon/aws/operators/sns.py', 'a/airflow/providers/amazon/aws/operators/emr_create_job_flow.py', 'a/airflow/providers/amazon/aws/transfers/mysql_to_s3.py', 'a/airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py', 'a/airflow/providers/grpc/operators/grpc.py', 'a/airflow/providers/amazon/aws/operators/sagemaker_base.py', 'a/airflow/operators/generic_transfer.py', 'a/airflow/providers/amazon/aws/operators/datasync.py', 'a/airflow/operators/trigger_dagrun.py', 'a/airflow/providers/amazon/aws/operators/batch.py', 'a/airflow/providers/snowflake/transfers/snowflake_to_slack.py', 'a/airflow/www/utils.py', 'a/airflow/providers/amazon/aws/operators/glue.py']
Current Recall: 0.03936728867135719

=========================================================

ISSUE: Triggering a DAG with the same run_id as a scheduled one causes the scheduler to crash
### Apache Airflow version

2.5.0

### What happened

A user with access to manually triggering DAGs can trigger a DAG. provide a run_id that matches the pattern used when creating scheduled runs and cause the scheduler to crash due to database unique key violation:
```
2022-12-12 12:58:00,793] {scheduler_job.py:776} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 759, in _execute
    self._run_scheduler_loop()
  File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 885, in _run_scheduler_loop
    num_queued_tis = self._do_scheduling(session)
  File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 956, in _do_scheduling
    self._create_dagruns_for_dags(guard, session)
  File "/usr/local/lib/python3.8/site-packages/airflow/utils/retries.py", line 78, in wrapped_function
    for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
  File "/usr/local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/usr/local/lib/python3.8/site-packages/tenacity/__init__.py", line 351, in iter
    return fut.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.8/site-packages/airflow/utils/retries.py", line 87, in wrapped_function
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 1018, in _create_dagruns_for_dags
    query, dataset_triggered_dag_info = DagModel.dags_needing_dagruns(session)
  File "/usr/local/lib/python3.8/site-packages/airflow/models/dag.py", line 3341, in dags_needing_dagruns
    for x in session.query(
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1713, in execute
    conn = self._connection_for_bind(bind)
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1552, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 721, in _connection_for_bind
    self._assert_active()
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 601, in _assert_active
    raise sa_exc.PendingRollbackError(
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "dag_run_dag_id_run_id_key"
DETAIL:  Key (dag_id, run_id)=(example_branch_dop_operator_v3, scheduled__2022-12-12T12:57:00+00:00) already exists.

[SQL: INSERT INTO dag_run (dag_id, queued_at, execution_date, start_date, end_date, state, run_id, creating_job_id, external_trigger, run_type, conf, data_interval_start, data_interval_end, last_scheduling_decision, dag_hash, log_template_id, updated_at) VALUES (%(dag_id)s, %(queued_at)s, %(execution_date)s, %(start_date)s, %(end_date)s, %(state)s, %(run_id)s, %(creating_job_id)s, %(external_trigger)s, %(run_type)s, %(conf)s, %(data_interval_start)s, %(data_interval_end)s, %(last_scheduling_decision)s, %(dag_hash)s, (SELECT max(log_template.id) AS max_1 
FROM log_template), %(updated_at)s) RETURNING dag_run.id]
[parameters: {'dag_id': 'example_branch_dop_operator_v3', 'queued_at': datetime.datetime(2022, 12, 12, 12, 58, 0, 435945, tzinfo=Timezone('UTC')), 'execution_date': DateTime(2022, 12, 12, 12, 57, 0, tzinfo=Timezone('UTC')), 'start_date': None, 'end_date': None, 'state': <DagRunState.QUEUED: 'queued'>, 'run_id': 'scheduled__2022-12-12T12:57:00+00:00', 'creating_job_id': 1, 'external_trigger': False, 'run_type': <DagRunType.SCHEDULED: 'scheduled'>, 'conf': <psycopg2.extensions.Binary object at 0x7f283a82af60>, 'data_interval_start': DateTime(2022, 12, 12, 12, 57, 0, tzinfo=Timezone('UTC')), 'data_interval_end': DateTime(2022, 12, 12, 12, 58, 0, tzinfo=Timezone('UTC')), 'last_scheduling_decision': None, 'dag_hash': '1653a588de69ed25c5b1dcfef928479c', 'updated_at': datetime.datetime(2022, 12, 12, 12, 58, 0, 436871, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
```

Worse yet, the scheduler will keep crashing after a restart with the same exception.

### What you think should happen instead

A user should not be able to crash the scheduler from the UI.  
I see 2 alternatives for solving this:

1. Reject custom run_id that would (or could) collide with a scheduled one, preventing this situation from happening.
2. Handle the database error and assign a different run_id to the scheduled run.

### How to reproduce

1. Find an unpaused DAG.
2. Trigger DAG w/ config, set the run id to something like scheduled__2022-11-21T12:00:00+00:00 (adjust the time to be in the future where there is no run yet).
3. Let the manual DAG run finish.
4. Wait for the scheduler to try to schedule another DAG run with the same run id.
5.  :boom: 
6. Attempt to restart the scheduler.
7. :boom: 

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

apache-airflow-providers-postgres==5.3.1

### Deployment

Docker-Compose

### Deployment details

I'm using a Postgres docker container as a metadata database that is linked via docker networking to the scheduler and the rest of the components. Scheduler, workers and webserver are all running in separate containers (using CeleryExecutor backed by a Redis container), though I do not think it is relevant in this case. 

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py']
Ground Truth : ['a/airflow/models/dag.py']
Current Recall: 0.03936728867135719

=========================================================

ISSUE: Unauthenticated access with RBAC to URL has_dag_access results lose redirection
<!--

**Apache Airflow version**: latest

**Environment**: Linux

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:
Unauthenticated access with RBAC to URL has_dag_access results lose redirection

**What you expected to happen**:
Redirection maintained, user can login and redirected to proper page

Missing next URL parameter

**How to reproduce it**:
Setup airflow with RBAC
Have a user who hasn't log in access a URL that uses decorator has_dag_access, i.e:
  http://localhost/graph?dag_id=dag_id
User will be sent to login page, (note the next= argument that required to redirect to original page is gone)
Login properly, user then will be sent to bad page


If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/views/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/operators/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/tests/test_aws_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/redirects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py']
Ground Truth : ['a/airflow/www/decorators.py']
Current Recall: 0.03936728867135719

=========================================================

ISSUE: Databricks SQL fails on Python 3.10
### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

The databricks SQL does not work on Python 3.10 due to "from collections import Iterable" in the `databricks-sql-connector`

* https://pypi.org/project/databricks-sql-connector/

Details of this issue dicussed in https://github.com/apache/airflow/pull/22050

For now we will likely just exclude the tests (and mark databricks provider as non-python 3.10 compatible). But once this is fixed (in either 1.0.2 or upcoming 2.0.0 version of the library, we wil restore it back). 

### Apache Airflow version

main (development)

### Operating System

All

### Deployment

Other

### Deployment details

Just Breeze with Python 3.10

### What happened

The tests are failing:

```
  self = <databricks.sql.common.ParamEscaper object at 0x7fe81c6dd6c0>
  item = ['file1', 'file2', 'file3']
  
      def escape_item(self, item):
          if item is None:
              return 'NULL'
          elif isinstance(item, (int, float)):
              return self.escape_number(item)
          elif isinstance(item, basestring):
              return self.escape_string(item)
  >       elif isinstance(item, collections.Iterable):
  E       AttributeError: module 'collections' has no attribute 'Iterable'
  ```

https://github.com/apache/airflow/runs/5523057543?check_suite_focus=true#step:8:16781 


### What you expected to happen

Test succeed :)

### How to reproduce

Run `TestDatabricksSqlCopyIntoOperator` in Python 3.10 environment.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/openlineage/utils/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.03936728867135719

=========================================================

ISSUE: upgrade_check fails db version check
**Apache Airflow version**: 1.10.14 with AWS RDS mysql 5.7.26 as metastore db


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): v1.16.15

**Environment**: DEV

- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): Debian GNU/Linux 10 (buster)
- **Kernel** (e.g. `uname -a`): Linux airflow-scheduler-765f664c56-4bsfq 4.14.186-146.268.amzn2.x86_64 #1 SMP Tue Jul 14 18:16:52 UTC 2020 x86_64 GNU/Linux
- **Install tools**: 
- **Others**: Running on K8S as docker container with apache/airflow:1.10.14 as base

**What happened**: Running `airflow upgrade_check` returns the following error:
```
airflow@airflow-web-54d6577c8b-g9vcn:/opt/airflow$ airflow upgrade_check

==================================================== STATUS ====================================================
Check for latest versions of apache-airflow and checker...............................................SUCCESS
Remove airflow.AirflowMacroPlugin class...............................................................SUCCESS
/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/helpers.py:442: DeprecationWarning: Importing 'DummyOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
  DeprecationWarning)
Ensure users are not using custom metaclasses in custom operators.....................................SUCCESS
Chain between DAG and operator not allowed............................................................SUCCESS
Connection.conn_type is not nullable..................................................................SUCCESS
Custom Executors now require full path................................................................SUCCESS
Traceback (most recent call last):
  File "/home/airflow/.local/bin/airflow", line 37, in <module>
    args.func(args)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/upgrade/checker.py", line 118, in run
    all_problems = check_upgrade(formatter, rules)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/upgrade/checker.py", line 38, in check_upgrade
    rule_status = RuleStatus.from_rule(rule)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/upgrade/problem.py", line 44, in from_rule
    result = rule.check()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/upgrade/rules/postgres_mysql_sqlite_version_upgrade_check.py", line 56, in check
    installed_mysql_version = Version(session.execute('SELECT VERSION();').scalar())
  File "/home/airflow/.local/lib/python3.6/site-packages/packaging/version.py", line 298, in __init__
    raise InvalidVersion("Invalid version: '{0}'".format(version))
packaging.version.InvalidVersion: Invalid version: '5.7.26-log'
airflow@airflow-web-54d6577c8b-g9vcn:/opt/airflow$
```

**What you expected to happen**: commands runs through and prints helpful messages

<!-- What do you think went wrong? -->
Running `SELECT VERSION();').scalar()` against the metastore db returns "5.7.26-log' which is possibly not a valid value for Version class `__init__` function because of the "-log" ending?

```
mysql> select VERSION();
+------------+
| VERSION()  |
+------------+
| 5.7.26-log |
+------------+
1 row in set (0.00 sec)
```

**How to reproduce it**: Run `airflow upgrade_check` again.

**Anything else we need to know**: 
Dockerfile snippet:
```
FROM apache/airflow:1.10.14
...
USER ${AIRFLOW_UID}
RUN pip install --user \
    airflow-kubernetes-job-operator \
    apache-airflow-backport-providers-cncf-kubernetes \
    apache-airflow-backport-providers-ssh \
    apache-airflow-upgrade-check
```

How often does this problem occur? Once? Every time etc?: Every time since last week. Has worked before.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py']
Ground Truth : ['a/airflow/upgrade/rules/postgres_mysql_sqlite_version_upgrade_check.py']
Current Recall: 0.03936728867135719

=========================================================

ISSUE: CSRF token should be expire with session
### Apache Airflow version

2.5.0

### What happened

In the default configuration, the CSRF token [expires in one hour](https://pythonhosted.org/Flask-WTF/config.html#forms-and-csrf). This setting leads to frequent errors in the UI  for no good reason.


### What you think should happen instead

A short expiration date for the CSRF token is not the right value in my view and I [agree with this answer](https://security.stackexchange.com/a/56520/22108) that the CSRF token should basically never expire, instead pegging itself to the current session.

That is, the CSRF token should last as long as the current session. The easiest way to accomplish this is by generating the CSRF token from the session id.



### How to reproduce

_No response_

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/views/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/default_webserver_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py']
Ground Truth : ['a/airflow/config_templates/default_webserver_config.py']
Current Recall: 0.03936728867135719

=========================================================

ISSUE: Consider and add common sensitive names
**Description** 

Since sensitive informations in the connection object (specifically the extras field) are now being masked based on sensitive key names, we should consider adding some common sensitive key names.

`private_key` from [ssh connection](https://airflow.apache.org/docs/apache-airflow-providers-ssh/stable/connections/ssh.html) is an examples.

**Use case / motivation**

Extras field used to be blocked out entirely before the sensitive value masking feature (#15599).

[Before in 2.0.2](https://github.com/apache/airflow/blob/2.0.2/airflow/hooks/base.py#L78
) and [after in 2.1.0](https://github.com/apache/airflow/blob/2.1.0/airflow/hooks/base.py#L78
).

Extras field containing sensitive information now shown unless the key contains sensitive names.

**Are you willing to submit a PR?** 

@ashb has expressed interest in adding this.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ssh/hooks/ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_rendered.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datacatalog.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dlp.py']
Ground Truth : ['a/airflow/utils/log/secrets_masker.py']
Current Recall: 0.04150861629448353

=========================================================

ISSUE: LocalFilesystemToS3Operator dest_key can not be a full s3:// style url
### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==3.3.0
apache-airflow-providers-ftp==2.1.2
apache-airflow-providers-http==2.1.2
apache-airflow-providers-imap==2.2.3
apache-airflow-providers-mongo==2.3.3
apache-airflow-providers-sqlite==2.1.3

### Apache Airflow version

2.3.0b1 (pre-release)

### Operating System

Arch Linux

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

`LocalFilesystemToS3Operator` does not accept full s3:// style url as `dest_key`, although it states, that it should:

```
    :param dest_key: The key of the object to copy to. (templated)

        It can be either full s3:// style url or relative path from root level.

        When it's specified as a full s3:// url, including dest_bucket results in a TypeError.
```

### What you think should happen instead

`LocalFilesystemToS3Operator` should behave as documented.

### How to reproduce

A modification of an existing UT:
```
    @mock_s3
    def test_execute_with_only_key(self):
        conn = boto3.client('s3')
        conn.create_bucket(Bucket=self.dest_bucket)
        operator = LocalFilesystemToS3Operator(
            task_id='s3_to_file_sensor',
            dag=self.dag,
            filename=self.testfile1,
            dest_key=f's3://dummy/{self.dest_key}',
            **self._config,
        )
        operator.execute(None)

        objects_in_dest_bucket = conn.list_objects(Bucket=self.dest_bucket, Prefix=self.dest_key)
        # there should be object found, and there should only be one object found
        assert len(objects_in_dest_bucket['Contents']) == 1
        # the object found should be consistent with dest_key specified earlier
        assert objects_in_dest_bucket['Contents'][0]['Key'] == self.dest_key
```

`FAILED tests/providers/amazon/aws/transfers/test_local_to_s3.py::TestFileToS3Operator::test_execute_with_only_key - TypeError: expected string or bytes-like object`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/transfers/test_local_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/local_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/transfers/test_http_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/transfers/test_s3_to_sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/transfers/test_sftp_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/http_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/triggers/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/azure_blob_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/gcs_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/s3.py', 'a/airflow/providers/amazon/aws/hooks/s3.py', 'a/airflow/providers/amazon/aws/sensors/s3.py', 'a/airflow/providers/amazon/aws/transfers/local_to_s3.py']
Current Recall: 0.042043948200265115

=========================================================

ISSUE: db+ string in result backend but not metadata secret
### Official Helm Chart version

1.1.0 (latest released)

### Apache Airflow version

2.1.3 (latest released)

### Kubernetes Version

1.21

### Helm Chart configuration

  data:
    metadataSecretName: "airflow-metadata"
    resultBackendSecretName: "airflow-result-backend"

### Docker Image customisations

_No response_

### What happened

If we only supply 1 secret with 
```
connection: postgresql://airflow:password@postgres.rds:5432/airflow?sslmode=disable
```
To use for both metadata and resultBackendConnection then we end up with a connection error because
resultBackendConnection expects the string to be formatted like 
```
connection: db+postgresql://airflow:password@postgres.rds:5432/airflow?sslmode=disable
```
from what i can tell

### What you expected to happen

I'd expect to be able to use the same secret for both using the same format if they are using the same connection. 

### How to reproduce

Make a secret structured like above to look like the metadataConnection auto-generated secret.
use that same secret for the result backend.
deploy.


### Anything else

Occurs always. 
To get around currently we make 2 secrets one with just the db+ prepended. 

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/security/test_result_backend_connection_secret.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/other/test_pgbouncer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/security/test_metadata_connection_secret.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/other/test_keda.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/variable.py']
Ground Truth : ['a/airflow/config_templates/default_celery.py']
Current Recall: 0.042043948200265115

=========================================================

ISSUE: ParamsDict represents the class object itself, not keys and values on Task Instance Details
### Apache Airflow version

2.3.3 (latest released)

### What happened

ParamsDict's printable presentation shows the class object itself like `<airflow.models.param.ParamsDict object at 0x7fd0eba9bb80>` on the page of Task Instance Detail because it does not have `__repr__` method in its class. 

<img width="791" alt="image" src="https://user-images.githubusercontent.com/16971553/180902761-88b9dd9f-7102-4e49-b8b8-0282b31dda56.png">

It used to be `dict` object and what keys and values Params include are shown on UI before replacing Params with the advanced Params by #17100. 



### What you think should happen instead

It was originally shown below when it was `dict` object. 

![image](https://user-images.githubusercontent.com/16971553/180904396-7b527877-5bc6-48d2-938f-7d338dfd79a7.png)


I think it can be fixed by adding `__repr__` method to the class like below.

```python
class ParamsDict(dict):
    ...
    def __repr__(self):
        return f"{self.dump()}"
```

### How to reproduce

I guess it all happens on Airflow using 2.2.0+

### Operating System

Linux, but it's not depending on OS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py']
Ground Truth : ['a/airflow/models/param.py']
Current Recall: 0.042043948200265115

=========================================================

ISSUE: Webserver doesn't mask rendered fields for pending tasks
### Apache Airflow version

2.2.5 (latest released)

### What happened

When triggering a new dagrun the webserver will not mask secrets in the rendered fields for that dagrun's tasks which didn't start yet.

Tasks which have completed or are in state running are not affected by this.

### What you think should happen instead

The webserver should mask all secrets for tasks which have started or not started.

<img width="628" alt="Screenshot 2022-04-04 at 15 36 29" src="https://user-images.githubusercontent.com/7921017/161628806-c2c579e2-faea-40cc-835c-ac6802d15dc1.png">
.

### How to reproduce

Create a variable `my_secret` and run this DAG

```python
from datetime import timedelta

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.sensors.time_delta import TimeDeltaSensor
from airflow.utils.dates import days_ago

with DAG(
    "secrets",
    start_date=days_ago(1),
    schedule_interval=None,
) as dag:
    wait = TimeDeltaSensor(
        task_id="wait",
        delta=timedelta(minutes=1),
    )

    task = wait >> BashOperator(
        task_id="secret_task",
        bash_command="echo '{{ var.value.my_secret }}'",
    )
```

While the first task `wait` is running, displaying rendered fields for the second task `secret_task` will show the unmasked secret variable.

<img width="1221" alt="Screenshot 2022-04-04 at 15 33 43" src="https://user-images.githubusercontent.com/7921017/161628734-b7b13190-a3fe-4898-8fa9-ff7537245c1c.png">



### Operating System

Debian (Astronomer Airflow Docker image)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==1!3.2.0
apache-airflow-providers-cncf-kubernetes==1!3.0.0
apache-airflow-providers-elasticsearch==1!3.0.2
apache-airflow-providers-ftp==1!2.1.2
apache-airflow-providers-google==1!6.7.0
apache-airflow-providers-http==1!2.1.2
apache-airflow-providers-imap==1!2.2.3
apache-airflow-providers-microsoft-azure==1!3.7.2
apache-airflow-providers-mysql==1!2.2.3
apache-airflow-providers-postgres==1!4.1.0
apache-airflow-providers-redis==1!2.0.4
apache-airflow-providers-slack==1!4.2.3
apache-airflow-providers-sqlite==1!2.1.3
apache-airflow-providers-ssh==1!2.4.3
```

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

We have seen this issue also in Airflow 2.2.3.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_rendered.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py']
Ground Truth : ['a/airflow/dag_processing/manager.py', 'a/airflow/models/taskinstance.py', 'a/airflow/configuration.py', 'a/airflow/providers/databricks/hooks/databricks_base.py', 'a/airflow/utils/db.py', 'a/airflow/utils/file.py', '/dev/null', 'a/dev/chart/build_changelog_annotations.py', 'a/dev/breeze/src/airflow_breeze/commands/main_command.py', 'a/airflow/migrations/utils.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/providers/databricks/operators/databricks.py', 'a/airflow/utils/process_utils.py', 'a/airflow/providers/microsoft/psrp/operators/psrp.py', 'a/airflow/providers/amazon/aws/hooks/sns.py', 'a/setup.py', 'a/airflow/providers/google/cloud/utils/credentials_provider.py', 'a/airflow/www/views.py', 'a/dev/provider_packages/prepare_provider_packages.py', 'a/dev/breeze/src/airflow_breeze/params/shell_params.py', 'a/dev/breeze/src/airflow_breeze/global_constants.py', 'a/airflow/cli/commands/dag_processor_command.py', 'a/airflow/providers/amazon/aws/hooks/ses.py', 'a/airflow/models/dag.py', 'a/dev/breeze/src/airflow_breeze/commands/developer_commands.py', 'a/dev/breeze/src/airflow_breeze/utils/run_utils.py', 'a/airflow/decorators/base.py', 'a/airflow/providers/amazon/aws/hooks/eks.py', 'a/airflow/providers/databricks/hooks/databricks.py', 'a/airflow/utils/email.py', 'a/airflow/utils/log/secrets_masker.py', 'a/airflow/settings.py', 'a/airflow/www/fab_security/manager.py', 'a/scripts/in_container/run_migration_reference.py', 'a/airflow/providers/google/cloud/hooks/kubernetes_engine.py', 'a/scripts/ci/pre_commit/pre_commit_supported_versions.py', 'a/airflow/example_dags/example_subdag_operator.py', 'a/airflow/providers/databricks/operators/databricks_repos.py', 'a/airflow/providers/amazon/aws/hooks/sqs.py', 'a/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', 'a/scripts/ci/pre_commit/pre_commit_flake8.py', 'a/dev/breeze/src/airflow_breeze/utils/reinstall.py', 'a/airflow/providers/microsoft/azure/hooks/cosmos.py', 'a/airflow/providers/ssh/hooks/ssh.py', 'a/dev/breeze/src/airflow_breeze/utils/path_utils.py', 'a/airflow/hooks/dbapi.py', 'a/airflow/api_connexion/schemas/task_schema.py', 'a/scripts/ci/pre_commit/pre_commit_mypy.py', 'a/dev/breeze/src/airflow_breeze/utils/image.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/www/utils.py', 'a/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py']
Current Recall: 0.042043948200265115

=========================================================

ISSUE: Kubernetes logging errors - attempting to adopt taskinstance which was not specified by database
### Apache Airflow version

2.4.3

### What happened

Using following config
```
executor = CeleryKubernetesExecutor
delete_worker_pods = False
```

1. Start a few dags running in kubernetes, wait for them to complete.
2. Restart Scheduler.
3. Logs are flooded with hundreds of errors like` ERROR - attempting to adopt taskinstance which was not specified by database: TaskInstanceKey(dag_id='xxx', task_id='yyy', run_id='zzz', try_number=1, map_index=-1)`

This is problematic because:
* Our installation has thousands of dags and pods so this becomes very noisy and the adoption-process adds excessive startup-time to the scheduler, up to a minute some times.
* It's hiding actual errors with resetting orphaned tasks, something that also happens for inexplicable reasons on scheduler restart with following log: `Reset the following 6 orphaned TaskInstances`. Making such much harder to debug. The cause of them can not be easily correlated with those that were not specified by database.


The cause of these logs are the Kubernetes executor on startup loads all pods (`try_adopt_task_instances`), it then cross references them with all `RUNNING` TaskInstances loaded via `scheduler_job.adopt_or_reset_orphaned_tasks`.
For all pods where a running TI can not be found, it logs the error above - But for TIs that were already completed this is not an error, and the pods should not have to be loaded at all.

I have an idea of adding some code in the kubernetes_executor that patches in something like a `completion-acknowleged`-label whenever a pod is completed (unless `delete_worker_pods` is set). Then on startup, all pods having this label can be excluded. Is this a good idea or do you see other potential solutions? 
Another potential solution is to inside `try_adopt_task_instances` only fetch the exact pod-id specified in each task-instance, instead of listing all to later cross-reference them.

### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Operating System

Ubuntu 22.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py']
Ground Truth : ['a/airflow/executors/kubernetes_executor.py']
Current Recall: 0.044185275823391455

=========================================================

ISSUE: Show  schedule_interval/timetable description for schedule cron expression in Airflow UI
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

**Description**
<!-- A short description of your feature -->
Users should be able to see a short description of the cron expression when hovering over the expression in a tooltip.

![image](https://user-images.githubusercontent.com/16856802/123651190-e7045d80-d848-11eb-9ed5-529d806992d1.png)

In the above case, the user should see description like  **At 09:30 PM, only on Friday**

**Use case / motivation**
It will be really easy to get schedule information for the users who are not really great at reading CRON,
even for the experience users CRON can be really complicated at times.

We will create a tooltip component which will evaluate underlying cron and show the description.

**Are you willing to submit a PR?**
YES

**Related Issues**
NONE

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/_cron.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py']
Ground Truth : ['a/airflow/timetables/interval.py', '/dev/null', 'a/airflow/models/dag.py', 'a/airflow/timetables/simple.py', 'a/airflow/timetables/base.py']
Current Recall: 0.044613541348016726

=========================================================

ISSUE: SQLAlchemy engine configuration is not passed to FAB based UI
SQLAlchemy engine configuration is not passed to FAB based UI. Faced this issue when running Airflow with MySQL metadata store with wait_timeout = 120. Webserver is failing with Internal Server Error due to "[MySQL Server has gone away](https://docs.sqlalchemy.org/en/13/faq/connections.html#mysql-server-has-gone-away)" error. Settings like `sql_alchemy_pool_recycle` and  `sql_alchemy_pool_pre_ping` have no impact on FAB internal communication with MySQL.

**Apache Airflow version**: 1.10.12

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0118_2_4_2_add_missing_autoinc_fab.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/teradata/hooks/teradata.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/base.py']
Ground Truth : ['a/airflow/www/app.py', 'a/airflow/settings.py']
Current Recall: 0.045684205159579896

=========================================================

ISSUE: Databricks hook: Retry also on HTTP Status 429 - rate limit exceeded
### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

2.2.0

### Apache Airflow version

2.2.3 (latest released)

### Operating System

Any

### Deployment

Other

### Deployment details

_No response_

### What happened

Operations aren't retried when Databricks API returns HTTP Status 429 - rate limit exceeded

### What you expected to happen

the operation should retry

### How to reproduce

this happens when you have multiple calls to API, especially when it happens outside of Airflow

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/synapse.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py']
Ground Truth : ['a/airflow/providers/databricks/operators/databricks.py', 'a/airflow/providers/databricks/hooks/databricks_base.py', 'a/airflow/providers/databricks/hooks/databricks.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Webserver fails to pull secrets from Hashicorp Vault on start up
### Apache Airflow version

2.3.4

### What happened

Since upgrading to Airflow 2.3.4 our webserver fails on start up to pull secrets from our Vault instance.  Setting AIRFLOW__WEBSERVER_WORKERS = 1 allowed the webserver to start up successfully, but reverting the change added here [https://github.com/apache/airflow/pull/25556](url) was the only way we found to fix the issue without adjusting the webserver's worker count.

### What you think should happen instead

The airflow webserver should be able to successfully read from Vault with AIRFLOW__WEBSERVERS__WORKERS > 1.

### How to reproduce

Star a Webserver instance set to authenticate with Vault using the approle method and AIRFLOW__DATABASE__SQL_ALCHEMY_CONN_SECRET and AIRFLOW__WEBSERVER__SECRET_KEY_SECRET set.  The webserver should fail to initialize all of the gunicorn workers and exit.

### Operating System

Fedora 29

### Versions of Apache Airflow Providers

apache-airflow-providers-hashicorp==3.1.0

### Deployment

Docker-Compose

### Deployment details

Python 3.9.13
Vault 1.9.4

### Anything else

None

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_webserver_command.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Elasticsearch Backport Provider Incompatible with Airflow 1.10.12
**Apache Airflow version**: 1.10.12


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): 1.16.9

**Environment**: 

- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): 
- **Kernel** (e.g. `uname -a`): 
- **Install tools**: Docker image running in k8s Pods
- **Others**: Rancher-provisioned k8s clusters

**What happened**:

After configuring the latest version of the Elasticsearch backport provider as my log handler via `config/airflow_local_settings.py` resulted in an error on the webserver when trying to read logs from Elasticsearch

```
[2020-10-12 21:02:00,487] {app.py:1892} ERROR - Exception on /get_logs_with_metadata [GET]
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.7/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/decorators.py", line 121, in wrapper
    return f(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/security/decorators.py", line 109, in wraps
    return f(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/decorators.py", line 56, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/views.py", line 733, in get_logs_with_metadata
    logs, metadata = _get_logs_with_metadata(try_number, metadata)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/views.py", line 724, in _get_logs_with_metadata
    logs, metadatas = handler.read(ti, try_number, metadata=metadata)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py", line 194, in read
    logs[i] += log
TypeError: can only concatenate str (not "list") to str
```

Here is the relevant section of my customized `airflow_local_settings.py` file with the updated Elasticsearch handler from the backport provider:
```
...
    elif ELASTICSEARCH_HOST:
        ELASTICSEARCH_LOG_ID_TEMPLATE: str = conf.get('elasticsearch', 'LOG_ID_TEMPLATE')
        ELASTICSEARCH_END_OF_LOG_MARK: str = conf.get('elasticsearch', 'END_OF_LOG_MARK')
        ELASTICSEARCH_FRONTEND: str = conf.get('elasticsearch', 'frontend')
        ELASTICSEARCH_WRITE_STDOUT: bool = conf.getboolean('elasticsearch', 'WRITE_STDOUT')
        ELASTICSEARCH_JSON_FORMAT: bool = conf.getboolean('elasticsearch', 'JSON_FORMAT')
        ELASTICSEARCH_JSON_FIELDS: str = conf.get('elasticsearch', 'JSON_FIELDS')

        ELASTIC_REMOTE_HANDLERS: Dict[str, Dict[str, Union[str, bool]]] = {
            'task': {
                'class': 'airflow.providers.elasticsearch.log.es_task_handler.ElasticsearchTaskHandler',
                'formatter': 'airflow',
                'base_log_folder': str(os.path.expanduser(BASE_LOG_FOLDER)),
                'log_id_template': ELASTICSEARCH_LOG_ID_TEMPLATE,
                'filename_template': FILENAME_TEMPLATE,
                'end_of_log_mark': ELASTICSEARCH_END_OF_LOG_MARK,
                'host': ELASTICSEARCH_HOST,
                'frontend': ELASTICSEARCH_FRONTEND,
                'write_stdout': ELASTICSEARCH_WRITE_STDOUT,
                'json_format': ELASTICSEARCH_JSON_FORMAT,
                'json_fields': ELASTICSEARCH_JSON_FIELDS
            },
        }

        LOGGING_CONFIG['handlers'].update(ELASTIC_REMOTE_HANDLERS)
...
```

**What you expected to happen**:

Airflow's web UI properly displays the logs from Elasticsearch

**How to reproduce it**:
Configure custom logging via `config/airflow_local_settings.py` to `airflow.providers.elasticsearch.log.es_task_handler.ElasticsearchTaskHandler` and set the `logging_config_class` in `airflow.cfg`

When a task has been run, try to view its logs in the web UI and check the webserver logs to see the error above


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/airflow_local_settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py']
Ground Truth : ['a/provider_packages/refactor_provider_packages.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Tasks in an infinite slots pool are never scheduled
**Apache Airflow version**: v2.0.0 and up
**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): not tested with K8
**Environment**:
all

**What happened**:

Executing the unit test included below, or create an infinite pool ( `-1` slots ) and tasks that should be executed in that pool.
```
INFO     airflow.jobs.scheduler_job.SchedulerJob:scheduler_job.py:991 Not scheduling since there are -1 open slots in pool test_scheduler_verify_infinite_pool
```

**What you expected to happen**:

To schedule tasks, or to drop support for infinite slots pools?

**How to reproduce it**:
easiest one is this unit test:
```
def test_scheduler_verify_infinite_pool(self):
    """
    Test that TIs are still scheduled if we only have one infinite pool.
    """
    dag = DAG(dag_id='test_scheduler_verify_infinite_pool', start_date=DEFAULT_DATE)
    BashOperator(
        task_id='test_scheduler_verify_infinite_pool_t0',
        dag=dag,
        owner='airflow',
        pool='test_scheduler_verify_infinite_pool',
        bash_command='echo hi',
    )

    dagbag = DagBag(
        dag_folder=os.path.join(settings.DAGS_FOLDER, "no_dags.py"),
        include_examples=False,
        read_dags_from_db=True,
    )
    dagbag.bag_dag(dag=dag, root_dag=dag)
    dagbag.sync_to_db()

    session = settings.Session()
    pool = Pool(pool='test_scheduler_verify_infinite_pool', slots=-1)
    session.add(pool)
    session.commit()

    dag = SerializedDAG.from_dict(SerializedDAG.to_dict(dag))

    scheduler = SchedulerJob(executor=self.null_exec)
    scheduler.processor_agent = mock.MagicMock()

    dr = dag.create_dagrun(
        run_type=DagRunType.SCHEDULED,
        execution_date=DEFAULT_DATE,
        state=State.RUNNING,
    )
    scheduler._schedule_dag_run(dr, {}, session)

    task_instances_list = scheduler._executable_task_instances_to_queued(max_tis=32, session=session)

    # Let's make sure we don't end up with a `max_tis` == 0
    assert len(task_instances_list) >= 1
```

**Anything else we need to know**:

Overall I'm not sure whether it's worth fixing in those various spots:
https://github.com/bperson/airflow/blob/master/airflow/jobs/scheduler_job.py#L908
https://github.com/bperson/airflow/blob/master/airflow/jobs/scheduler_job.py#L971
https://github.com/bperson/airflow/blob/master/airflow/jobs/scheduler_job.py#L988
https://github.com/bperson/airflow/blob/master/airflow/jobs/scheduler_job.py#L1041
https://github.com/bperson/airflow/blob/master/airflow/jobs/scheduler_job.py#L1056

Or whether to restrict `-1` ( infinite ) slots in pools:
https://github.com/bperson/airflow/blob/master/airflow/models/pool.py#L49

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_subdag_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/scheduler_dag_execution_timing.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py']
Ground Truth : ['a/airflow/models/pool.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: No more SQL Exception in 2.1.0
**Apache Airflow version**:

2.1.0

**Environment**:

- self hosted docker-compose based stack

**What happened**:

Using JDBCOperator if the sql result in an error we got only 
```
[2021-06-21 11:05:55,377] {local_task_job.py:151} INFO - Task exited with return code 1
```

Before upgrading from 2.0.1 we got error details in logs:
```
jaydebeapi.DatabaseError: java.sql.SQLException:... [MapR][DrillJDBCDriver](500165) Query execution error. Details: VALIDATION ERROR:...
``` 

**What you expected to happen**:

See `SQLException` in logs

**How to reproduce it**:

Perform a generic SQL task with broken SQL.

**Anything else we need to know**:

I think is somehow related to https://github.com/apache/airflow/commit/abcd48731303d9e141bdc94acc2db46d73ccbe12#diff-4fd3febb74d94b2953bf5e9b4a981b617949195f83d96f4a589c3078085959b7R202 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Zombie tasks in RESTARTING state are not cleaned
### Apache Airflow version

2.7.0

Also reproduced on 2.5.0

### What happened

Recently we added some automation to restarting Airflow tasks with "clear" command so we use this feature a lot. We often clear tasks in RUNNING state, which means that they go into RESTARTING state. We noticed that a lot of those tasks get stuck in RESTARTING state. Our Airflow infrastructure runs in an environment where any process can get suddenly killed without graceful shutdown.

We run Airflow on GKE but I managed to reproduce this behaviour on local environment with SequentialExecutor. See **"How to reproduce"** below for details.

### What you think should happen instead

Tasks should get cleaned after scheduler restart and eventually get scheduled and executed.

### How to reproduce

After some code investigation, I reproduced this kind of behaviour on local environment and it seems that RESTARTING tasks are only properly handled if the original restarting task is gracefully shut down so it can mark task as UP_FOR_RETRY or at least there is a healthy scheduler to do it if they fail for any other reason. The problem is with the following scenario:

1. Task is initially in RUNNING state.
2. Scheduler process dies suddenly.
3. The task process also dies suddenly.
4. "clear" command is executed on the task so the state is changed to RESTARTING state by webserver process.
5. From now on, even if we restart scheduler, the task will never get scheduled or change its state. It needs to have its state manually fixed, e.g. by clearing it again.

A recording of steps to reproduce on local environment:
https://vimeo.com/857192666?share=copy

### Operating System

MacOS Ventura 13.4.1

### Versions of Apache Airflow Providers

N/A

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

N/A

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py']
Ground Truth : ['a/airflow/jobs/scheduler_job_runner.py', 'a/airflow/utils/state.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Task stuck in queued state after pod fails to start
**Apache Airflow version**: `2.0.1`


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):  
`Client Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.1", GitCommit:"7879fc12a63337efff607952a323df90cdc7a335", GitTreeState:"clean", BuildDate:"2020-04-10T21:53:58Z", GoVersion:"go1.14.2", Compiler:"gc", Platform:"darwin/amd64"}`  

`Server Version: version.Info{Major:"1", Minor:"16+", GitVersion:"v1.16.15-gke.7800", GitCommit:"cef3156c566a1d1a4b23ee360a760f45bfbaaac1", GitTreeState:"clean", BuildDate:"2020-12-14T09:12:37Z", GoVersion:"go1.13.15b4", Compiler:"gc", Platform:"linux/amd64"}`

**Environment**:

- **Cloud provider or hardware configuration**: `GKE`
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**: We use scheduler HA with 2 instances. 

**What happened**:
- We are running Airflow 2.0.1 using KubernetesExecutor and PostgreSQL 9.6.2.
- Task is `up_for_retry` after its worker pod fails to start
- It gets stuck in `queued` state
- It runs only after a scheduler restart. 

**What you expected to happen**:
- Task gets rescheduled and runs successfully.

**How to reproduce it**:


**Anything else we need to know**:

Logs below. 

<details>


Its upstream task succeeds and schedules it via fast follow.  
`[2021-03-01 17:08:51,306] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=datalake_dag_id, task_id=processidlogs, execution_date=20210301T163000, start_date=20210301T170546, end_date=20210301T170851`

`[2021-03-01 17:08:51,339] {taskinstance.py:1220} INFO - 1 downstream tasks scheduled from follow-on schedule check`

`[2021-03-01 17:08:51,357] {local_task_job.py:146} INFO - Task exited with return code 0`


Task is attempted to be run.  

`[2021-03-01 17:08:52,229] {scheduler_job.py:1105} INFO - Sending TaskInstanceKey(dag_id='datalake_dag_id', task_id='delta_id_logs', execution_date=datetime.datetime(2021, 3, 1, 16, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default`

`[2021-03-01 17:08:52,308] {kubernetes_executor.py:306} DEBUG - Kubernetes running for command ['airflow', 'tasks', 'run', 'datalake_dag_id', 'delta_id_logs', '2021-03-01T16:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/dags/data_lake/some_tasks/some_tasks.py']`

`[2021-03-01 17:08:52,332] {scheduler_job.py:1206} INFO - Executor reports execution of datalake_dag_id.delta_id_logs execution_date=2021-03-01 16:30:00+00:00 exited with status queued for try_number 1`



Pod fails to start.  

`[2021-03-01 17:12:17,319] {kubernetes_executor.py:197} INFO - Event: Failed to start pod datalakedagiddeltaidlogs.5fa98ae3856f4cb4b6c8810ac13e5c6a, will reschedule`


It is put as up_for_reschedule.  

`[2021-03-01 17:12:23,912] {kubernetes_executor.py:343} DEBUG - Processing task ('datalakedagiddeltaidlogs.5fa98ae3856f4cb4b6c8810ac13e5c6a', 'prod', 'up_for_reschedule', {'dag_id': 'datalake_dag_id', 'task_id': 'delta_id_logs', 'execution_date': '2021-03-01T16:30:00+00:00', 'try_number': '1'}, '1172208829')`


`[2021-03-01 17:12:23,930] {kubernetes_executor.py:528} INFO - Changing state of (TaskInstanceKey(dag_id='datalake_dag_id', task_id='delta_id_logs', execution_date=datetime.datetime(2021, 3, 1, 16, 30, tzinfo=tzlocal()), try_number=1), 'up_for_reschedule', 'datalakedagiddeltaidlogs.5fa98ae3856f4cb4b6c8810ac13e5c6a', 'prod', '1172208829') to up_for_reschedule`

`[2021-03-01 17:12:23,941] {scheduler_job.py:1206} INFO - Executor reports execution of datalake_dag_id.delta_id_logs execution_date=2021-03-01 16:30:00+00:00 exited with status up_for_reschedule for try_number 1`


A few minutes later, another scheduler finds it in queued state.  

`[2021-03-01 17:15:39,177] {taskinstance.py:851} DEBUG - Dependencies all met for <TaskInstance: datalake_dag_id.delta_id_logs 2021-03-01 16:30:00+00:00 [queued]>`

`[2021-03-01 17:15:40,477] {taskinstance.py:866} DEBUG - <TaskInstance: datalake_dag_id.delta_id_logs 2021-03-01 16:30:00+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The context specified that being in a retry period was permitted.`

`[2021-03-01 17:15:40,478] {taskinstance.py:866} DEBUG - <TaskInstance: datalake_dag_id.delta_id_logs 2021-03-01 16:30:00+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.`


It stays in that state for another hour and a half until the scheduler is restarted.  

Finally, it is rescheduled.  
`[2021-03-01 18:58:10,475] {kubernetes_executor.py:463} INFO - TaskInstance: <TaskInstance: datalake_dag_id.delta_id_logs 2021-03-01 16:30:00+00:00 [queued]> found in queued state but was not launched, rescheduling`

</details>

Other tasks run fine while that one is stuck in queued state. We have a cron job to restart the scheduler as a hack to recover from when such cases happen, but we would like to avoid it as much as possible. 

We run 60 DAGs with 50-100 tasks each every 30 minutes. We have been seeing this issue at least once daily since we upgraded to Airflow 2.0.1. 

I understand there are some open issues about the scheduler or tasks getting stuck. But I could not tell if this is related, since hundreds of other tasks run as expected. Apologies if this turns out to be a duplicate of an existing issue. Thank you. 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_cluster_activity.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py']
Ground Truth : ['a/airflow/executors/kubernetes_executor.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Difference of extras Airflow 2.0 vs. Airflow 1.10
**Description**

When airflow 2.0 is installed from  PyPI, providers are not installed by default. In order to install them, you should add an appropriate extra. While this behavior is identical in Airflow 1.10 for those "providers" that required additional packages, there were a few "providers" that did not require any extras to function (example http, ftp) - we have "http", "ftp" extras for them now, but maybe some of those are popular enough to be included by default?.

We have to make a decision now:

- [x] should all of them (or some of them) be included by default when you install Airflow?
- [x] if we decide to exclude only some (or none), we should add them in UPGRADING_to_2_0 and in UPDATING documentation.

**Use case / motivation**

We want people to get a familiar experience when installing airflow. Why we provide familiar mechanism (with extras) and people will expect a slightly different configurations, installation and we can describe the differences, maybe some of those providers are so popular that we should include them by default? 

**Related Issues**

#12685 - where we discuss which of the extras should be included in the Production Image of 2.0.


**Additional info**

Here is the list of all "providers" that were present in 1.10 and had no additional dependencies - so basically they woudl work out-fhe-box in 1.10, but they need appropriate "extra" in 2.0.


*  "apache.pig": [],
*  "apache.sqoop": [],
*  "dingding": [],
*  "discord": [],
*  "ftp": [],
*  "http": [],
*  "imap": [],
*  "openfaas": [],
*  "opsgenie": [],
*  "sqlite": [],

Also here I appeal to the wisdom of crowd: @ashb, @dimberman @kaxil, @turbaszek, @mik-laj. @XD-DENG, @feluelle, @eladkal, @ryw, @vikramkoka, @KevinYang21  - let me know WDYT before I bring it to devlist?

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/hooks/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Extra field widgets of custom connections do not properly save data
**Apache Airflow version**: 2.0.0

**Environment**: Docker image `apache/airflow:2.0.0-python3.8` on Win10 with WSL

**What happened**:

I built a custom provider with a number of custom connections.

This works:
- The connections are properly registered
- The UI does not show hidden fields as per `get_ui_field_behaviour`
- The UI correctly relabels fields as per `get_ui_field_behaviour`
- The UI correctly shows added widgets as per `get_connection_form_widgets` (well, mostly)

What does not work:
- The UI does not save values entered for additional widgets

I used the [JDBC example](https://github.com/apache/airflow/blob/master/airflow/providers/jdbc/hooks/jdbc.py) to string myself along by copying it and pasting it as a hook into my custom provider package. (I did not install the JDBC provider package, unless it is installed in the image I use - but if I don't add it in my own provider package, I don't have the connection type in the UI, so I assume it is not). Curiously, The JDBC hook works just fine. I then created the following file:

```Python
"""
You find two child classes of DbApiHook in here. One is the exact copy of the JDBC
provider hook, minus some irrelevant logic (I only care about the UI stuff here).
The other is the exact same thing, except I added an "x" behind every occurance
of "jdbc" in strings and names.
"""

from typing import Any, Dict, Optional
from airflow.hooks.dbapi import DbApiHook

class JdbcXHook(DbApiHook):
    """
    Copy of JdbcHook below. Added an "x" at various places, including the class name.
    """

    conn_name_attr = 'jdbcx_conn_id'  # added x
    default_conn_name = 'jdbcx_default' # added x
    conn_type = 'jdbcx' # added x
    hook_name = 'JDBCx Connection' # added x
    supports_autocommit = True

    @staticmethod
    def get_connection_form_widgets() -> Dict[str, Any]:
        """Returns connection widgets to add to connection form"""
        from flask_appbuilder.fieldwidgets import BS3TextFieldWidget
        from flask_babel import lazy_gettext
        from wtforms import StringField

        # added an x in the keys
        return {
            "extra__jdbcx__drv_path": StringField(lazy_gettext('Driver Path'), widget=BS3TextFieldWidget()),
            "extra__jdbcx__drv_clsname": StringField(
                lazy_gettext('Driver Class'), widget=BS3TextFieldWidget()
            ),
        }

    @staticmethod
    def get_ui_field_behaviour() -> Dict:
        """Returns custom field behaviour"""
        return {
            "hidden_fields": ['port', 'schema', 'extra'],
            "relabeling": {'host': 'Connection URL'},
        }

class JdbcHook(DbApiHook):
    """
    General hook for jdbc db access.
    JDBC URL, username and password will be taken from the predefined connection.
    Note that the whole JDBC URL must be specified in the "host" field in the DB.
    Raises an airflow error if the given connection id doesn't exist.
    """

    conn_name_attr = 'jdbc_conn_id'
    default_conn_name = 'jdbc_default'
    conn_type = 'jdbc'
    hook_name = 'JDBC Connection plain'
    supports_autocommit = True

    @staticmethod
    def get_connection_form_widgets() -> Dict[str, Any]:
        """Returns connection widgets to add to connection form"""
        from flask_appbuilder.fieldwidgets import BS3TextFieldWidget
        from flask_babel import lazy_gettext
        from wtforms import StringField

        return {
            "extra__jdbc__drv_path": StringField(lazy_gettext('Driver Path'), widget=BS3TextFieldWidget()),
            "extra__jdbc__drv_clsname": StringField(
                lazy_gettext('Driver Class'), widget=BS3TextFieldWidget()
            ),
        }

    @staticmethod
    def get_ui_field_behaviour() -> Dict:
        """Returns custom field behaviour"""
        return {
            "hidden_fields": ['port', 'schema', 'extra'],
            "relabeling": {'host': 'Connection URL'},
        }
```

**What you expected to happen**:

After doing the above, I expected
- Seeing both in the add connection UI
- Being able to use both the same way

**What actually happenes**:
- I _do_ see both in the UI (Screenshot 1)
- For some reason, the "normal" hook has BOTH extra fields - not just his own two? (Screenshot 2)
- If I add the connection as in Screenshot 2, they are saved in the four fields (his own two + the two for the "x" hook) properly as shown in Screenshot 3
- If I seek to edit the connection again, they are also they - all four fields - with the correct values in the UI
- If I add the connection for the "x" type as in Screenshot 4, it ostensibly saves it - with two fields as defined in the code
- You can see in screenshot 5, that the extra is saved as an empty string?!
- When trying to edit the connection in the UI, you also see that there is no data saved for two extra widgets?!
- I added a few more screenshots of airflow providers CLI command results (note that the package `ewah` has a number of other custom hooks, and the issue above occurs for *all* of them)

*Screenshot 1:*
![image](https://user-images.githubusercontent.com/46958547/104121824-9acc6c00-5341-11eb-821c-4bff40a0e7c7.png)

*Screenshot 2:*
![image](https://user-images.githubusercontent.com/46958547/104121854-c94a4700-5341-11eb-8d3c-80b6380730d9.png)

*Screenshot 3:*
![image](https://user-images.githubusercontent.com/46958547/104121912-247c3980-5342-11eb-8030-11c7348309f3.png)

*Screenshot 4:*
![image](https://user-images.githubusercontent.com/46958547/104121944-5e4d4000-5342-11eb-83b7-870711ccd367.png)

*Screenshot 5:*
![image](https://user-images.githubusercontent.com/46958547/104121971-82a91c80-5342-11eb-83b8-fee9386c0c4f.png)

*Screenshot 6 - airflow providers behaviours:*
![image](https://user-images.githubusercontent.com/46958547/104122073-1c70c980-5343-11eb-88f6-6130e5de9e92.png)

*Screenshot 7 - airflow providers get:*
![image](https://user-images.githubusercontent.com/46958547/104122092-41fdd300-5343-11eb-9bda-f6849812ba56.png)
(Note: This error occurs with pre-installed providers as well)

*Screenshot 8 - airflow providers hooks:*
![image](https://user-images.githubusercontent.com/46958547/104122109-65288280-5343-11eb-8322-dda73fef6649.png)

*Screenshot 9 - aorflow providers list:*
![image](https://user-images.githubusercontent.com/46958547/104122191-c94b4680-5343-11eb-80cf-7f510d4b6e9a.png)

*Screenshot 10 - airflow providers widgets:*
![image](https://user-images.githubusercontent.com/46958547/104122142-930dc700-5343-11eb-96be-dec43d87a59d.png)


**How to reproduce it**:

- create a custom provider package
- add the code snippet pasted above somewhere
- add the two classes to the `hook-class-names` list in the provider info
- install the provider package
- do what I described above



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/hooks/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jdbc/hooks/jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/hooks/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/redis/hooks/redis.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/container_registry.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/adx.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/hooks/yandex.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/hooks/kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/forms.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/smtp/hooks/smtp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: airflow tasks run --ship-dag not able to generate pickeled dag
**Apache Airflow version**: 2.0.1


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): No

**Environment**:

- **Cloud provider or hardware configuration**: local machine
- **OS** (e.g. from /etc/os-release): 18.04.5 LTS (Bionic Beaver)
- **Kernel** (e.g. `uname -a`): wsl2

**What happened**:
Getting Pickled_id: None

```
root@12c7fd58e084:/opt/airflow# airflow tasks run example_bash_operator runme_0 now --ship-dag --interactive
[2021-05-09 13:11:33,247] {dagbag.py:487} INFO - Filling up the DagBag from /files/dags
Running <TaskInstance: example_bash_operator.runme_0 2021-05-09T13:11:31.788923+00:00 [None]> on host 12c7fd58e084
Pickled dag <DAG: example_bash_operator> as pickle_id: None
Sending to executor.
[2021-05-09 13:11:34,722] {base_executor.py:82} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2021-05-09T13:11:31.788923+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/airflow/example_dags/example_bash_operator.py']
[2021-05-09 13:11:34,756] {local_executor.py:81} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2021-05-09T13:11:31.788923+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/airflow/example_dags/example_bash_operator.py']
[2021-05-09 13:11:34,757] {local_executor.py:386} INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
[2021-05-09 13:11:34,817] {dagbag.py:487} INFO - Filling up the DagBag from /opt/airflow/airflow/example_dags/example_bash_operator.py
Running <TaskInstance: example_bash_operator.runme_0 2021-05-09T13:11:31.788923+00:00 [None]> on host 12c7fd58e084
```

**What you expected to happen**:

Pickled_id should get generated 

```
Pickled dag <DAG: example_bash_operator> as pickle_id: None
```

**How to reproduce it**:
run below command from command line in airflow environment
```
airflow tasks run example_bash_operator runme_0 now --ship-dag --interactive
```
**Would like to submit PR for this issue**: YES



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Unable to access operator attrs within Jinja context for mapped tasks
### Apache Airflow version

2.3.2 (latest released)

### What happened

When attempting to generate mapped SQL tasks using a Jinja-templated query that access operator attributes, an exception like the following is thrown:

`jinja2.exceptions.UndefinedError: 'airflow.models.mappedoperator.MappedOperator object' has no attribute '<operator attribute>'`

For example, when attempting to map `SQLValueCheckOperator` tasks with respect to `database` using a query of `SELECT COUNT(*) FROM {{ task.database }}.tbl;`:
`jinja2.exceptions.UndefinedError: 'airflow.models.mappedoperator.MappedOperator object' has no attribute 'database'`

Or, when using `SnowflakeOperator` and mapping via `parameters` of a query like `SELECT * FROM {{ task.parameters.tbl }};`:
`jinja2.exceptions.UndefinedError: 'airflow.models.mappedoperator.MappedOperator object' has no attribute 'parameters'`

### What you think should happen instead

When using Jinja-template SQL queries, the attribute that is being using for the mapping should be accessible via `{{ task.<operator attribute> }}`. Executing the same SQL query with classic, non-mapped tasks allows for this operator attr access from the `task` context object.

Ideally, the same interface should apply for both non-mapped and mapped tasks. Also with the preference of using `parameters` over `params` in SQL-type operators, having the ability to map over `parameters` will help folks move from using `params` to `parameters`.

### How to reproduce

Consider the following DAG:
```python
from pendulum import datetime

from airflow.decorators import dag
from airflow.operators.sql import SQLValueCheckOperator
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator


CORE_SQL = "SELECT COUNT(*) FROM {{ task.database }}.tbl;"
SNOWFLAKE_SQL = """SELECT * FROM {{ task.parameters.tbl }};"""


@dag(dag_id="map-city", start_date=datetime(2022, 6, 7), schedule_interval=None)
def map_city():
        classic_sql_value_check = SQLValueCheckOperator(
        task_id="classic_sql_value_check",
        conn_id="snowflake",
        sql=CORE_SQL,
        database="dev",
        pass_value=20000,
    )

    mapped_value_check = SQLValueCheckOperator.partial(
        task_id="check_row_count",
        conn_id="snowflake",
        sql=CORE_SQL,
        pass_value=20000,
    ).expand(database=["dev", "production"])

    classic_snowflake_task = SnowflakeOperator(
        task_id="classic_snowflake_task",
        snowflake_conn_id="snowflake",
        sql=SNOWFLAKE_SQL,
        parameters={"tbl": "foo"},
    )

    mapped_snowflake_task = SnowflakeOperator.partial(
        task_id="mapped_snowflake_task", snowflake_conn_id="snowflake", sql=SNOWFLAKE_SQL
    ).expand(
        parameters=[
            {"tbl": "foo"},
            {"tbl": "bar"},
        ]
    )


_ = map_city()
```

**`SQLValueCheckOperator` tasks**
The logs for the "classic_sql_value_check", non-mapped task show the query executing as expected:
 `[2022-06-11, 02:01:03 UTC] {sql.py:204} INFO - Executing SQL check: SELECT COUNT(*) FROM dev.tbl;`
while the mapped "check_row_count" task fails with the following exception:
```bash
[2022-06-11, 02:01:03 UTC] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'map-city', 'check_row_count', 'manual__2022-06-11T02:01:01.831761+00:00', '--job-id', '350', '--raw', '--subdir', 'DAGS_FOLDER/map_city.py', '--cfg-path', '/tmp/tmpm5bg9mt5', '--map-index', '0', '--error-file', '/tmp/tmp2kbilt2l']
[2022-06-11, 02:01:03 UTC] {standard_task_runner.py:80} INFO - Job 350: Subtask check_row_count
[2022-06-11, 02:01:03 UTC] {task_command.py:370} INFO - Running <TaskInstance: map-city.check_row_count manual__2022-06-11T02:01:01.831761+00:00 map_index=0 [running]> on host 569596df5be5
[2022-06-11, 02:01:03 UTC] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1451, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1555, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 2212, in render_templates
    rendered_task = self.task.render_template_fields(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 726, in render_template_fields
    self._do_render_template_fields(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/abstractoperator.py", line 344, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/usr/local/lib/python3.9/site-packages/airflow/models/abstractoperator.py", line 391, in render_template
    return render_template_to_string(template, context)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/helpers.py", line 296, in render_template_to_string
    return render_template(template, context, native=False)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/helpers.py", line 291, in render_template
    return "".join(nodes)
  File "<template>", line 13, in root
  File "/usr/local/lib/python3.9/site-packages/jinja2/runtime.py", line 903, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'airflow.models.mappedoperator.MappedOperator object' has no attribute 'database'
```

**`SnowflakeOperator` tasks**
Similarly, the "classic_snowflake_task" non-mapped task is able to execute the SQL query as expected:
`[2022-06-11, 02:01:04 UTC] {snowflake.py:324} INFO - Running statement: SELECT * FROM foo;, parameters: {'tbl': 'foo'}`
while the mapped "mapped_snowflake_task task fails to execute the query:
```bash
[2022-06-11, 02:01:03 UTC] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'map-city', 'mapped_snowflake_task', 'manual__2022-06-11T02:01:01.831761+00:00', '--job-id', '347', '--raw', '--subdir', 'DAGS_FOLDER/map_city.py', '--cfg-path', '/tmp/tmp6kmqs5ew', '--map-index', '0', '--error-file', '/tmp/tmpkufg9xqx']
[2022-06-11, 02:01:03 UTC] {standard_task_runner.py:80} INFO - Job 347: Subtask mapped_snowflake_task
[2022-06-11, 02:01:03 UTC] {task_command.py:370} INFO - Running <TaskInstance: map-city.mapped_snowflake_task manual__2022-06-11T02:01:01.831761+00:00 map_index=0 [running]> on host 569596df5be5
[2022-06-11, 02:01:03 UTC] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1451, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1555, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 2212, in render_templates
    rendered_task = self.task.render_template_fields(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 726, in render_template_fields
    self._do_render_template_fields(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/abstractoperator.py", line 344, in _do_render_template_fields
    rendered_content = self.render_template(
  File "/usr/local/lib/python3.9/site-packages/airflow/models/abstractoperator.py", line 391, in render_template
    return render_template_to_string(template, context)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/helpers.py", line 296, in render_template_to_string
    return render_template(template, context, native=False)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/helpers.py", line 291, in render_template
    return "".join(nodes)
  File "<template>", line 13, in root
  File "/usr/local/lib/python3.9/site-packages/jinja2/sandbox.py", line 326, in getattr
    value = getattr(obj, attribute)
  File "/usr/local/lib/python3.9/site-packages/jinja2/runtime.py", line 910, in __getattr__
    return self._fail_with_undefined_error()
  File "/usr/local/lib/python3.9/site-packages/jinja2/runtime.py", line 903, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'airflow.models.mappedoperator.MappedOperator object' has no attribute 'parameters'
```

### Operating System

Debian GNU/Linux 10 (buster)

### Versions of Apache Airflow Providers

apache-airflow-providers-snowflake==2.7.0

### Deployment

Astronomer

### Deployment details

Astronomer Runtime 5.0.3

### Anything else

Even though using the `{{ task.<operator attr> }}` method does not work for mapped tasks, there is a workaround. Given the `SnowflakeOperator` example from above attempting to execute the query: `SELECT * FROM {{ task.parameters.tbl }};`, users can modify the templated query to `SELECT * FROM {{ task.mapped_kwargs.parameters[ti.map_index].tbl }};` for successful execution. This workaround isn't very obvious though and requires from solid digging into the new 2.3.0 code.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/models/abstractoperator.py', 'a/airflow/models/baseoperator.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/utils/context.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Fix custom waiter function in AWS provider package
### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

2.5.0

### Operating System

MacOS

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

Discussed in #28294

### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/lambda_function.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/waiter.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py']
Ground Truth : ['a/airflow/providers/amazon/aws/utils/waiter.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Test that dataset not updated when task skipped
the AIP specifies that when a task is skipped, that we dont mark the dataset as updated. we should simply add a test that verifies that this is what happens (and make changes if necessary) 

@blag, i tried to make this an issue so i could assign to you but can't. anyway, can reference in PR with `closes`

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py']
Ground Truth : ['a/airflow/example_dags/example_datasets.py', 'a/airflow/models/taskinstance.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: WTFroms new release 2.3.0 breaks airflow 1.10.10
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 1.10.10


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
NAME="Ubuntu"
VERSION="16.04.3 LTS (Xenial Xerus)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 16.04.3 LTS"
VERSION_ID="16.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
- **Kernel** (e.g. `uname -a`): ubuntu 4.15.0-96-generic #97~16.04.1-Ubuntu SMP Wed Apr 1 03:03:31 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux **Install tools**: pip 
- **Others**:

**What happened**:
airflow initdb 
/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/airflow/configuration.py:652: DeprecationWarning: You have two airflow.cfg files: /home/sgu/airflow/airflow.cfg and /home/sgu/tmp20200221/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/sgu/tmp20200221/airflow.cfg, and you should remove the other file
  category=DeprecationWarning,
Traceback (most recent call last):
  File "/home/sgu/miniconda3/envs/tmp/bin/airflow", line 26, in <module>
    from airflow.bin.cli import CLIFactory
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/airflow/bin/cli.py", line 71, in <module>
    from airflow.www_rbac.app import cached_app as cached_app_rbac
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/airflow/www_rbac/app.py", line 28, in <module>
    from flask_appbuilder import AppBuilder, SQLA
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/flask_appbuilder/__init__.py", line 6, in <module>
    from .base import AppBuilder  # noqa: F401
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/flask_appbuilder/base.py", line 8, in <module>
    from .api.manager import OpenApiManager
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/flask_appbuilder/api/manager.py", line 7, in <module>
    from flask_appbuilder.baseviews import BaseView
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/flask_appbuilder/baseviews.py", line 21, in <module>
    from .forms import GeneralModelConverter
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/flask_appbuilder/forms.py", line 17, in <module>
    from .fieldwidgets import (
  File "/home/sgu/miniconda3/envs/tmp/lib/python3.6/site-packages/flask_appbuilder/fieldwidgets.py", line 3, in <module>
    from wtforms.widgets import html_params, HTMLString
ImportError: cannot import name 'HTMLString'

<!-- (please include exact error messages if you can) -->

**What you expected to happen**:

<!-- What do you think went wrong? -->

**How to reproduce it**:
pip install apache-airflow
airflow initdb 
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md sytle of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:
wtforms just released 2.3.0 which breaks flask which  breaks airflow. 
https://wtforms.readthedocs.io/en/2.3.x/changes/#version-2-3-0
<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Mark Success on a mapped task, reruns other failing mapped tasks
### Apache Airflow version

2.3.0b1 (pre-release)

### What happened

Have a DAG with mapped tasks. Mark at least two mapped tasks as failed. Mark one of the failures as success. See the other task(s) switch to `no_status` and rerun.

![Apr-22-2022 10-21-41](https://user-images.githubusercontent.com/4600967/164734320-bafe267d-6ef0-46fb-b13f-6d85f9ef86ba.gif)



### What you think should happen instead

Marking a single mapped task as a success probably shouldn't affect other failed mapped tasks.

### How to reproduce

_No response_

### Operating System

OSX

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/api/common/mark_tasks.py', 'a/airflow/models/dag.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Remove adding Operators via plugins in 2.0
In Airflow 2.0 we should remove the ability to add operators and hooks via plugins. 

I think we should deprecate adding Operators and Hooks via the Airflow plugin mechanism.

I think plugins should be reserved for any mechanism that a plain-ol python module import won't work for (which is basically anything that needs to tie deeply in to the Webserver or Scheduler process).

To that end I think we should deprecate adding operators via plugins:

```python
    from airflow.operators.my_plugin import MyOperator
```

can become

```python
    from my_plugin import MyOperator
```

with no impact on functionality.

For this not to be a hard/sudden breaking change we should issue a deprecation warning for this in 1.10.12 - https://github.com/apache/airflow/issues/9500


Discussed here:

https://lists.apache.org/thread.html/a1453d6a6f113709386b61c68c3f5cd61b258fe78f07811169500fe3%40%3Cdev.airflow.apache.org%3E


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/mock_operators.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py']
Ground Truth : ['a/airflow/cli/commands/plugins_command.py', 'a/airflow/www/views.py', 'a/airflow/plugins_manager.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: [BUG] 2.4.0b1 - google-provider - AttributeError: 'str' object has no attribute 'version'
### Apache Airflow version

main (development)

### What happened

when I start airflow 2.4.0b1 with the `apache-airflow-providers-google==8.3.0`

the webserver log give : 

```log
[2022-09-08 14:39:53,158] {webserver_command.py:251} ERROR - [0 / 0] Some workers seem to have died and gunicorn did not restart them as expected
[2022-09-08 14:39:53,275] {providers_manager.py:228} WARNING - Exception when importing 'airflow.providers.google.common.hooks.base_google.GoogleBaseHook' from 'apache-airflow-providers-google' package
2022-09-08T14:39:53.276959961Z Traceback (most recent call last):
2022-09-08T14:39:53.276965441Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers_manager.py", line 260, in _sanity_check
2022-09-08T14:39:53.276969533Z     imported_class = import_string(class_name)
2022-09-08T14:39:53.276973476Z   File "/usr/local/lib/python3.8/site-packages/airflow/utils/module_loading.py", line 32, in import_string
2022-09-08T14:39:53.276977496Z     module = import_module(module_path)
2022-09-08T14:39:53.276981203Z   File "/usr/local/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-09-08T14:39:53.276985012Z     return _bootstrap._gcd_import(name[level:], package, level)
2022-09-08T14:39:53.277005418Z   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-09-08T14:39:53.277011581Z   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-09-08T14:39:53.277016414Z   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-09-08T14:39:53.277020883Z   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-09-08T14:39:53.277025840Z   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-09-08T14:39:53.277029603Z   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-09-08T14:39:53.277032868Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 49, in <module>
2022-09-08T14:39:53.277036076Z     from airflow.providers.google.cloud.utils.credentials_provider import (
2022-09-08T14:39:53.277038762Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 36, in <module>
2022-09-08T14:39:53.277041651Z     from airflow.providers.google.cloud._internal_client.secret_manager_client import _SecretManagerClient
2022-09-08T14:39:53.277044383Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/cloud/_internal_client/secret_manager_client.py", line 26, in <module>
2022-09-08T14:39:53.277047248Z     from airflow.providers.google.common.consts import CLIENT_INFO
2022-09-08T14:39:53.277050101Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/common/consts.py", line 23, in <module>
2022-09-08T14:39:53.277052974Z     CLIENT_INFO = ClientInfo(client_library_version='airflow_v' + version.version)
2022-09-08T14:39:53.277055720Z AttributeError: 'str' object has no attribute 'version'
[2022-09-08 14:39:53,299] {providers_manager.py:228} WARNING - Exception when importing 'airflow.providers.google.cloud.hooks.cloud_sql.CloudSQLHook' from 'apache-airflow-providers-google' package
2022-09-08T14:39:53.300816697Z Traceback (most recent call last):
2022-09-08T14:39:53.300822358Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers_manager.py", line 260, in _sanity_check
2022-09-08T14:39:53.300827098Z     imported_class = import_string(class_name)
2022-09-08T14:39:53.300831757Z   File "/usr/local/lib/python3.8/site-packages/airflow/utils/module_loading.py", line 32, in import_string
2022-09-08T14:39:53.300836033Z     module = import_module(module_path)
2022-09-08T14:39:53.300840058Z   File "/usr/local/lib/python3.8/importlib/__init__.py", line 127, in import_module
2022-09-08T14:39:53.300844580Z     return _bootstrap._gcd_import(name[level:], package, level)
2022-09-08T14:39:53.300862499Z   File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
2022-09-08T14:39:53.300867522Z   File "<frozen importlib._bootstrap>", line 991, in _find_and_load
2022-09-08T14:39:53.300871975Z   File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
2022-09-08T14:39:53.300876819Z   File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
2022-09-08T14:39:53.300880682Z   File "<frozen importlib._bootstrap_external>", line 843, in exec_module
2022-09-08T14:39:53.300885112Z   File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
2022-09-08T14:39:53.300889697Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/cloud_sql.py", line 51, in <module>
2022-09-08T14:39:53.300893842Z     from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
2022-09-08T14:39:53.300898141Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/common/hooks/base_google.py", line 49, in <module>
2022-09-08T14:39:53.300903254Z     from airflow.providers.google.cloud.utils.credentials_provider import (
2022-09-08T14:39:53.300906904Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/cloud/utils/credentials_provider.py", line 36, in <module>
2022-09-08T14:39:53.300911707Z     from airflow.providers.google.cloud._internal_client.secret_manager_client import _SecretManagerClient
2022-09-08T14:39:53.300916818Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/cloud/_internal_client/secret_manager_client.py", line 26, in <module>
2022-09-08T14:39:53.300920595Z     from airflow.providers.google.common.consts import CLIENT_INFO
2022-09-08T14:39:53.300926003Z   File "/usr/local/lib/python3.8/site-packages/airflow/providers/google/common/consts.py", line 23, in <module>
2022-09-08T14:39:53.300931078Z     CLIENT_INFO = ClientInfo(client_library_version='airflow_v' + version.version)
2022-09-08T14:39:53.300934596Z AttributeError: 'str' object has no attribute 'version'
....
```

### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Operating System

ubuntu 22.04.1

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/utils/test_pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/flink/sensors/test_flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0120_2_5_0_add_updated_at_to_dagrun_and_ti.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0110_2_3_2_add_cascade_to_dag_tag_foreignkey.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0019_1_7_1_add_fractional_seconds_to_mysql_tables.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_rendered.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py']
Ground Truth : ['a/airflow/__init__.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: TaskFlow API `multiple_outputs` inferral causes import errors when using TYPE_CHECKING
### Apache Airflow version

2.5.1

### What happened

When using the TaskFlow API, I like to generally keep a good practice of adding type annotations in the TaskFlow functions so others reading the DAG and task code have better context around inputs/outputs, keep imports solely used for typing behind `typing.TYPE_CHECKING`, and utilize PEP 563 for forwarding annotation evaluations. Unfortunately, when using ~PEP 563 _and_ `TYPE_CHECKING`~ just TYPE_CHECKING, DAG import errors occur with a "NameError: <name> is not defined." exception.

### What you think should happen instead

Users should be free to use ~PEP 563 and~ `TYPE_CHECKING` when using the TaskFlow API and not hit DAG import errors along the way.

### How to reproduce

Using a straightforward use case of transforming a DataFrame, let's assume this toy example:

```py
from __future__ import annotations

from typing import TYPE_CHECKING, Any

from pendulum import datetime

from airflow.decorators import dag, task

if TYPE_CHECKING:
    from pandas import DataFrame


@dag(start_date=datetime(2023, 1, 1), schedule=None)
def multiple_outputs():
    @task()
    def transform(df: DataFrame) -> dict[str, Any]:
        ...

    transform()

multiple_outputs()
```
Add this DAG to your DAGS_FOLDER and the following import error should be observed:
<img width="641" alt="image" src="https://user-images.githubusercontent.com/48934154/217713685-ec29d5cc-4a48-4049-8dfa-56cbd76cddc3.png">


### Operating System

Debian GNU/Linux

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==6.2.0
apache-airflow-providers-apache-hive==5.1.1
apache-airflow-providers-apache-livy==3.2.0
apache-airflow-providers-celery==3.1.0
apache-airflow-providers-cncf-kubernetes==5.1.1
apache-airflow-providers-common-sql==1.3.3
apache-airflow-providers-databricks==4.0.0
apache-airflow-providers-dbt-cloud==2.3.1
apache-airflow-providers-elasticsearch==4.3.3
apache-airflow-providers-ftp==3.3.0
apache-airflow-providers-google==8.8.0
apache-airflow-providers-http==4.1.1
apache-airflow-providers-imap==3.1.1
apache-airflow-providers-microsoft-azure==5.1.0
apache-airflow-providers-postgres==5.4.0
apache-airflow-providers-redis==3.1.0
apache-airflow-providers-sftp==4.2.1
apache-airflow-providers-snowflake==4.0.2
apache-airflow-providers-sqlite==3.3.1
apache-airflow-providers-ssh==3.4.0
astronomer-providers==1.14.0

### Deployment

Astronomer

### Deployment details

OOTB local Airflow install with LocalExecutor built with the Astro CLI.

### Anything else

- This behavior/error was not observed using Airflow 2.4.3. 
- As a workaround, `multiple_outputs` can be explicitly set on the TaskFlow function to skip the inferral.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py']
Ground Truth : ['a/airflow/decorators/base.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: XComs from another task group fail to populate dynamic task mapping metadata
### Apache Airflow version

2.3.3

### What happened

When a task returns a mappable Xcom within a task group, the dynamic task mapping feature (via `.expand`) causes the Airflow Scheduler to infinitely loop with a runtime error:

```
Traceback (most recent call last):

  File "/home/airflow/.local/bin/airflow", line 8, in <module>

    sys.exit(main())

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 38, in main

    args.func(args)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 51, in command

    return func(*args, **kwargs)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 99, in wrapper

    return f(*args, **kwargs)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 75, in scheduler

    _run_scheduler_job(args=args)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 46, in _run_scheduler_job

    job.run()

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/base_job.py", line 244, in run

    self._execute()

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 751, in _execute

    self._run_scheduler_loop()

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 839, in _run_scheduler_loop

    num_queued_tis = self._do_scheduling(session)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 921, in _do_scheduling

    callback_to_run = self._schedule_dag_run(dag_run, session)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1163, in _schedule_dag_run

    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 524, in update_state

    info = self.task_instance_scheduling_decisions(session)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper

    return func(*args, **kwargs)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 654, in task_instance_scheduling_decisions

    schedulable_tis, changed_tis, expansion_happened = self._get_ready_tis(

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagrun.py", line 710, in _get_ready_tis

    expanded_tis, _ = schedulable.task.expand_mapped_task(self.run_id, session=session)

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 614, in expand_mapped_task

    operator.mul, self._resolve_map_lengths(run_id, session=session).values()

  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 600, in _resolve_map_lengths

    raise RuntimeError(f"Failed to populate all mapping metadata; missing: {keys}")

RuntimeError: Failed to populate all mapping metadata; missing: 'x'
```

### What you think should happen instead

Xcoms from different task groups should be mappable within other group scopes.

### How to reproduce

```
from airflow import DAG
from airflow.decorators import task
from airflow.utils.task_group import TaskGroup

import pendulum

@task
def enumerate(x):
    return [i for i in range(x)]

@task 
def addOne(x):
    return x+1

with DAG(
    dag_id="TaskGroupMappingBug",
    schedule_interval=None,
    start_date=pendulum.now().subtract(days=1),
) as dag:
    
    with TaskGroup(group_id="enumerateNine"):
        y = enumerate(9)
        
    with TaskGroup(group_id="add"):
        # airflow scheduler throws error here so this is never reached
        z = addOne.expand(x=y)
    

```

### Operating System

linux/amd64 via Docker (apache/airflow:2.3.3-python3.9)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

docker-compose version 1.29.2, build 5becea4c

Docker Engine v20.10.14


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py']
Ground Truth : ['a/airflow/models/taskmixin.py', 'a/airflow/models/abstractoperator.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Ability to provide sample conf JSON for a dag in trigger page
**Description**
In the trigger page, there is a text area to enter (optional) conf json. It would be great if a sample JSON can be provided programatically while defining DAG

**Use case / motivation**

This will improve usability of the UI that triggers a DAG


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_trigger_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/operators/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/client/json_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/pinecone/example_pinecone_openai.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/trigger_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/log_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/databricks/operators/test_databricks.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Mapped KubernetesPodOperator "fails" but UI shows it is as still running
### Apache Airflow version

2.3.0b1 (pre-release)

### What happened

This dag has a problem.  The `name` kwarg is missing from one of the mapped instances.

```python3
from datetime import datetime
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (
    KubernetesPodOperator,
)

from airflow.configuration import conf

namespace = conf.get("kubernetes", "NAMESPACE")

with DAG(
    dag_id="kpo_mapped",
    start_date=datetime(1970, 1, 1),
    schedule_interval=None,
) as dag:

    KubernetesPodOperator(
        task_id="cowsay_static_named",
        name="cowsay_statc",
        namespace=namespace,
        image="docker.io/rancher/cowsay",
        cmds=["cowsay"],
        arguments=["moo"],
    )

    KubernetesPodOperator.partial(
        task_id="cowsay_mapped",
        # name="cowsay_mapped",  # required field missing
        image="docker.io/rancher/cowsay",
        namespace=namespace,
        cmds=["cowsay"],
    ).expand(arguments=[["mooooove"], ["cow"], ["get out the way"]])

    KubernetesPodOperator.partial(
        task_id="cowsay_mapped_named",
        name="cowsay_mapped",
        namespace=namespace,
        image="docker.io/rancher/cowsay",
        cmds=["cowsay"],
    ).expand(arguments=[["mooooove"], ["cow"], ["get out the way"]])

```

If you omit that field in an unmapped task, you get a dag parse error, which is appropriate.  But omitting it from the mapped task gives you this runtime error in the task logs:

```
[2022-04-20, 05:11:02 UTC] {standard_task_runner.py:52} INFO - Started process 60 to run task
[2022-04-20, 05:11:02 UTC] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'kpo_mapped', 'cowsay_mapped', 'manual__2022-04-20T05:11:01+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/dags/taskmap/kpo_mapped.py', '--cfg-path', '/tmp/tmp_g3sj496', '--map-index', '0', '--error-file', '/tmp/tmp2_313wxj']
[2022-04-20, 05:11:02 UTC] {standard_task_runner.py:80} INFO - Job 12: Subtask cowsay_mapped
[2022-04-20, 05:11:02 UTC] {task_command.py:369} INFO - Running <TaskInstance: kpo_mapped.cowsay_mapped manual__2022-04-20T05:11:01+00:00 map_index=0 [running]> on host airflow-worker-65f9fd9d5b-vpgnk
[2022-04-20, 05:11:02 UTC] {taskinstance.py:1863} WARNING - We expected to get frame set in local storage but it was not. Please report this as an issue with full logs at https://github.com/apache/airflow/issues/new
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1440, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1544, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 2210, in render_templates
    rendered_task = self.task.render_template_fields(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 722, in render_template_fields
    unmapped_task = self.unmap(unmap_kwargs=kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 508, in unmap
    op = self.operator_class(**unmap_kwargs, _airflow_from_mapped=True)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 390, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py", line 259, in __init__
    self.name = self._set
_name(name)
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py", line 442, in _set_name
    raise AirflowException("`name` is required unless `pod_template_file` or `full_pod_spec` is set")
airflow.exceptions.AirflowException: `name` is required unless `pod_template_file` or `full_pod_spec` is set
```

But rather than failing the task, Airflow just thinks that the task is still running:

<img width="833" alt="Screen Shot 2022-04-19 at 11 13 47 PM" src="https://user-images.githubusercontent.com/5834582/164156155-41986d3a-d171-4943-8443-a0fc3c542988.png">



### What you think should happen instead

Ideally this error would be surfaced when the dag is first parsed.  If that's not possible, then it should fail the task completely (i.e. a red square should show up in the grid view).

### How to reproduce

Run the dag above

### Operating System

ubuntu (microk8s)

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes  | 4.0.0

### Deployment

Astronomer

### Deployment details

Deployed via the astronomer airflow helm chart, values:

```
airflow:
  airflowHome: /usr/local/airflow
  defaultAirflowRepository: 172.28.11.191:30500/airflow
  defaultAirflowTag: tb11c-inner-operator-expansion
  env:
  - name: AIRFLOW__CORE__DAGBAG_IMPORT_ERROR_TRACEBACK_DEPTH
    value: '99'
  executor: CeleryExecutor
  gid: 50000
  images:
    airflow:
      pullPolicy: Always
      repository: 172.28.11.191:30500/airflow
    flower:
      pullPolicy: Always
    pod_template:
      pullPolicy: Always
  logs:
    persistence:
      enabled: true
      size: 2Gi
  scheduler:
    livenessProbe:
      timeoutSeconds: 45
  triggerer:
    livenessProbe:
      timeoutSeconds: 45
```

Image base: `quay.io/astronomer/ap-airflow-dev:main`
Airflow version: `2.3.0.dev20220414`



### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/sensors/test_spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py']
Ground Truth : ['a/airflow/dag_processing/processor.py', 'a/airflow/models/taskfail.py', 'a/airflow/models/taskinstance.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: Airflow 2.3 scheduler error: 'V1Container' object has no attribute '_startup_probe'
### Apache Airflow version

2.3.0 (latest released)

### What happened

After migrating from Airflow 2.2.4 to 2.3.0 scheduler fell into crash loop throwing:

```
--- Logging error ---
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 736, in _execute
    self._run_scheduler_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 826, in _run_scheduler_loop
    self.executor.heartbeat()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/executors/base_executor.py", line 171, in heartbeat
    self.sync()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/executors/kubernetes_executor.py", line 613, in sync
    self.kube_scheduler.run_next(task)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/executors/kubernetes_executor.py", line 300, in run_next
    self.log.info('Kubernetes job is %s', str(next_job).replace("\n", " "))
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod.py", line 214, in __repr__
    return self.to_str()
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod.py", line 210, in to_str
    return pprint.pformat(self.to_dict())
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod.py", line 196, in to_dict
    result[attr] = value.to_dict()
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod_spec.py", line 1070, in to_dict
    result[attr] = list(map(
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod_spec.py", line 1071, in <lambda>
    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_container.py", line 672, in to_dict
    value = getattr(self, attr)
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_container.py", line 464, in startup_probe
    return self._startup_probe
AttributeError: 'V1Container' object has no attribute '_startup_probe'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/logging/__init__.py", line 1083, in emit
    msg = self.format(record)
  File "/usr/local/lib/python3.9/logging/__init__.py", line 927, in format
    return fmt.format(record)
  File "/usr/local/lib/python3.9/logging/__init__.py", line 663, in format
    record.message = record.getMessage()
  File "/usr/local/lib/python3.9/logging/__init__.py", line 367, in getMessage
    msg = msg % self.args
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod.py", line 214, in __repr__
    return self.to_str()
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod.py", line 210, in to_str
    return pprint.pformat(self.to_dict())
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod.py", line 196, in to_dict
    result[attr] = value.to_dict()
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod_spec.py", line 1070, in to_dict
    result[attr] = list(map(
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_pod_spec.py", line 1071, in <lambda>
    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_container.py", line 672, in to_dict
    value = getattr(self, attr)
  File "/home/airflow/.local/lib/python3.9/site-packages/kubernetes/client/models/v1_container.py", line 464, in startup_probe
    return self._startup_probe
AttributeError: 'V1Container' object has no attribute '_startup_probe'
Call stack:
  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 38, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 51, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 75, in scheduler
    _run_scheduler_job(args=args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 46, in _run_scheduler_job
    job.run()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/base_job.py", line 244, in run
    self._execute()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 757, in _execute
    self.executor.end()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/executors/kubernetes_executor.py", line 809, in end
    self._flush_task_queue()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/executors/kubernetes_executor.py", line 767, in _flush_task_queue
    self.log.warning('Executor shutting down, will NOT run task=%s', task)
Unable to print the message and arguments - possible formatting error.
Use the traceback above to help find the error.
```
kubernetes python library version was exactly as specified in constraints file: https://raw.githubusercontent.com/apache/airflow/constraints-2.3.0/constraints-3.9.txt

### What you think should happen instead

Scheduler should work

### How to reproduce

Not 100% sure but:
1. Run Airflow 2.2.4 using official Helm Chart
2. Run some dags to have some records in DB
3. Migrate to 2.3.0 (replace 2.2.4 image with 2.3.0 one)

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

irrelevant

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

KubernetesExecutor
PostgreSQL (RDS) as Airflow DB
Python 3.9
Docker images build from `apache/airflow:2.3.0-python3.9` (some additional libraries installed)

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py']
Ground Truth : ['a/airflow/exceptions.py', 'a/airflow/executors/kubernetes_executor.py', 'a/airflow/models/taskinstance.py', 'a/airflow/kubernetes/pod_generator.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: UI doesn't handle whitespace/empty dataset URI's well
### Apache Airflow version

main (development)

### What happened

Here are some poor choices for dataset URI's:

```python3
empty = Dataset("")
colons = Dataset("::::::")
whitespace = Dataset("\t\n")
emoji = Dataset("")
long = Dataset(5000 * "x")
injection = Dataset("105'; DROP TABLE 'dag")
```

And a dag file which replicates the problems mentioned below: https://gist.github.com/MatrixManAtYrService/a32bba5d382cd9a925da72571772b060 (full tracebacks included as comments)

Here's how they did:
|dataset|behavior|
|:-:|:--|
|empty| dag triggered with no trouble, not selectable in the datasets UI|
|emoji| `airflow dags reserialize`: `UnicodeEncodeError: 'ascii' codec can't encode character '\U0001f60a' in position 0: ordinal not in range(128)`|
|colons| no trouble|
|whitespace| dag triggered with no trouble, selectable in the datasets UI, but shows no history|
|long|sqlalchemy error during serialization|
|injection| no trouble|

Finally, here's a screenshot:

<img width="1431" alt="Screen Shot 2022-09-13 at 11 29 02 PM" src="https://user-images.githubusercontent.com/5834582/190069341-dc17c66a-f941-424d-a455-cd531580543a.png">

Notice that there are two empty rows in the datasets list, one for `empty`, the other for `whitespace`.  Only `whitespace` is selectable, both look weird.

### What you think should happen instead

I propose that we add a uri sanity check during serialization and just reject dataset URI's that are:
- only whitespace
- empty
- long enough that they're going to cause a database problem

The `emoji` case failed in a nice way.  Ideally `whitespace`, `long` and `empty` can fail in the same way.  If implemented, this would prevent any of the weird cases above from making it to the UI in the first place.


### How to reproduce

Unpause the above dags

### Operating System

Docker/debian

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

`astro dev start`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/datasets/test_dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/datasets/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py']
Ground Truth : ['a/airflow/datasets/__init__.py']
Current Recall: 0.04639798103395534

=========================================================

ISSUE: KubernetesPodOperator/KubernetesExecutor: Failed to adopt pod 422
### Apache Airflow version

2.3.0

### What happened

Here i provide steps to reproduce this.

Goal of this: to describe how to reproduce the "Failed to Adopt pod" error condition.

The DAG->step Described Below should be of type KubernetesPodOperator

NOTE: under normal operation,
(where the MAIN_AIRFLOW_POD is never recycled by k8s, we will never see this edge-case)
(it is only when the workerPod is still running, but the MAIN_AIRFLOW_POD is suddenly restarted/stopped)
(that we would see orphan->workerPods)

1] Implement a contrived-DAG, with a single step -> which is long-running (e.g. 6 minutes)
2] Deploy your airflow-2.1.4 / airfow-2.3.0 together with the contrived-DAG
3] Run your contrived-DAG.
4] in the middle of running the single-step, check via "kubectl" that your Kubernetes->workerPod has been created / running
5] while workerPod still running, do "kubectl delete pod <OF_MAIN_AIRFLOW_POD>". This will mean that the workerPod becomes an orphan.
6] the workerPod still continues to run through to completion. after which the K8S->status of the pod will be Completed, however the pod doesn't shut down itself.
7] "kubectl" start up a new <MAIN_AIRFLOW_POD> so the web-ui is running again.
8] MAIN_AIRFLOW_POD->webUi - Run your contrived-DAG again
9] while the contrived-DAG is starting/tryingToStart etc, you will see in the logs printed out "Failed to adopt pod" -> with 422 error code.

The step-9 with the error message, you will find two appearances of this error msg in the airflow-2.1.4, airflow-2.3.0 source-code.
The step-7 may also - general logging from the MAIN_APP - may also output the "Failed to adopt pod" error message also.



### What you think should happen instead

On previous versions of airflow e.g. 1.10.x, the orphan-workerPods would be adopted by the 2nd run-time of the airflowMainApp and either used to continue the same DAG and/or cleared away when complete.

This is not happening with the newer airflow 2.1.4 / 2.3.0 (presumably because the code changed), and upon the 2nd run-time of the airflowMainApp - it would seem to try to adopt-workerPod but fails at that point ("Failed to adopt pod" in the logs and hence it cannot clear away orphan pods).

Given this is an edge-case only, (i.e. we would not expect k8s to be recycling the main airflowApp/pod anyway), it doesn't seem totally urgent bug. However, the only reason for me raising this issue with yourselves is that given any k8s->namespace, in particular in PROD,   over time (e.g. 1 month?) the namespace will slowly be being filled up with orphanPods and somebody would need to manually log-in to delete old pods.

### How to reproduce

Here i provide steps to reproduce this.

Goal of this: to describe how to reproduce the "Failed to Adopt pod" error condition.

The DAG->step Described Below should be of type KubernetesPodOperator

NOTE: under normal operation,
(where the MAIN_AIRFLOW_POD is never recycled by k8s, we will never see this edge-case)
(it is only when the workerPod is still running, but the MAIN_AIRFLOW_POD is suddenly restarted/stopped)
(that we would see orphan->workerPods)

1] Implement a contrived-DAG, with a single step -> which is long-running (e.g. 6 minutes)
2] Deploy your airflow-2.1.4 / airfow-2.3.0 together with the contrived-DAG
3] Run your contrived-DAG.
4] in the middle of running the single-step, check via "kubectl" that your Kubernetes->workerPod has been created / running
5] while workerPod still running, do "kubectl delete pod <OF_MAIN_AIRFLOW_POD>". This will mean that the workerPod becomes an orphan.
6] the workerPod still continues to run through to completion. after which the K8S->status of the pod will be Completed, however the pod doesn't shut down itself.
7] "kubectl" start up a new <MAIN_AIRFLOW_POD> so the web-ui is running again.
8] MAIN_AIRFLOW_POD->webUi - Run your contrived-DAG again
9] while the contrived-DAG is starting/tryingToStart etc, you will see in the logs printed out "Failed to adopt pod" -> with 422 error code.

The step-9 with the error message, you will find two appearances of this error msg in the airflow-2.1.4, airflow-2.3.0 source-code.
The step-7 may also - general logging from the MAIN_APP - may also output the "Failed to adopt pod" error message also.



### Operating System

kubernetes

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

nothing special.

it (CI/CD pipeline) builds the app. using requirements.txt to pull-in all the required python dependencies (including there is a dependency for the airflow-2.1.4 / 2.3.0)

it (CI/CD pipeline) packages the app as an ECR image & then deploy directly to k8s namespace.

### Anything else

this is 100% reproducible each & every time.
i have tested this multiple times.

also - i tested this on the old airflow-1.10.x a couple of times to verify that the bug did not exist previously

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_base.py']
Ground Truth : ['a/airflow/executors/kubernetes_executor.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: Applying labels to task groups shows a cycle in the graph view for the dag
### Apache Airflow version

2.2.2

### Operating System

Docker (debian:buster)

### Versions of Apache Airflow Providers

N/A

### Deployment

Astronomer

### Deployment details

run airflow with this dag

```python3
with DAG(
    dag_id="label_bug_without_chain"
) as dag:

    with TaskGroup(group_id="group1") as taskgroup1:
        t1 = DummyOperator(task_id="dummy1")
        t2 = DummyOperator(task_id="dummy2")
        t3 = DummyOperator(task_id="dummy3")
    
    t4 = DummyOperator(task_id="dummy4")

chain([Label("branch three")], taskgroup1, t4,)
```


### What happened

expanded task views look like they have cycles

<img width="896" alt="Screen Shot 2021-11-22 at 2 33 49 PM" src="https://user-images.githubusercontent.com/17841735/143083099-d250fd7e-963f-4b34-b544-405b51ee2859.png">


### What you expected to happen

The task group shouldn't display as if it has loops in it.

### How to reproduce

View the dag shown in the deployment details.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/apache/kafka/example_dag_hello_kafka.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/dags/elastic_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py']
Ground Truth : ['a/airflow/utils/task_group.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: [AIRFLOW-6361] Run LocalTaskJob directly in Celery task
Hello,

The executor runs multiple processes to perform one task. Many processes have a very short life cycle, so the process of starting it is a significant overhead.

Firstly, the Celery executor trigger Celery tasks - app.task. This task runs the CLI  command (first process), which contains LocalTaskJob. LocalTaskJob runs the separate command (second process) that executes user-code. This level of isolation is redundant because LocalTaskJob doesn't execute unsafe code. The first command is run by a new process creation, not by a fork, so this is an expensive operation. I suggest running code from the first process as part of the celery task to reduce the need to create new processes.

The code currently uses CLIFactory to run the LocalTaskJob It is better to do this without unnecessary dependence on CLI, but it is a big change and I plan to do it in a separate PR.
WIP PR: https://github.com/mik-laj/incubator-airflow/pull/10 (Travis green :-D )

Performance benchmark: 
===================
Example DAG from Airflow with unneeded sleep instructions deleted.
```python
"""Example DAG demonstrating the usage of the BashOperator."""

from datetime import timedelta

import airflow
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.dummy_operator import DummyOperator

args = {
    'owner': 'airflow',
    'start_date': airflow.utils.dates.days_ago(2),
}

dag = DAG(
    dag_id='example_bash_operator',
    default_args=args,
    schedule_interval='0 0 * * *',
    dagrun_timeout=timedelta(minutes=60),
)

run_this_last = DummyOperator(
    task_id='run_this_last',
    dag=dag,
)

# [START howto_operator_bash]
run_this = BashOperator(
    task_id='run_after_loop',
    bash_command='echo 1',
    dag=dag,
)
# [END howto_operator_bash]

run_this >> run_this_last

for i in range(3):
    task = BashOperator(
        task_id='runme_' + str(i),
        bash_command='echo "{{ task_instance_key_str }}",
        dag=dag,
    )
    task >> run_this

# [START howto_operator_bash_template]
also_run_this = BashOperator(
    task_id='also_run_this',
    bash_command='echo "run_id={{ run_id }} | dag_run={{ dag_run }}"',
    dag=dag,
)
# [END howto_operator_bash_template]
also_run_this >> run_this_last

if __name__ == "__main__":
    dag.cli()

```
```python
import airflow
from airflow import DAG
from airflow.models import DagBag

dagbag = airflow.models.DagBag()
dag: DAG = dagbag.get_dag("example_bash_operator")

dag.clear()
dag.run()
```
Environment: Brreze
```
unset AIRFLOW__CORE__DAGS_FOLDER
unset AIRFLOW__CORE__UNIT_TEST_MODE
chmod -R 777 /root
sudo -E su airflow
export AIRFLOW__CORE__EXECUTOR="CeleryExecutor"
export AIRFLOW__CELERY__BROKER_URL="redis://redis:6379/0"
export AIRFLOW__CELERY__WORKER_CONCURRENCY=8
seq 1 10 | xargs -n 1 -I {} bash -c "time python /files/benchmark_speed.py > /dev/null 2>&1" | grep '^(real\|user\|sys)';
```

Result: 

|Fn.     | After | Before | Change|
|--------|-------|--------|-------|
|AVERAGE | 56.48 | 38.32  | -32%  |
|VAR     | 23.60 | 0.04   | -98%  |
|MAX     | 68.29 | 38.68  | -43%  |
|MIN     | 53.26 | 38.08  | -28%  |
|STDEV   | 4.86  | 0.19   | -96%. |

Raw data
After:
```
real 0m38.394s
user 0m4.340s
sys 0m1.600s

real 0m38.355s
user 0m4.700s
sys 0m1.340s

real 0m38.675s
user 0m4.760s
sys 0m1.530s

real 0m38.488s
user 0m4.770s
sys 0m1.280s

real 0m38.434s
user 0m4.600s
sys 0m1.390s

real 0m38.378s
user 0m4.500s
sys 0m1.270s

real 0m38.106s
user 0m4.200s
sys 0m1.100s

real 0m38.082s
user 0m4.170s
sys 0m1.030s

real 0m38.173s
user 0m4.290s
sys 0m1.340s

real 0m38.161s
user 0m4.460s
sys 0m1.370s
```

Before:
```
real 0m53.488s
user 0m5.140s
sys 0m1.700s

real 1m8.288s
user 0m6.430s
sys 0m2.200s

real 0m53.371s
user 0m5.330s
sys 0m1.630s

real 0m58.939s
user 0m6.470s
sys 0m1.730s

real 0m53.255s
user 0m4.950s
sys 0m1.640s

real 0m58.802s
user 0m5.970s
sys 0m1.790s

real 0m58.449s
user 0m5.380s
sys 0m1.580s

real 0m53.308s
user 0m5.120s
sys 0m1.430s

real 0m53.485s
user 0m5.220s
sys 0m1.290s

real 0m53.387s
user 0m5.020s
sys 0m1.590s
```

---
Link to JIRA issue: https://issues.apache.org/jira/browse/AIRFLOW-6361

- [x] Description above provides context of the change
- [x] Commit message starts with `[AIRFLOW-NNNN]`, where AIRFLOW-NNNN = JIRA ID*
- [x] Unit tests coverage for changes (not needed for documentation changes)
- [x] Commits follow "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)"
- [x] Relevant documentation is updated including usage instructions.
- [x] I will engage committers as explained in [Contribution Workflow Example](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#contribution-workflow-example).

(*) For document-only changes, no JIRA issue is needed. Commit message starts `[AIRFLOW-XXXX]`.

---
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dags/test_miscellaneous.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_bash_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py']
Ground Truth : ['a/airflow/executors/celery_executor.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: Use exec instead of subprocess for webserver command
Stopping the webserver using systemd only kills the parent process but leaves the gunicorn master and workers around, similar to what was reported with [gunicorn+supervisor](https://github.com/benoitc/gunicorn/issues/520). It appears the problem is related to running gunicorn with subprocess.Popen in cli.py instead of os.exec which would replace the process under control by systemd.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/serve_logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/_common_cli_classes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/process_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/base_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py']
Ground Truth : ['a/setup.py', 'a/airflow/executors/local_executor.py', 'a/airflow/bin/cli.py', 'a/airflow/executors/base_executor.py', 'a/airflow/jobs.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: Airflow doesn't re-use a secrets backend instance when loading configuration values
### Apache Airflow version

main (development)

### What happened

When airflow is loading its configuration, it creates a new secrets backend instance for each configuration backend it loads from secrets and then additionally creates a global secrets backend instance that is used in `ensure_secrets_loaded` which code outside of the configuration file uses. This can cause issues with the vault backend (and possibly others, not sure) since logging in to vault can be an expensive operation server-side and each instance of the vault secrets backend needs to re-login to use its internal client.

### What you think should happen instead

Ideally, airflow would attempt to create a single secrets backend instance and re-use this. This can possibly be patched in the vault secrets backend, but instead I think updating the `configuration` module to cache the secrets backend would be preferable since it would then apply to any secrets backend.

### How to reproduce

Use the hashicorp vault secrets backend and store some configuration in `X_secret` values. See that it logs in more than you'd expect.

### Operating System

Ubuntu 18.04

### Versions of Apache Airflow Providers

```
apache-airflow==2.3.0
apache-airflow-providers-hashicorp==2.2.0
hvac==0.11.2
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/secrets/key_vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/systems_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/secrets/secret_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/secrets/lockbox.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/variable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py']
Ground Truth : ['/dev/null', 'a/dev/breeze/src/airflow_breeze/pre_commit_ids.py', 'a/airflow/utils/serve_logs.py', 'a/airflow/www/compile_assets.sh', 'a/airflow/plugins_manager.py', 'a/airflow/utils/context.py', 'a/airflow/sensors/time_sensor.py', 'a/dev/breeze/src/airflow_breeze/params/shell_params.py', 'a/dev/breeze/src/airflow_breeze/global_constants.py', 'a/airflow/cli/commands/dag_processor_command.py', 'a/dev/breeze/src/airflow_breeze/params/build_ci_params.py', 'a/airflow/settings.py', 'a/airflow/callbacks/callback_requests.py', 'a/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', 'a/airflow/models/dagrun.py', 'a/airflow/templates.py', 'a/airflow/providers/microsoft/azure/hooks/cosmos.py', 'a/dev/breeze/src/airflow_breeze/utils/common_options.py', 'a/dev/breeze/src/airflow_breeze/utils/reinstall.py', 'a/airflow/models/taskmixin.py', 'a/airflow/config_templates/airflow_local_settings.py', 'a/airflow/models/taskreschedule.py', 'a/dev/breeze/src/airflow_breeze/utils/visuals.py', 'a/airflow/cli/commands/celery_command.py', 'a/setup.py', 'a/airflow/cli/commands/db_command.py', 'a/dev/provider_packages/prepare_provider_packages.py', 'a/scripts/in_container/verify_providers.py', 'a/scripts/ci/pre_commit/pre_commit_www_lint.py', 'a/airflow/providers/google/cloud/operators/cloud_sql.py', 'a/airflow/models/dag.py', 'a/airflow/cli/commands/kerberos_command.py', 'a/airflow/api_connexion/endpoints/extra_link_endpoint.py', 'a/scripts/ci/pre_commit/pre_commit_breeze_cmd_line.py', 'a/dev/breeze/src/airflow_breeze/utils/console.py', 'a/dev/breeze/src/airflow_breeze/utils/path_utils.py', 'a/scripts/ci/pre_commit/pre_commit_ui_lint.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/dag_processing/manager.py', 'a/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', 'a/airflow/models/taskinstance.py', 'a/airflow/configuration.py', 'a/airflow/utils/db.py', 'a/dev/breeze/src/airflow_breeze/commands/main_command.py', 'a/dev/breeze/src/airflow_breeze/commands/production_image_commands.py', 'a/airflow/operators/bash.py', 'a/airflow/ti_deps/deps/ready_to_reschedule.py', 'a/airflow/providers/amazon/aws/transfers/sql_to_s3.py', 'a/airflow/cli/commands/triggerer_command.py', 'a/dev/breeze/src/airflow_breeze/commands/developer_commands.py', 'a/airflow/models/param.py', 'a/dev/breeze/src/airflow_breeze/commands/ci_commands.py', 'a/airflow/www/fab_security/manager.py', 'a/scripts/in_container/run_migration_reference.py', 'a/scripts/ci/pre_commit/pre_commit_supported_versions.py', 'a/dev/breeze/src/airflow_breeze/params/build_prod_params.py', 'a/airflow/cli/cli_parser.py', 'a/dev/breeze/src/airflow_breeze/utils/image.py', 'a/dev/breeze/src/airflow_breeze/configure_rich_click.py', 'a/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', 'a/airflow/kubernetes/kube_client.py', 'a/dev/breeze/src/airflow_breeze/utils/selective_checks.py', 'a/airflow/providers/amazon/aws/transfers/dynamodb_to_s3.py', 'a/airflow/utils/entry_points.py', 'a/airflow/mypy/plugin/decorators.py', 'a/airflow/models/xcom.py', 'a/dev/check_files.py', 'a/dev/breeze/src/airflow_breeze/utils/run_utils.py', 'a/airflow/www/extensions/init_views.py', 'a/dev/breeze/src/airflow_breeze/params/common_build_params.py', 'a/dev/breeze/src/airflow_breeze/breeze.py', 'a/airflow/cli/commands/scheduler_command.py', 'a/airflow/dag_processing/processor.py', 'a/dev/breeze/src/airflow_breeze/commands/configuration_and_maintenance_commands.py', 'a/airflow/operators/trigger_dagrun.py', 'a/airflow/cli/commands/webserver_command.py', 'a/airflow/utils/db_cleanup.py', 'a/airflow/jobs/scheduler_job.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: Allow variables to be printed to STDOUT
### Description

Currently, the `airflow variables export` command requires an explicit file path and does not support output to stdout. However connections can be printed to stdout using `airflow connections export -`. This inconsistency between the two export commands can lead to confusion and limits the flexibility of the variables export command.

### Use case/motivation

To bring some consistency, similar to connections, variables should also be printed to STDOUT, using `-` instead of filename. 

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/hooks/test_subprocess.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_variable_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_connection_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py']
Ground Truth : ['a/airflow/cli/commands/connection_command.py', 'a/airflow/cli/commands/variable_command.py', '/dev/null', 'a/airflow/cli/cli_config.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: CI: Reuse Breeze CI image building for CI
We already have a basic Build image function in `Breeze` - we should be able to use it to build images in CI.

There are actions that build images in `./build-imaages.yml` and they should simply (similarely to free-space) use Python commands develped in ./dev/breeze.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/main_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_mypy_folder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_mypy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/docker_tests_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/md5_build_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/minor_release_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_check_provider_yaml_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/configure_rich_click.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py']
Ground Truth : ['a/dev/breeze/src/airflow_breeze/utils/path_utils.py', 'a/dev/breeze/src/airflow_breeze/breeze.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: Enabling Datadog to tag metrics results in AttributeError
**Apache Airflow version**: 2.0.1
**Python version**: 3.8
**Cloud provider or hardware configuration**: AWS

**What happened**:
In order to add tags to [Airflow metrics,](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/metrics.html), it's required to set `AIRFLOW__METRICS__STATSD_DATADOG_ENABLED` to `True` and add tags in the `AIRFLOW__METRICS__STATSD_DATADOG_TAGS` variable. We were routing our statsd metrics to Datadog anyway, so this should theoretically have not changed anything other than the addition of any specified tags.
Setting the environment variable `AIRFLOW__METRICS__STATSD_DATADOG_ENABLED` to `True` (along with the other required statsd connection variables) results in the following error, which causes the process to terminate. This is from the scheduler, but this would apply anywhere that `Stats.timer()` is being called.
```
AttributeError: 'DogStatsd' object has no attribute 'timer'
return Timer(self.dogstatsd.timer(stat, *args, tags=tags, **kwargs))
File "/usr/local/lib/python3.8/site-packages/airflow/stats.py", line 345, in timer
return fn(_self, stat, *args, **kwargs)
File "/usr/local/lib/python3.8/site-packages/airflow/stats.py", line 233, in wrapper
timer = Stats.timer('scheduler.critical_section_duration')
File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 1538, in _do_scheduling
num_queued_tis = self._do_scheduling(session)
File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 1382, in _run_scheduler_loop
self._run_scheduler_loop()
File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 1280, in _execute
Traceback (most recent call last):
```

**What you expected to happen**:
The same default Airflow metrics get sent by connecting to datadog, tagged with the metrics specified in `AIRFLOW__METRICS__STATSD_DATADOG_TAGS`.

**What do you think went wrong?**:
There is a bug in the implementation of the `Timer` method of `SafeDogStatsdLogger`. https://github.com/apache/airflow/blob/master/airflow/stats.py#L341-L347
`DogStatsd` has no method called `timer`. Instead it should be `timed`: https://datadogpy.readthedocs.io/en/latest/#datadog.dogstatsd.base.DogStatsd.timed

**How to reproduce it**:
Set the environment variables (or their respective config values) `AIRFLOW__METRICS__STATSD_ON`, `AIRFLOW__METRICS__STATSD_HOST`, `AIRFLOW__METRICS__STATSD_PORT`, and then set `AIRFLOW__METRICS__STATSD_DATADOG_ENABLED` to `True` and start up Airflow.


**Anything else we need to know**:
How often does this problem occur? Every time



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/datadog_logger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/otel_logger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/statsd_logger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_stats.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/stats.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: airflow db clean is unable to delete the table rendered_task_instance_fields
### Apache Airflow version

Other Airflow 2 version

### What happened

Hi All,

When I run the below command in Airflow 2.3.4:

`airflow db clean --clean-before-timestamp '2022-09-18T00:00:00+05:30' --yes`

I receive an error within a warning which says

`[2022-09-20 10:33:30,971] {db_cleanup.py:302} WARNING - Encountered error when attempting to clean table 'rendered_task_instance_fields'.`

All other tables like `log`, `dag`, `xcom` get deleted properly. On my analysis, `rendered_task_instance_fields` was the 5th largest table by rows in the DB, so the impact of it's data size is significant.

On analyzing the table itself on PostGres 13 DB, I found that the table `rendered_task_instance_fields` has no timestamp column that records when the entry was inserted.

https://imgur.com/a/Qys2uwD

Thus, there would be no way the code can filter out older records and delete them.

### What you think should happen instead

A timestamp field needs to be added to the table `rendered_task_instance_fields` basis of which older records can be deleted.

### How to reproduce

Run the below command in airflow v2.3.4 and check the output.

`airflow db clean --clean-before-timestamp '2022-09-18T00:00:00+05:30' --yes`

### Operating System

Ubuntu 20.04 LTS

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==5.1.0
apache-airflow-providers-celery==3.0.0
apache-airflow-providers-common-sql==1.2.0
apache-airflow-providers-ftp==3.1.0
apache-airflow-providers-google==8.3.0
apache-airflow-providers-http==4.0.0
apache-airflow-providers-imap==3.0.0
apache-airflow-providers-mongo==3.0.0
apache-airflow-providers-mysql==3.2.0
apache-airflow-providers-slack==5.1.0
apache-airflow-providers-sqlite==3.2.1

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_db_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py']
Ground Truth : ['a/airflow/utils/db_cleanup.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: KubernetesPodOperator stops tailing logs in long living task
KubernetesPodOperator stops tailing logs after 4 hours in long living task. When log tailing is interrupted, task log is flooded by `Event: pod had an event of type Running` messages. Messages are generated every 2 seconds.  
Log tail interruption occurs every time after 4 hours. Interruption can be related to timeout on AKS control plane. Nevertheless, KubernetesPodOperator should continue to tail logs after interruption. I'm going to submit fix. 

**Apache Airflow version**: 1.10.12
**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): 1.17.7
**Environment**: Azure Kubernetes Service

```
[2020-09-23 13:35:43,077] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 600: percentage: 2.00\n'
[2020-09-23 13:51:01,477] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 900: percentage: 3.00\n'
[2020-09-23 14:05:50,749] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 1200: percentage: 4.00\n'
[2020-09-23 14:20:42,323] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 1500: percentage: 5.00\n'
[2020-09-23 14:35:42,412] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 1800: percentage: 6.00\n'
[2020-09-23 14:50:52,880] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 2100: percentage: 7.00\n'
[2020-09-23 15:06:05,138] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 2400: percentage: 8.00\n'
[2020-09-23 15:21:24,700] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 2700: percentage: 9.00\n'
[2020-09-23 15:36:52,563] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 3000: percentage: 10.00\n'
[2020-09-23 15:52:32,510] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 3300: percentage: 11.00\n'
[2020-09-23 16:08:53,489] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 3600: percentage: 12.00\n'
[2020-09-23 16:24:13,732] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 3900: percentage: 13.00\n'
[2020-09-23 16:24:14,633] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 3900: percentage: 13.00\n'
[2020-09-23 16:24:14,690] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 3900: percentage: 13.00\n'
[2020-09-23 16:24:14,883] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 3900: percentage: 13.00\n'
[2020-09-23 16:39:40,816] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 4200: percentage: 14.00\n'
[2020-09-23 16:55:21,781] {pod_launcher.py:156} INFO - b'\x1b[2K\ron 4500: percentage: 15.00\n'
[2020-09-23 17:10:58,901] {pod_launcher.py:171} INFO - Event: some-pod had an event of type Running
[2020-09-23 17:10:58,927] {pod_launcher.py:166} INFO - Pod some-pod has state running
[2020-09-23 17:11:01,148] {pod_launcher.py:171} INFO - Event: some-pod had an event of type Running
[2020-09-23 17:11:01,154] {pod_launcher.py:166} INFO - Pod some-pod has state running
[2020-09-23 17:11:01,154] {pod_launcher.py:166} INFO - Pod some-pod has state running
...
the same message for next 10 hours every 2 seconds
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/utils/test_pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/flink/sensors/test_flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0067_2_0_0_add_external_executor_id_to_ti.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/sensors/test_spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/hooks/test_dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/parallel.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/docs_build/docs_builder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/beam/hooks/test_beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_time_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/transfers/test_mssql_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/transfers/test_postgres_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/postgres/example_postgres.py']
Ground Truth : ['a/airflow/kubernetes/pod_launcher.py']
Current Recall: 0.048539308657081684

=========================================================

ISSUE: S3KeySensor wildcard_match only matching key prefixes instead of full patterns
### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

3.4.0

### Apache Airflow version

2.3.2 (latest released)

### Operating System

Debian GNU/Linux 10

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

For patterns like "*.zip" the S3KeySensor succeeds for all files, does not take full pattern into account i.e. the ".zip" part). 
Bug introduced in https://github.com/apache/airflow/pull/22737

### What you think should happen instead

Full pattern match as in version 3.3.0 (in S3KeySensor poke()):

```
...
if self.wildcard_match:
            return self.get_hook().check_for_wildcard_key(self.bucket_key, self.bucket_name)
...
```

alternatively the files obtained by `files = self.get_hook().get_file_metadata(prefix, bucket_name)` which only match the prefix should be further filtered.

### How to reproduce

create a DAG with a key sensor task containing wildcard and suffix, e.g. the following task should succeed only if any ZIP-files are available in "my-bucket", but succeeds for all instead:

`S3KeySensor(task_id="wait_for_file", bucket_name="my-bucket", bucket_key="*.zip", wildcard_match=True)`

### Anything else

Not directly part of this issue but at the same time I would suggest to include additional file attributes in method _check_key, e.g. the actual key of the files. This way more filters, e.g. exclude specific keys, could be implemented by using the check_fn.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/sensors/test_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/triggers/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/gcs_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/providers/amazon/aws/sensors/s3.py']
Current Recall: 0.050680636280208025

=========================================================

ISSUE: Dynamic Tasks inside of TaskGroup do not have group_id prepended to task_id
### Apache Airflow version

2.3.3 (latest released)

### What happened

As the title states, if you have dynamically mapped tasks inside of a `TaskGroup`, those tasks do not get the `group_id` prepended to their respective `task_id`s.  This causes at least a couple of undesirable side effects:

1. Task names are truncated in Grid/Graph* View.  The tasks below are named `plus_one` and `plus_two`:

![Screenshot from 2022-07-19 13-29-05](https://user-images.githubusercontent.com/7269927/179826453-a4293c14-2a83-4739-acf2-8b378e4e85e9.png)
![Screenshot from 2022-07-19 13-47-47](https://user-images.githubusercontent.com/7269927/179826442-b9e3d24d-52ff-49fc-a8cc-fe1cb5143bcb.png)

Presumably this is because the UI normally strips off the `group_id` prefix.

\* Graph View was very inconsistent in my experience.  Sometimes the names are truncated, and sometimes they render correctly.  I haven't figured out the pattern behind this behavior.

2. Duplicate `task_id`s between groups result in a `airflow.exceptions.DuplicateTaskIdFound`, even if the `group_id` would normally disambiguate them.


### What you think should happen instead

These dynamic tasks inside of a group should have the `group_id` prepended for consistent behavior.

### How to reproduce

```
#!/usr/bin/env python3
import datetime

from airflow.decorators import dag, task
from airflow.utils.task_group import TaskGroup


@dag(
    start_date=datetime.datetime(2022, 7, 19),
    schedule_interval=None,
)
def test_dag():
    with TaskGroup(group_id='group'):
        @task
        def plus_one(x: int):
            return x + 1

        plus_one.expand(x=[1, 2, 3])

    with TaskGroup(group_id='ggg'):
        @task
        def plus_two(x: int):
            return x + 2

        plus_two.expand(x=[1, 2, 3])


dag = test_dag()


if __name__ == '__main__':
    dag.cli()
```

### Operating System

CentOS Stream 8

### Versions of Apache Airflow Providers

N/A

### Deployment

Other

### Deployment details

Standalone

### Anything else

Possibly related: #12309

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py']
Ground Truth : ['a/airflow/decorators/base.py']
Current Recall: 0.050680636280208025

=========================================================

ISSUE: Deferred TI's `next_method` and `next_kwargs` not cleared on retries
### Apache Airflow version

main (development)

### Operating System

macOS 11.5.2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

If your first try fails and you have retries enabled, the subsequent tries skip right to the final `next_method(**next_kwargs)` instead of starting with `execute` again.

### What you expected to happen

Like we reset things like start date, we should wipe `next_method` and `next_kwargs` so we can retry the task from the beginning.

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_cleartasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.050680636280208025

=========================================================

ISSUE: Spark JDBC Hook fails if spark_conf is not specified
**Apache Airflow version**: 1.10.10

**What happened**:

At SparkJDBCHook, the `spark_conf` parameter has default None, if kept like that it raise an error:
```
Traceback (most recent call last):
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 983, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/contrib/operators/spark_jdbc_operator.py", line 211, in execute
    self._hook.submit_jdbc_job()
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/contrib/hooks/spark_jdbc_hook.py", line 243, in submit_jdbc_job
    "/spark_jdbc_script.py")
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 383, in submit
    spark_submit_cmd = self._build_spark_submit_command(application)
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 254, in _build_spark_submit_command
    for key in self._conf:
TypeError: 'NoneType' object is not iterable
```

**What you expected to happen**:

Following the same behaviour than SparkSubmitHook, the a`spark_conf` should have default empty dict "{}"
```
self._conf = conf or {}
```

**How to reproduce it**:
Create a DAG with SparkJDBCOperator and don't specify the parameter `spark_conf`
```
    spark_to_jdbc_job = SparkJDBCOperator(
        cmd_type='spark_to_jdbc',
        jdbc_table="foo",
        spark_jars="${SPARK_HOME}/jars/postgresql-42.2.12.jar",
        jdbc_driver="org.postgresql.Driver",
        metastore_table="bar",
        save_mode="append",
        task_id="spark_to_jdbc_job"
    )
```


**Anything else we need to know**:

I am happy to implement this change.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/operators/spark_jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/apache/spark/example_spark_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/operators/test_spark_jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/operators/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py']
Ground Truth : ['a/airflow/providers/apache/spark/example_dags/example_spark_dag.py', 'a/airflow/providers/apache/spark/hooks/spark_jdbc.py']
Current Recall: 0.051751300091771195

=========================================================

ISSUE: Spark SQL Hook not using connections
**Apache Airflow version**: 1.10.10

**What happened**:

`SparkSqlHook` is not using any connection, the default conn_id is `spark_sql_default`, if this connection doesn't exist, the hook returns an error:
```
Traceback (most recent call last):
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 983, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/contrib/operators/spark_sql_operator.py", line 109, in execute
    yarn_queue=self._yarn_queue
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/contrib/hooks/spark_sql_hook.py", line 75, in __init__
    self._conn = self.get_connection(conn_id)
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 84, in get_connection
    conn = random.choice(list(cls.get_connections(conn_id)))
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/hooks/base_hook.py", line 80, in get_connections
    return secrets.get_connections(conn_id)
  File "/Users/rbottega/Documents/airflow_latest/env/lib/python3.7/site-packages/airflow/secrets/__init__.py", line 56, in get_connections
    raise AirflowException("The conn_id `{0}` isn't defined".format(conn_id))
airflow.exceptions.AirflowException: The conn_id `spark_sql_default` isn't defined
```
If specified any valid connection, it does nothing, the `self._conn` variable is never used and there is an empty `get_conn` method.
```
    def get_conn(self):
        pass
```

**What you expected to happen**:

It should follow the same behaviour of `SparkSubmitHook` to receive the master host and extra parameters from the connection OR don't request a connection ID.

**How to reproduce it**:
Just create a DAG with a `SparkSqlOperator` and have not created the connection `spark_sql_default`.
```
    sql_job = SparkSqlOperator(
        sql="SELECT * FROM test",
        master="local",
        task_id="sql_job"
    )
```


**Anything else we need to know**:

I am happy to implement any of these solutions.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/secrets/local_filesystem.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/postgres/hooks/postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/operators/spark_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/hooks/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py']
Ground Truth : ['a/airflow/providers/apache/spark/operators/spark_sql.py', 'a/airflow/providers/apache/spark/hooks/spark_sql.py']
Current Recall: 0.051751300091771195

=========================================================

ISSUE: When an ECS Task fails to start, ECS Operator raises a CloudWatch exception
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 1.10.13

**Environment**:

- **Cloud provider or hardware configuration**:AWS
- **OS** (e.g. from /etc/os-release): Amazon Linux 2
- **Kernel** (e.g. `uname -a`):  4.14.209-160.339.amzn2.x86_64
- **Install tools**: pip
- **Others**:

**What happened**:

When an ECS Task exits with `stopCode: TaskFailedToStart`, the ECS Operator will exit with a ResourceNotFoundException for the GetLogEvents operation. This is because the task has failed to start, so no log is created.

```
[2021-03-14 02:32:49,792] {ecs_operator.py:147} INFO - ECS Task started: {'tasks': [{'attachments': [], 'availabilityZone': 'ap-northeast-1c', 'clusterArn': 'arn:aws:ecs:ap-northeast-1:xxxx:cluster/ecs-cluster', 'containerInstanceArn': 'arn:aws:ecs:ap-northeast-1:xxxx:container-instance/ecs-cluster/xxxx', 'containers': [{'containerArn': 'arn:aws:ecs:ap-northeast-1:xxxx:container/xxxx', 'taskArn': 'arn:aws:ecs:ap-northeast-1:xxxx:task/ecs-cluster/xxxx', 'name': 'container_image', 'image': 'xxxx.dkr.ecr.ap-northeast-1.amazonaws.com/ecr/container_image:latest', 'lastStatus': 'PENDING', 'networkInterfaces': [], 'cpu': '128', 'memoryReservation': '128'}], 'cpu': '128', 'createdAt': datetime.datetime(2021, 3, 14, 2, 32, 49, 770000, tzinfo=tzlocal()), 'desiredStatus': 'RUNNING', 'group': 'family:task', 'lastStatus': 'PENDING', 'launchType': 'EC2', 'memory': '128', 'overrides': {'containerOverrides': [{'name': 'container_image', 'command': ['/bin/bash', '-c', 'xxxx']}], 'inferenceAcceleratorOverrides': []}, 'startedBy': 'airflow', 'tags': [], 'taskArn': 'arn:aws:ecs:ap-northeast-1:xxxx:task/ecs-cluster/xxxx', 'taskDefinitionArn': 'arn:aws:ecs:ap-northeast-1:xxxx:task-definition/task:1', 'version': 1}], 'failures': [], 'ResponseMetadata': {'RequestId': 'xxxx', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'xxxx', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1471', 'date': 'Sun, 14 Mar 2021 02:32:48 GMT'}, 'RetryAttempts': 0}}
[2021-03-14 02:34:15,022] {ecs_operator.py:168} INFO - ECS Task stopped, check status: {'tasks': [{'attachments': [], 'availabilityZone': 'ap-northeast-1c', 'clusterArn': 'arn:aws:ecs:ap-northeast-1:xxxx:cluster/ecs-cluster', 'connectivity': 'CONNECTED', 'connectivityAt': datetime.datetime(2021, 3, 14, 2, 32, 49, 770000, tzinfo=tzlocal()), 'containerInstanceArn': 'arn:aws:ecs:ap-northeast-1:xxxx:container-instance/ecs-cluster/xxxx', 'containers': [{'containerArn': 'arn:aws:ecs:ap-northeast-1:xxxx:container/xxxx', 'taskArn': 'arn:aws:ecs:ap-northeast-1:xxxx:task/ecs-cluster/xxxx', 'name': 'container_image', 'image': 'xxxx.dkr.ecr.ap-northeast-1.amazonaws.com/ecr/container_image:latest', 'lastStatus': 'STOPPED', 'reason': 'CannotPullContainerError: failed to register layer: Error processing tar file(exit status 1): write /var/lib/xxxx: no space left on device', 'networkInterfaces': [], 'healthStatus': 'UNKNOWN', 'cpu': '128', 'memoryReservation': '128'}], 'cpu': '128', 'createdAt': datetime.datetime(2021, 3, 14, 2, 32, 49, 770000, tzinfo=tzlocal()), 'desiredStatus': 'STOPPED', 'executionStoppedAt': datetime.datetime(2021, 3, 14, 2, 34, 12, 810000, tzinfo=tzlocal()), 'group': 'family:task', 'healthStatus': 'UNKNOWN', 'lastStatus': 'STOPPED', 'launchType': 'EC2', 'memory': '128', 'overrides': {'containerOverrides': [{'name': 'container_image', 'command': ['/bin/bash', '-c', 'xxxx']}], 'inferenceAcceleratorOverrides': []}, 'pullStartedAt': datetime.datetime(2021, 3, 14, 2, 32, 51, 68000, tzinfo=tzlocal()), 'pullStoppedAt': datetime.datetime(2021, 3, 14, 2, 34, 13, 584000, tzinfo=tzlocal()), 'startedBy': 'airflow', 'stopCode': 'TaskFailedToStart', 'stoppedAt': datetime.datetime(2021, 3, 14, 2, 34, 12, 821000, tzinfo=tzlocal()), 'stoppedReason': 'Task failed to start', 'stoppingAt': datetime.datetime(2021, 3, 14, 2, 34, 12, 821000, tzinfo=tzlocal()), 'tags': [], 'taskArn': 'arn:aws:ecs:ap-northeast-1:xxxx:task/ecs-cluster/xxxx', 'taskDefinitionArn': 'arn:aws:ecs:ap-northeast-1:xxxx:task-definition/task:1', 'version': 2}], 'failures': [], 'ResponseMetadata': {'RequestId': 'xxxx', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'xxxx', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1988', 'date': 'Sun, 14 Mar 2021 02:34:14 GMT'}, 'RetryAttempts': 0}}
[2021-03-14 02:34:15,024] {ecs_operator.py:172} INFO - ECS Task logs output:
[2021-03-14 02:34:15,111] {credentials.py:1094} INFO - Found credentials in environment variables.
[2021-03-14 02:34:15,416] {taskinstance.py:1150} ERROR - An error occurred (ResourceNotFoundException) when calling the GetLogEvents operation: The specified log stream does not exist.
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/contrib/operators/ecs_operator.py", line 152, in execute
    self._check_success_task()
  File "/usr/local/lib/python3.7/site-packages/airflow/contrib/operators/ecs_operator.py", line 175, in _check_success_task
    for event in self.get_logs_hook().get_log_events(self.awslogs_group, stream_name):
  File "/usr/local/lib/python3.7/site-packages/airflow/contrib/hooks/aws_logs_hook.py", line 85, in get_log_events
    **token_arg)
  File "/usr/local/lib/python3.7/site-packages/botocore/client.py", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/usr/local/lib/python3.7/site-packages/botocore/client.py", line 676, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.ResourceNotFoundException: An error occurred (ResourceNotFoundException) when calling the GetLogEvents operation: The specified log stream does not exist.
```

<!-- (please include exact error messages if you can) -->

**What you expected to happen**:

ResourceNotFoundException is misleading because it feels like a problem with CloudWatchLogs. Expect AirflowException to indicate that the task has failed.

<!-- What do you think went wrong? -->

**How to reproduce it**:
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->

This can be reproduced by running an ECS Task that fails to start, for example by specifying a non-existent entry_point.

**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->

I suspect Issue #11663 has the same problem, i.e. it's not a CloudWatch issue, but a failure to start an ECS Task.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/sensors/test_emr_step.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/ecs.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Allow to set statement behavior for PostgresOperator
### Body

Add the ability to pass parameters like `statement_timeout` from PostgresOperator.
https://www.postgresql.org/docs/14/runtime-config-client.html#GUC-STATEMENT-TIMEOUT

The goal is to allow to control over specific query rather than setting the parameters on the connection level.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/postgres/operators/test_postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/postgres/operators/postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/oracle/hooks/oracle.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/teradata/hooks/teradata.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/sql_to_slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/pubsub.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/exasol/hooks/test_exasol.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py']
Ground Truth : ['a/airflow/providers/postgres/operators/postgres.py', 'a/airflow/providers/postgres/example_dags/example_postgres.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Graph View shows other relations than in DAG
**Apache Airflow version**:

master

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**: breeze

**What happened**:

This DAG
```python
from airflow import models
from airflow.operators.dummy_operator import DummyOperator
from airflow.utils.dates import days_ago

with models.DAG("test", start_date=days_ago(1), schedule_interval=None,) as dag:
    t1 = DummyOperator(task_id="t1")
    t2 = DummyOperator(task_id="t2")
    t1 >> t2
```

is rendering like that:

<img width="1374" alt="Screenshot 2020-08-27 at 19 59 41" src="https://user-images.githubusercontent.com/9528307/91478403-11d7fb00-e8a0-11ea-91d0-d7d578bcb5a2.png">

**What you expected to happen**:

I expect to see same relations as defined in DAG file

**How to reproduce it**:

Render the example DAG from above.


**Anything else we need to know**:

I'm surprised by this bug  


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/dags/elastic_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/docker/example_docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/cncf/kubernetes/example_spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/cncf/kubernetes/example_kubernetes_resource.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/apache/kafka/example_dag_hello_kafka.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_setup_teardown.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/edgemodifier.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_endpoint.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: KubernetesPodOperator pod_template_file content doesn't support jinja airflow template variables
KubernetesPodOperator  pod_template_file content doesn't support jinja airflow template variables. pod_template_file  is part of templated_fields list. 
https://github.com/apache/airflow/blob/master/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py#L165

template_fields: Iterable[str] = (        'image',        'cmds',        'arguments',        'env_vars',        'labels',        'config_file',        'pod_template_file',    )

But pod_template_file content is not supporting template variables. pod_template_file can be implemented the way SparkKubernetesOperator implemented using template_ext

https://github.com/apache/airflow/blob/master/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py#L46
 template_ext = ('yaml', 'yml', 'json')

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/test_pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/template/templater.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py']
Ground Truth : ['a/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Extra Links do not works with mapped operators
### Apache Airflow version

main (development)

### What happened

I found that Extra Links do not work with dynamic tasks at all - links inaccessible, but same Extra Links works fine with not mapped operators.

I think the nature of that extra links assign to parent task instance (i do not know how to correct name this TI) but not to actual mapped TIs.

As result we only have `number extra links defined` in operator not `(number extra links defined in operator) x number of mapped TIs.`

### What you think should happen instead

_No response_

### How to reproduce

```python
from pendulum import datetime

from airflow.decorators import dag
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.operators.empty import EmptyOperator

EXTERNAL_DAG_IDS = [f"example_external_dag_{ix:02d}" for ix in range(3)]
DAG_KWARGS = {
    "start_date": datetime(2022, 7, 1),
    "schedule_interval": "@daily",
    "catchup": False,
    "tags": ["mapped_extra_links", "AIP-42", "serialization"],
}


def external_dags():
    EmptyOperator(task_id="dummy")


@dag(**DAG_KWARGS)
def external_regular_task_sensor():
    for external_dag_id in EXTERNAL_DAG_IDS:
        ExternalTaskSensor(
            task_id=f'wait_for_{external_dag_id}',
            external_dag_id=external_dag_id,
            poke_interval=5,
        )


@dag(**DAG_KWARGS)
def external_mapped_task_sensor():
    ExternalTaskSensor.partial(
        task_id='wait',
        poke_interval=5,
    ).expand(external_dag_id=EXTERNAL_DAG_IDS)


dag_external_regular_task_sensor = external_regular_task_sensor()
dag_external_mapped_task_sensor = external_mapped_task_sensor()

for dag_id in EXTERNAL_DAG_IDS:
    globals()[dag_id] = dag(dag_id=dag_id, **DAG_KWARGS)(external_dags)()
```

https://user-images.githubusercontent.com/3998685/180994213-847b3fd3-d351-4836-b246-b54056f34ad6.mp4

### Operating System

macOs 12.5

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_external_task_marker_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/core/example_external_task_parent_deferrable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/triggers/test_external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py']
Ground Truth : ['a/airflow/providers/qubole/operators/qubole.py', 'a/airflow/models/abstractoperator.py', 'a/airflow/models/baseoperator.py', 'a/airflow/operators/trigger_dagrun.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Choose setting for sqlalchemy SQLALCHEMY_TRACK_MODIFICATIONS
### Body

We need to determine what to do about this warning:

```
/Users/dstandish/.virtualenvs/2.4.0/lib/python3.8/site-packages/flask_sqlalchemy/__init__.py:872 FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future.  Set it to True or False to suppress this warning.
```

Should we set to true or false?

@ashb @potiuk @jedcunningham @uranusjr 

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/scheduler_dag_execution_timing.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py']
Ground Truth : ['a/airflow/utils/db.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: GCSToBigQueryOperator no longer uses field_delimiter or time_partitioning
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

google=8.5.0

### Apache Airflow version

2.4.3

### Operating System

Debian GNU/Linux 11 (bullseye)

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

The newest version of the google providers no longer provides the `field_delimiter` or `time_partitioning` fields to the bq job configuration for the GCStoBQ transfers. Looking at the code it seems like this behavior was removed during the change to use deferrable operations

### What you think should happen instead

These fields should continue to be provided

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Tasks get stuck in "scheduled" state and starved when dags with huge amount of tasks is scheduled
### Apache Airflow version

2.0.2

### Operating System

amzn linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

AIRFLOW__CORE__PARALLELISM: "128"
worker_concurrency: 32
2 Celery workers
MySQL 8.0.23 RDS as a DB backend

### What happened

* Tasks get stuck in "scheduled" state for hours, in task details it says that "All dependencies are met but the task instance is not running"
* The stuck tasks are executed eventually
* Usually, at the same time, there're DAGs with >100 tasks are running
* The big dags are limited by dag-level `concurrency` parameter to 10 tasks at a time
* Workers and pools have plenty of free slots
* If big dags are switched off - starving tasks are picked up immediately, even if tasks from the big dags are still running
* In scheduler logs, the starving task do not appear in the "tasks up for execution" list
* Number of concurrent tasks that are actually running is around 30 total on both executors (out of 64 available slots)

### What you expected to happen

As there are enough slots on the workers & pools, I expect tasks that are ready and actually *can* run to be picked up and moved to queued by scheduler

### How to reproduce

This example dag should reproduce the problem on environment with at least 20-25 available slots and core parallelism of 128. The dag that will get starved is the "tester_multi_load_3". Not each task, but on my env there were holes of up to 20 minutes between tasks execution. Guess the starvation time depends on ordering (?), as I'm not adding any weights...

<details><summary>CLICK ME</summary>
<p>


```python
import os
import time
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator


default_args = {
    'owner': 'Tester',
    'depends_on_past': False,
    'start_date': datetime(2021, 7, 17),
    'retries': 5
}


def sleep(timer):
    if not timer:
        timer = 60
    print(f'Timer is {str(timer)}')
    time.sleep(timer)


with DAG(
        dag_id=os.path.basename(__file__).replace('.py', '') + '_1',  # name of the dag
        default_args=default_args,
        concurrency=10,
        max_active_runs=5,
        schedule_interval='@hourly',
        orientation='LR',
        tags=['testers']

) as dag1:

    for i in range(150):
        t = PythonOperator(
            task_id=f'python_{i}',
            python_callable=sleep,
            op_args=[""],
            priority_weight=-100,
        )

with DAG(
        os.path.basename(__file__).replace('.py', '') + '_2',  # name of the dag
        default_args=default_args,
        concurrency=7,
        max_active_runs=2,
        schedule_interval='@hourly',
        orientation='LR',
        tags=['testers']

) as dag2:

    for i in range(150):
        t = PythonOperator(task_id=f'python_{i}',
                           python_callable=sleep,
                           op_args=[""],
                           )


with DAG(
        os.path.basename(__file__).replace('.py', '') + '_3',  # name of the dag
        default_args=default_args,
        concurrency=1,
        max_active_runs=1,
        schedule_interval='@hourly',
        orientation='LR',
        tags=['testers']

) as dag3:

    t1 = PythonOperator(task_id=f'python', python_callable=sleep, op_args=[""])

    for i in range(10):
        t2 = PythonOperator(task_id=f'python_{i}', python_callable=sleep, op_args=[""])
        t1 >> t2
        t1 = t2
```

</p>
</details>


### Anything else

Digging around the code, I found that there's a limit on the query scheduler preforms [here](https://github.com/apache/airflow/blob/10023fdd65fa78033e7125d3d8103b63c127056e/airflow/jobs/scheduler_job.py#L928) , that comes from [here](https://github.com/apache/airflow/blob/10023fdd65fa78033e7125d3d8103b63c127056e/airflow/jobs/scheduler_job.py#L1141), and actually seems to be calculated overall from the global `parallelism` value.
So actually what happens, is that scheduler queries DB with a limit, gets back a partial list of tasks that are actually cannot be executed because of the dag-level concurrency, and gets to other tasks that are able to run only when there's a window between big dags execution. Increasing the `parallelism` to 1024 solved the issue in our case.
The `parallelism` parameter in this case is very confusing, because it should indicate tasks that can be `run concurrently`, but actually limits the scheduler's ability to move tasks from scheduled to queued... 

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/apache/kafka/example_dag_hello_kafka.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Audit Log for Connections/Variables update
### Description

We have audit logs for major actions in Airflow except for Connection/Variables creation, update, and deletion

### Use case/motivation

Connections and variables form the primary component of DAGs. Having a track of who changed what when will come in handy when troubleshooting

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py']
Ground Truth : ['a/airflow/models/log.py', 'a/airflow/www/decorators.py', 'a/airflow/www/views.py', '/dev/null', 'a/airflow/api_connexion/endpoints/connection_endpoint.py', 'a/airflow/api_connexion/endpoints/variable_endpoint.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: SubDagOperators report success when BackfillJobs deadlock
As discussed with @syvineckruyk, follow up from #1225. 

The `BackfillJob` is deadlocking as expected but it doesn't raise an error, so the `SubDagOperator` thinks it succeeded.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dags/test_issue_1225.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/subdag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_subdag_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/mark_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_blocked.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py']
Ground Truth : ['a/airflow/configuration.py', 'a/airflow/models.py', 'a/airflow/bin/cli.py', 'a/airflow/jobs.py', 'a/airflow/utils/state.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: task_instance_mutation_hook not using my custom implementation
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: v1.10.11


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): NO

**Environment**: airflow 1.10.11 running on ubuntu 18.04 (used pip for installation) 

- **Cloud provider or hardware configuration**: bare metal 
- **OS** (e.g. from /etc/os-release): ubuntu 18.04
- **Kernel** (e.g. `uname -a`): Linux dev1 4.15.0-108-generic 109-Ubuntu SMP Fri Jun 19 11:33:10 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**: pip
- **Others**:

**What happened**:
I'm not able to get task_instance_mutation_hook (https://github.com/apache/airflow/pull/8852) to load my custom implementation from `$AIRFLOW_HOME/config/airflow_local_settings.py`.
<!-- (please include exact error messages if you can) -->

**What you expected to happen**: Airflow to use task_instance_mutation_hook provided in `$AIRFLOW_HOME/config/airflow_local_settings.py`.

<!-- What do you think went wrong? -->

**How to reproduce it**: make sure your running v1.10.11 and create a custom `$AIRFLOW_HOME/config/airflow_local_settings.py` where you define a dummy `task_instance_mutation_hook` as next
```python
def task_instance_mutation_hook(ti):
   # every new or up to retry task will be added to non existent queue, so no task will pass through
   if ti.state in [State.NONE, State.UP_FOR_RETRY]:
      ti.queue = 'dummy-queue'
```
if any of the airflow example dags run successfully then, the previous hook didn't work.
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md sytle of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**: task_instance_mutation_hook is a new feature introduced in v1.10.11 

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py']
Ground Truth : ['a/airflow/operators/python_operator.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Revert "Display parameter values from serialized dag in trigger dag view. (#27482)

This reverts commit 9409293514cef574179a5320ed3ed50881064423.

@tirkarthi we can't view grid view of a dag if the Param type is boolean

```python
int1 = randint(20, 200)
bool1 = []
bools = bool1.append(True) if int1 % 2 == 0 else bool1.append(False)

@task
def fail_if_invalid(val):
        print(val)
        assert type(val[0]) == bool


@dag(
    start_date=datetime(1970, 1, 1),
    schedule_interval=timedelta(days=365 * 30),
    params={"val": Param(False, type="boolean")},
    doc_md=docs,
    tags=["core", "taskflow-api", "dag-params"],
)
def bool_taskflow_test(bool_val):

    # val is a DagParam object
    # print(val)  # <airflow.models.param.DagParam object at 0x103360b50>

    # the task dereferences it
    fail_if_invalid(bool_val)


the_dag = bool_taskflow_test(bool1)
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/oracle/hooks/oracle.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/hooks/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py']
Ground Truth : ['a/airflow/serialization/serialized_objects.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: option to disable "lots of circles" in error page
**Description**

The "lots of circles" error page is very rough via Remote Desktop connections. In the current global pandemic many people are working remotely via already constrained connections. Needless redraws caused by the highly animated circles can cause frustrating slowdowns and sometimes lost connections.

**Use case / motivation**

It should be trivially simple to disable the animated portion of the error page and instead use a standard error page. Ideally this would be something easily achievable via configuration options and exposed in the Helm chart.

**Related Issues**

N/A


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/psrp/hooks/psrp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/pipeline_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/stackdriver.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/pipeline_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/stackdriver_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/hooks/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/www/extensions/init_views.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Multibyte Upload Behavior of S3Hook.load_file Causing Issues
When using the `S3Hook.load_file`, if `multipart_bytes` is set, uploads will fail if the file is smaller than `multipart_bytes`. The acute cause can be found [here](https://github.com/airbnb/airflow/blob/master/airflow/hooks/S3_hook.py#L316).

What happens is when the method calculates the number of chunks to use, it gets zero. The range iterator thus returns an empty set and no parts get uploaded. When the `complete_upload` method of the multipart uploader gets called, the whole thing crashes.

We've created a plugin to copy files from HDFS to S3 and it calls an `S3Hook`. We want to use multipart uploads if the file is larger than `multipart_bytes`. It would be nice if the hook could make that distinction and automatically choose whether to use multipart upload or not. At the very least, it should definitely not fail.

The easy way to fix this would be to stat the file to get its size, and then branch to either a multipart upload or a regular upload. Would be happy to submit a PR for the fix if that sounds reasonable.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/s3_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/data_lake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/transfers/local_to_adls.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/suite/transfers/local_to_drive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/suite/hooks/drive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/sql_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/azure_blob_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/alibaba/cloud/log/oss_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/gcs_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/s3.py']
Ground Truth : ['a/airflow/hooks/S3_hook.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: AIP-44 Migrate DagFileProcessorManager._deactivate_stale_dags to Internal API


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_internal/internal_api_call.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/dag_run.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_pydantic_models.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_internal/endpoints/rpc_api_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_internal/test_internal_api_call.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/test_parameters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_internal/endpoints/test_rpc_api_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/dag_processor_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_processor_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py']
Ground Truth : ['a/airflow/dag_processing/manager.py', 'a/airflow/api_internal/endpoints/rpc_api_endpoint.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Dynamic task mapping creates too many mapped instances when task pushed non-default XCom
### Apache Airflow version

2.3.0 (latest released)

### What happened

Excess tasks are created when using dynamic task mapping with KubernetesPodOperator, but only in certain cases which I do not understand. I have a simple working example of this where the flow is:
- One task that returns a list XCom (list of lists, since I'm partial-ing to `KubernetesPodOperator`'s `arguments`) of length 3. This looks like `[["a"], ["b"], ["c"]]`
- A `partial` from this, which is expanded on the above's result. Each resulting task has an XCom of a single element list that looks like `["d"]`. We expect the `expand` to result in 3 tasks, which it does. So far so good. Why doesn't the issue occur at this stage? No clue.
- A `partial` from the above. We expect 3 tasks in this final stage, but get 9. 3 succeed and 6 fail consistently. This 3x rule scales to as many tasks as you define in step 2 (e.g. 2 tasks in step 2 -> 6 tasks in step 3, where 4 fail)

![image](https://user-images.githubusercontent.com/71299310/169179360-d1ddfe49-f20e-4f27-909f-4dd101386a5a.png)

If I recreate this using the TaskFlow API with `PythonOperator`s, I get the expected result of 1 task -> 3 tasks -> 3 tasks

![image](https://user-images.githubusercontent.com/71299310/169179409-d2f71f3e-6e8c-42e0-8120-1ecccff439c0.png)

Futhermore, if I attempt to look at the `Rendered Template` of the failed tasks in the `KubernetesPodOperator` implementation (first image), I consistently get `Error rendering template` and all the fields are `None`. The succeeded tasks look normal.

![image](https://user-images.githubusercontent.com/71299310/169181052-8d182722-197b-44a5-b145-3a983e259036.png)

Since the `Rendered Template` view fails to load, I can't confirm what is actually getting provided to these failing tasks' `argument` parameter. If there's a way I can query the meta database to see this, I'd be glad to if given instruction. 

### What you think should happen instead

I think this has to do with how XComs are specially handled with the `KubernetesPodOperator`. If we look at the XComs tab of the upstream task (`some-task-2` in the above images), we see that the return value specifies `pod_name` and `pod_namespace` along with `return_value`.

![image](https://user-images.githubusercontent.com/71299310/169179724-984682d0-c2fc-4097-9527-fa3cbf3ad93f.png)

Whereas in the `t2` task of the TaskFlow version, it only contains `return_value`. 

![image](https://user-images.githubusercontent.com/71299310/169179983-b22347de-eef0-4a9a-ae85-6b5e5d2bfa42.png)

I haven't dug through the code to verify, but I have a strong feeling these extra values `pod_name` and `pod_namespace` are being used to generate the `OperatorPartial`/`MappedOperator` as well when they shouldn't be.

### How to reproduce

Run this DAG in a k8s context:

```
from datetime import datetime
from airflow import XComArg
from airflow.models import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator


def make_operator(
    **kwargs
):
    return KubernetesPodOperator(
        **{
            'get_logs': True,
            'in_cluster': True,
            'is_delete_operator_pod': True,
            'namespace': 'default',
            'startup_timeout_seconds': 600,
            **kwargs,
        }
    )


def make_partial_operator(
    **kwargs
):
    return KubernetesPodOperator.partial(
        **{
            'get_logs': True,
            'in_cluster': True,
            'is_delete_operator_pod': True,
            'namespace': 'default',
            'startup_timeout_seconds': 600,
            **kwargs,
        }
    )


with DAG(dag_id='test-pod-xcoms',
         schedule_interval=None,
         start_date=datetime(2020, 1, 1),
         max_active_tasks=20) as dag:

    op1 = make_operator(
        cmds=['python3', '-c' 'import json;f=open("/airflow/xcom/return.json", "w");f.write(json.dumps([["a"], ["b"], ["c"]]))'],
        image='python:3.9-alpine',
        name='airflow-private-image-pod-1',
        task_id='some-task-1',
        do_xcom_push=True
    )

    op2 = make_partial_operator(
        cmds=['python3', '-c' 'import json;f=open("/airflow/xcom/return.json", "w");f.write(json.dumps(["d"]))'],
        image='python:3.9-alpine',
        name='airflow-private-image-pod-2',
        task_id='some-task-2',
        do_xcom_push=True
    )

    op3 = make_partial_operator(
        cmds=['echo', 'helloworld'],
        image='alpine:latest',
        name='airflow-private-image-pod-3',
        task_id='some-task-3',
    )

    op3.expand(arguments=XComArg(op2.expand(arguments=XComArg(op1))))
```

For the TaskFlow version of this that works, run this DAG (doesn't have to be k8s context):

```
from datetime import datetime

from airflow.decorators import task
from airflow.models import DAG, Variable


@task
def t1():
    return [[1], [2], [3]]


@task
def t2(val):
    return val


@task
def t3(val):
    print(val)


with DAG(dag_id='test-mapping',
         schedule_interval=None,
         start_date=datetime(2020, 1, 1)) as dag:

    t3.partial().expand(val=t2.partial().expand(val=t1()))
```

### Operating System

MacOS 11.6.5

### Versions of Apache Airflow Providers

Relevant:

```
apache-airflow-providers-cncf-kubernetes==4.0.1
```

Full:

```
apache-airflow-providers-amazon==3.3.0
apache-airflow-providers-celery==2.1.4
apache-airflow-providers-cncf-kubernetes==4.0.1
apache-airflow-providers-docker==2.6.0
apache-airflow-providers-elasticsearch==3.0.3
apache-airflow-providers-ftp==2.1.2
apache-airflow-providers-google==6.8.0
apache-airflow-providers-grpc==2.0.4
apache-airflow-providers-hashicorp==2.2.0
apache-airflow-providers-http==2.1.2
apache-airflow-providers-imap==2.2.3
apache-airflow-providers-microsoft-azure==3.8.0
apache-airflow-providers-mysql==2.2.3
apache-airflow-providers-odbc==2.0.4
apache-airflow-providers-postgres==4.1.0
apache-airflow-providers-redis==2.0.4
apache-airflow-providers-sendgrid==2.0.4
apache-airflow-providers-sftp==2.6.0
apache-airflow-providers-slack==4.2.3
apache-airflow-providers-sqlite==2.1.3
apache-airflow-providers-ssh==2.4.3
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Docker (Docker Desktop)

- Server Version: 20.10.13
- API Version: 1.41
- Builder: 2

Kubernetes (Docker Desktop)

- Env: docker-desktop
- Context: docker-desktop
- Cluster Name: docker-desktop
- Namespace: default
- Container Runtime: docker
- Version: v1.22.5

Helm:

- version.BuildInfo{Version:"v3.6.3", GitCommit:"d506314abfb5d21419df8c7e7e68012379db2354", GitTreeState:"dirty", GoVersion:"go1.16.5"}

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/cncf/kubernetes/example_kubernetes_async.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py']
Ground Truth : ['a/airflow/models/expandinput.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: API Endpoints - Read-only - Import errors
**Description**
Hello 

We need to create several endpoints that perform basic read-only operations on **Import error** . We need the following endpoints:

- GET /importErrors
- GET /importErrors/{import_error_id}

For now, we focus only on read-only operations, but others will also be implemented as the next step.

Detailed information is available in the issue:
https://github.com/apache/airflow/issues/8118

**Use case / motivation**
N/A

**Related Issues**
N/A

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py']
Ground Truth : ['/dev/null', 'a/airflow/api_connexion/endpoints/import_errror_endpoint.py', 'a/airflow/api_connexion/exceptions.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Airflow UI - Go button on Tree View causes a Invalid Date
**Apache Airflow version**:
1.10.10

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

kubeenetes ver: 1.15.5

**Environment**:

- **Cloud provider or hardware configuration**: Used docker based Airflow Image
- **OS** (e.g. from /etc/os-release): Debian
- **Kernel** (e.g. `uname -a`): 4.19.76-linuxkit x86_64 Linux


**What happened**:

When user navigates to tree view of dag and keep on clicking on `Go` button at `number of runs` the` base date` cycle back to one dag run and in the end when there is no dag run left, it shows `invalid date`. In the end user gets below exception: 

```
                          ____/ (  (    )   )  \___
                         /( (  (  )   _    ))  )   )\
                       ((     (   )(    )  )   (   )  )
                     ((/  ( _(   )   (   _) ) (  () )  )
                    ( (  ( (_)   ((    (   )  .((_ ) .  )_
                   ( (  )    (      (  )    )   ) . ) (   )
                  (  (   (  (   ) (  _  ( _) ).  ) . ) ) ( )
                  ( (  (   ) (  )   (  ))     ) _)(   )  )  )
                 ( (  ( \ ) (    (_  ( ) ( )  )   ) )  )) ( )
                  (  (   (  (   (_ ( ) ( _    )  ) (  )  )   )
                 ( (  ( (  (  )     (_  )  ) )  _)   ) _( ( )
                  ((  (   )(    (     _    )   _) _(_ (  (_ )
                   (_((__(_(__(( ( ( |  ) ) ) )_))__))_)___)
                   ((__)        \\||lll|l||///          \_))
                            (   /(/ (  )  ) )\   )
                          (    ( ( ( | | ) ) )\   )
                           (   /(| / ( )) ) ) )) )
                         (     ( ((((_(|)_)))))     )
                          (      ||\(|(|)|/||     )
                        (        |(||(||)||||        )
                          (     //|/l|||)|\\ \     )
                        (/ / //  /|//||||\\  \ \  \ _)
-------------------------------------------------------------------------------
Node: c6059861e1e9
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/pendulum/parsing/parser.py", line 352, in _parse
    yearfirst=self._options['year_first']
  File "/usr/local/lib/python3.7/site-packages/dateutil/parser/_parser.py", line 1374, in parse
    return DEFAULTPARSER.parse(timestr, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/dateutil/parser/_parser.py", line 649, in parse
    raise ParserError("Unknown string format: %s", timestr)
dateutil.parser._parser.ParserError: Unknown string format: Invalid date
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.7/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/decorators.py", line 121, in wrapper
    return f(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/flask_appbuilder/security/decorators.py", line 109, in wraps
    return f(self, *args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/decorators.py", line 92, in view_func
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/decorators.py", line 56, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/www_rbac/views.py", line 1406, in tree
    base_date = timezone.parse(base_date)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/timezone.py", line 180, in parse
    return pendulum.parse(string, tz=timezone or TIMEZONE)
  File "/usr/local/lib/python3.7/site-packages/pendulum/parser.py", line 75, in parse
    return Parser(**options).parse(text)
  File "/usr/local/lib/python3.7/site-packages/pendulum/parser.py", line 31, in parse
    parsed = super(Parser, self).parse(text)
  File "/usr/local/lib/python3.7/site-packages/pendulum/parsing/parser.py", line 297, in parse
    return self.normalize(self._parse(text))
  File "/usr/local/lib/python3.7/site-packages/pendulum/parsing/parser.py", line 355, in _parse
    raise ParserError('Invalid date string: {}'.format(text))
pendulum.parsing.exceptions.ParserError: Invalid date string: Invalid date
```

**What you expected to happen**:

User should not receive exception pendulum.parsing.exceptions.ParserError: Invalid date string: Invalid date 

**How to reproduce it**:

1. Create a sample dag/example dag with shorter scheduled interval 
2. Have 2-3 dag runs.
3. Navigate to tree view and click on `Go `button
4. Keep clicking on Go button till there are no dag runs remaining, you would get  `pendulum.parsing.exceptions.ParserError: Invalid date string: Invalid date `exception.

**Screenshot**:

![image](https://user-images.githubusercontent.com/60378152/93097661-78904d80-f6c3-11ea-8616-bf7ac5b17293.png)

![image](https://user-images.githubusercontent.com/60378152/93097682-81811f00-f6c3-11ea-941d-0e82e71f198a.png)

![image](https://user-images.githubusercontent.com/60378152/93097729-8e9e0e00-f6c3-11ea-884c-8ec6ba363fa4.png)


How often does this problem occur? Once? Every time etc?

Everytime



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/models/dag.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE:  Celery Executor cannot connect to the database to get information, resulting in a scheduler exit abnormally
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened


We  use Celery Executor where using RabbitMQ as a broker and postgresql as a result backend

Airflow Version: 2.2.3
Celery Version: 5.2.3


apache-airflow-providers-celery==2.1.0



Below is the error message:
```
_The above exception was the direct cause of the following exception: Traceback (most recent call last):
File"/app/airflow2.2.3/airflow/airflow/jobs/schedulerjob.py, line 672, in _execute self._run_scheduler_loop()
File"/app/airflow2.2.3/airflow/airflow/jobs/scheduler_job.py", line 754, in _run_scheduler_loop self.executor.heartbeat()
File"/app/airflow2.2.3/airflow/airflow/executors/base_executor.py, line 168, in heartbeat self.sync()
File"/app/airflow2.2.3/airflow/airflow/executors/celery_executorpy, line 330, in sync self.update_all_task_states()
File"/app/airflow223/airflow/airflow/executors/celery_executor.py,line 442,in update_all_task_states state_and_info_by_celery_task_id=self.bulk_state_fetcher.get_many(self.tasks. values()) File"/app/airflow2.2.3/airflow/airflow/executors/celery_executorpy,line 598, in get_many result = self._get many_from db backend(async_results)
File"/app/airflow2.2.3/airflow/airflow/executors/celery_executor.py,line 618, in get_many_from_db_backend tasks-session.query(task_cls).filter(task_cls.task_id.in(task_ids)).all()
File/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/orm/query.py, line 3373, in all return list(self)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/orm/query.py, line 3535, in iter return self._execute_and_instances(context)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/orm/query.py,line 3556, in _execute_and_instances conn =self._get bind args(
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/salalchemy/orm/query.py, line 3571, in _get_bind_args return fn(
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/orm/query.py,line 3550, in _connection_from_session conn=self.session.connection(**kw)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/orm/session.py, line 1142, in connection return self._connection_for_bind(
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/orm/session.py,line 1150, in _connection_for_bind return self.transaction.connection_for bind(
File/app/airflow2.2.3/airflow2_env/Iib/python3.8/site-packages/sqlalchemy/orm/session.py, line 433, in _connection_for_bind conn=bind._contextual_connect()
File/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/salalchemy/engine/base.py,line 2302, in _contextual_connect self._wrap_pool_connect(self.pool.connect,None),
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/engine/base.py, line 2339, in _wrap_pool_connect
Tracking Connection.handle dbapi_exception_noconnection(
File "/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/engine/base.py line 1583,in handle_dbapi_exception_noconnection util.raise (
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/util/compat.py, line 182, in raise
ents raise exception File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/salalchemy/engine/base.py, line 2336, in _wrap_pool_connect
return fn()

2023-06-05 16:39:05.069 ERROR -Exception when executing SchedulerJob. run scheduler loop Traceback (most recent call last):
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/engine/base.py, line 2336,in _wrap_pool_connect return fno
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/pool/base.py, line 364, in connect returnConnectionFairy.checkout(self)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/pool/base.py, line 778, in _checkout fairy=ConnectionRecordcheckout(pool)
File"/app/airflow2.2.3/airflow2_env/lib/python3.8/site-packages/sqlalchemy/pool/base.py, line 495, in checkout rec=pool. do_get()
File/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/pool/impl.py, line 241, in _do_get return self._createconnection()
File"/app/airflow2.2.3/airflow2_env/lib/python3.8/site-packages/salalchemy/pool/base.py, line 309, in _create_connection return _ConnectionRecord(self)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/sitepackages/sqlalchemy/pool/base.py, line 440, in init self. connect(firstconnectcheck=True)
File"/app/airflow2.2.3/airflow2_env/lib/python3.8/site-packages/sqlalchemy/pool/base.py, line 661, in connect pool.logger.debug"Error onconnect(:%s"e)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/util/langhelpers.py, line 68, in exit compat.raise(
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/salalchemy/util/compat.py", line 182, in raise raise exception
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/pool/base.py, line 656, in _connect connection =pool.invoke_creator(sel f)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/engine/strategies.py, line 114, in connect return dialect.connect(*cargs, **cparans)
File"/app/airflow2.2.3/airflow2_env/1ib/python3.8/site-packages/sqlalchemy/engine/default.py,line 508, in connect return self.dbapi.connect(*cargs, **cparams)
File"/app/airflow2.2.3/airflow2_env/lib/python3.8/site-packages/psycopg2/init.py, line 126, in connect conn=connect(dsnconnection_factory=connection_factory, **kwasync) psycopg2.0perationalError: could not connect to server: Connection timed out
Is the server running on host"xxxxxxxxxxand accepting TCP/IP connections on port 5432?
```
### What you think should happen instead

I think it may be caused by network jitter issues, add retries to solve it

### How to reproduce

celeryExecutor fails to create a PG connection while retrieving metadata information, and it can be reproduced

### Operating System

NAME="RedFlag Asianux"  VERSION="7 (Lotus)"   

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/exampleinclude.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/redirects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py']
Ground Truth : ['a/airflow/executors/celery_executor_utils.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: Remove some more redundant parentheses
Removed redundant parentheses in airflow/_vendor/connexion/apis/flask_api.py and in many more

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/validators.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_sanitizer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/datadog/sensors/datadog.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/datadog/hooks/datadog.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/test_error_handling.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/_common_cli_classes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/appflow.py']
Ground Truth : ['a/airflow/_vendor/connexion/operations/swagger2.py', 'a/airflow/_vendor/connexion/operations/openapi.py', 'a/airflow/_vendor/connexion/apis/flask_api.py', 'a/airflow/_vendor/connexion/decorators/uri_parsing.py']
Current Recall: 0.053892627714897536

=========================================================

ISSUE: The S3ToGCSOperator fails on templated `dest_gcs` URL
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**:


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**: Docker

**What happened**:

When passing a templatized `dest_gcs` argument to the `S3ToGCSOperator` operator, the DAG fails to import because the constructor attempts to test the validity of the URL before the template has been populated in `execute`.

The error is:

```
Broken DAG: [/opt/airflow/dags/bad_gs_dag.py] Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 1051, in gcs_object_is_directory
    _, blob = _parse_gcs_url(bucket)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/hooks/gcs.py", line 1063, in _parse_gcs_url
    raise AirflowException('Please provide a bucket name')
airflow.exceptions.AirflowException: Please provide a bucket name
```

**What you expected to happen**:

The DAG should successfully parse when using a templatized `dest_gcs` value.

**How to reproduce it**:

Instantiating a `S3ToGCSOperator` task with `dest_gcs="{{ var.gcs_url }}"` fails. 

<details>

```python
from airflow.decorators import dag
from airflow.utils.dates import days_ago
from airflow.providers.google.cloud.transfers.s3_to_gcs import S3ToGCSOperator


@dag(
    schedule_interval=None,
    description="Demo S3-to-GS Bug",
    catchup=False,
    start_date=days_ago(1),
)
def demo_bug():

    S3ToGCSOperator(
        task_id="transfer_task",
        bucket="example_bucket",
        prefix="fake/prefix",
        dest_gcs="{{ var.gcs_url }}",
    )


demo_dag = demo_bug()
```
</details>


**Anything else we need to know**:

Should be fixable by moving the code that evaluates whether the URL is a folder to `execute()`.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py']
Ground Truth : ['a/airflow/providers/amazon/aws/transfers/local_to_s3.py', 'a/airflow/providers/google/cloud/transfers/s3_to_gcs.py', 'a/airflow/providers/google/cloud/transfers/azure_fileshare_to_gcs.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: Support airflowignore for plugins
Hello,

Airflow has a mechanism to ignore files before they are automatically loaded by Airflow using a .airflowignore file.
A .airflowignore file specifies the directories or files in DAG_FOLDER that Airflow should intentionally ignore.

> For example, you can prepare a .airflowignore file with contents
> ```
> project_a
> tenant_[\d]
> ```
> Then files like project_a_dag_1.py, TESTING_project_a.py, tenant_1.py, project_a/dag_1.py, and tenant_1/dag_1.py in your DAG_FOLDER would be ignored (If a directorys name matches any of the patterns, this directory and all its subfolders would not be scanned by Airflow at all. This improves efficiency of DAG finding).

More information: https://airflow.readthedocs.io/en/latest/concepts.html?highlight=airflowignore

It would be helpful to make a similar feature available to plugins. This improves the efficiency of plugins finding. 

If anyone is interested in this, I am willing to provide all the necessary tips and information.

Are you wondering how to start contributing to this project? Start by reading our [contributor guide](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)

Cheers


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/plugins/test_plugin_ignore.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/file.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/plugins_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_file.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/docs_build/lint_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/plugins_manager.py', 'a/airflow/utils/file.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: Exception within  LocalTaskJob._run_mini_scheduler_on_child_tasks brakes Sentry Handler
### Apache Airflow version

2.1.3 (latest released)

### Operating System

Debian GNU/Linux 10 (buster)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon @ file:///root/.cache/pypoetry/artifacts/7f/f7/23/fc7fd3543aa486275ef0385c29063ff0dc391b0fc95dc5aa6cab2cf4e5/apache_airflow_providers_amazon-2.2.0-py3-none-any.whl
apache-airflow-providers-celery @ file:///root/.cache/pypoetry/artifacts/14/80/39/0d9d57205da1d24189ac9c18eb3477664ed2c2618c1467c9809b9a2fbf/apache_airflow_providers_celery-2.0.0-py3-none-any.whl
apache-airflow-providers-ftp @ file:///root/.cache/pypoetry/artifacts/a5/13/da/bf14abc40193a1ee1b82bbd800e3ac230427d7684b9d40998ac3684bef/apache_airflow_providers_ftp-2.0.1-py3-none-any.whl
apache-airflow-providers-http @ file:///root/.cache/pypoetry/artifacts/fc/d7/d2/73c89ef847bbae1704fa403d7e92dba1feead757aae141613980db40ff/apache_airflow_providers_http-2.0.0-py3-none-any.whl
apache-airflow-providers-imap @ file:///root/.cache/pypoetry/artifacts/af/5d/de/21c10bfc7ac076a415dcc3fc909317547e77e38c005487552cf40ddd97/apache_airflow_providers_imap-2.0.1-py3-none-any.whl
apache-airflow-providers-postgres @ file:///root/.cache/pypoetry/artifacts/50/27/e0/9b0d8f4c0abf59967bb87a04a93d73896d9a4558994185dd8bc43bb67f/apache_airflow_providers_postgres-2.2.0-py3-none-any.whl
apache-airflow-providers-redis @ file:///root/.cache/pypoetry/artifacts/7d/95/03/5d2a65ace88ae9a9ce9134b927b1e9639c8680c13a31e58425deae55d1/apache_airflow_providers_redis-2.0.1-py3-none-any.whl
apache-airflow-providers-sqlite @ file:///root/.cache/pypoetry/artifacts/ec/e6/a3/e0d81fef662ccf79609e7d2c4e4440839a464771fd2a002d252c9a401d/apache_airflow_providers_sqlite-2.0.1-py3-none-any.whl
```


### Deployment

Other Docker-based deployment

### Deployment details

We are using the Sentry integration

### What happened

An exception within LocalTaskJobs mini scheduler was handled incorrectly by the Sentry integrations 'enrich_errors' method. This is because it assumes its applied to a method of a TypeInstance task

```
TypeError: cannot pickle 'dict_keys' object
  File "airflow/sentry.py", line 166, in wrapper
    return func(task_instance, *args, **kwargs)
  File "airflow/jobs/local_task_job.py", line 241, in _run_mini_scheduler_on_child_tasks
    partial_dag = task.dag.partial_subset(
  File "airflow/models/dag.py", line 1487, in partial_subset
    dag.task_dict = {
  File "airflow/models/dag.py", line 1488, in <dictcomp>
    t.task_id: copy.deepcopy(t, {id(t.dag): dag})  # type: ignore
  File "copy.py", line 153, in deepcopy
    y = copier(memo)
  File "airflow/models/baseoperator.py", line 970, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "copy.py", line 161, in deepcopy
    rv = reductor(4)

AttributeError: 'LocalTaskJob' object has no attribute 'task'
  File "airflow", line 8, in <module>
    sys.exit(main())
  File "airflow/__main__.py", line 40, in main
    args.func(args)
  File "airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "airflow/utils/cli.py", line 91, in wrapper
    return f(*args, **kwargs)
  File "airflow/cli/commands/task_command.py", line 238, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "airflow/cli/commands/task_command.py", line 64, in _run_task_by_selected_method
    _run_task_by_local_task_job(args, ti)
  File "airflow/cli/commands/task_command.py", line 121, in _run_task_by_local_task_job
    run_job.run()
  File "airflow/jobs/base_job.py", line 245, in run
    self._execute()
  File "airflow/jobs/local_task_job.py", line 128, in _execute
    self.handle_task_exit(return_code)
  File "airflow/jobs/local_task_job.py", line 166, in handle_task_exit
    self._run_mini_scheduler_on_child_tasks()
  File "airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "airflow/sentry.py", line 168, in wrapper
    self.add_tagging(task_instance)
  File "airflow/sentry.py", line 119, in add_tagging
    task = task_instance.task
```

### What you expected to happen

The error to be handled correctly and passed on to Sentry without raising another exception within the error handling system

### How to reproduce

In this case we were trying to backfill task for a DAG that at that point had a compilation error. This is quite an edge case yes :-)

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/sql_queries.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py']
Ground Truth : ['a/airflow/sentry.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: UI Doesn't use all of Bootstrap theme css, Airflow 2.0
**Apache Airflow version**: 2.0.0

**Environment**: Ubuntu

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:

Picked Cyborg.css in web_server config but background is still default
![image](https://user-images.githubusercontent.com/23406205/102648950-3f7dd400-411d-11eb-880a-d70e89a5c45c.png)



<!-- (please include exact error messages if you can) -->

**What you expected to happen**:

<!-- What do you think went wrong? -->

**How to reproduce it**:
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/config_templates/default_webserver_config.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: depend_on_past strange behavior
Dear Airflow Maintainers,

my environment is 
- Airflow version: 1.7.0
- Airflow components:  webserver and scheduler with a postgres database and LocalExecutor
- Airflow config

> arallelism = 32
> dag_concurrency = 16
> max_active_runs_per_dag = 16
- Python Version: 2.7.6
- Operating System: Linux ubuntu 3.19.0-25-generic #26~14.04.1-Ubuntu SMP Fri Jul 24 21:16:20 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
- Python packages: 
  iabel==1.3, Flask==0.10.1, Flask-Admin==1.4.0, Flask-Cache==0.13.1, Flask-Login==0.2.11, Flask-WTF==0.12, Jinja2==2.8, Landscape-Client==14.12, Mako==1.0.3, Markdown==2.6.5, MarkupSafe==0.23, PAM==0.4.2, Pygments==2.0.2, SQLAlchemy==1.0.9, Twisted-Core==13.2.0, WTForms==2.0.2, Werkzeug==0.11.2, airflow==1.7.0, alembic==0.8.3, apt-xapian-index==0.45, argparse==1.2.1, cffi==1.3.1, chardet==2.0.1, chartkick==0.4.2, colorama==0.2.5, configobj==4.7.2, croniter==0.3.10, cryptography==1.1.2, dill==0.2.4, enum34==1.1.1, future==0.15.2, gunicorn==19.3.0, html5lib==0.999, idna==2.0, ipaddress==1.0.15, itsdangerous==0.24, numpy==1.10.1, pandas==0.17.1, psycopg2==2.6.1, pyOpenSSL==0.13, pyasn1==0.1.9, pycparser==2.14, pyserial==2.6, python-apt==0.9.3.5ubuntu1, python-dateutil==2.4.2, python-debian==0.1.21-nmu2ubuntu2, python-editor==0.5, python-telegram-bot==3.4, pytz==2015.7, requests==2.2.1, setproctitle==1.1.9, six==1.5.2, ssh-import-id==3.21, thrift==0.9.3, urllib3==1.7.1, vertica-python==0.5.5, wheel==0.24.0, wsgiref==0.1.2, zope.interface==4.0.5

I have the following DAG:

``` python
from airflow import DAG
from airflow.operators import PythonOperator
from datetime import datetime

import logging
import time

default_args = {
    'owner': 'airflow',
    'depends_on_past': True,
    'start_date': datetime(2016, 4, 24),
}

dag_name = 'dp_test'

dag = DAG(
        dag_name,
        default_args=default_args,
        schedule_interval='10 1 * * *')

def cb(**kw):
        time.sleep(10)
        logging.info('Done %s' % kw['ds'])

d = PythonOperator(task_id="delay", provide_context=True, python_callable=cb, dag=dag)
```

It is run by the scheduer the following way:

the first run scheduled__2016-04-24T00:00:00 completes successfully
the second run scheduled__2016-04-24T01:10:00  is marked as running but the task is not run and it keeps hanging in this state.
I tried the same dag with the SequentialExecuter and also from the latest Airflow version from git. The behavior doesn't change.

Another strange thing that bothers me is the run scheduled at 2016-04-24T00:00:00. The dag schedule interval doesn't suggest such run.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_python_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py']
Ground Truth : ['a/airflow/models.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: BigQueryToGCSOperator: Invalid dataset ID error
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

`apache-airflow-providers-google==6.3.0`

### Apache Airflow version

2.2.3

### Operating System

Linux

### Deployment

Composer

### Deployment details

- Composer Environment version: `composer-2.0.3-airflow-2.2.3`


### What happened

When I use  BigQueryToGCSOperator, I got following error.
```
Invalid dataset ID "MY_PROJECT:MY_DATASET". Dataset IDs must be alphanumeric (plus underscores and dashes) and must be at most 1024 characters long.
```

### What you expected to happen

I guess that it is due to I use colon (`:` ) as the separator between project_id and dataset_id in `source_project_dataset_table `.
I tried use dot(`.`) as separator and it worked.
However, [document of BigQueryToGCSOperator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/bigquery_to_gcs/index.html) states that it is possible to use colon as the separator between project_id and dataset_id. In fact, at least untill Airflow1.10.15 version, it also worked with colon separator.
In Airflow 1.10.*, it separate and extract project_id and dataset_id by colon in bigquery hook. But `apache-airflow-providers-google==6.3.0` doesn't have this process.
https://github.com/apache/airflow/blob/d3b066931191b82880d216af103517ea941c74ba/airflow/contrib/hooks/bigquery_hook.py#L2186-L2247

### How to reproduce

You can reproduce following steps.
- Create a test DAG to execute BigQueryToGCSOperator in Composer environment(`composer-2.0.3-airflow-2.2.3`).
- And give `source_project_dataset_table` arg source BigQuery table path in following format.
- Trigger DAG.
```
source_project_dataset_table = 'PROJECT_ID:DATASET_ID.TABLE_ID'
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/bigquery/example_bigquery_transfer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', 'a/airflow/providers/google/cloud/hooks/bigquery.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: AIP-56 - FAB AM - Role views
Move role related views to FAB Auth manager:

- List roles
- Edit role
- Create role
- View role

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_user_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_role_and_permission_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/schemas/role_and_permission_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/cli_commands/sync_perm_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0084_2_1_0_resource_based_permissions_for_default_.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/security/permissions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_acl.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py']
Ground Truth : ['a/airflow/www/fab_security/manager.py', '/dev/null', 'a/airflow/www/security.py', 'a/airflow/www/fab_security/views.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: None configuration variables being converted to 'None'
Unspecified parameters in airflow.cfg are being returned as 'None' instead of None.

For example, try creating an airflow.cfg without a core > plugins_folder configuration and then fire up python

```
>>> from airflow import configuration
>>> configuration.get('core', 'airflow_home')
'/data/airflow'
>>> configuration.get('core', 'plugins_folder')
'None'
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/systems_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/secrets/key_vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/operators/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/secrets/secret_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: Airflow webserver not starting with SQLAlchemy==1.3.16

**Apache Airflow version**: 1.10.9
**Environment**: Ubuntu 18.04 LTS

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):Ubuntu 18.04 LTS

**What happened**: airflow webserver error

airflow@airflow:~$ airflow webserver
[2020-04-08 09:45:49,843] {settings.py:253} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=30494
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2020-04-08 09:45:50,462] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-04-08 09:45:50,463] {dagbag.py:403} INFO - Filling up the DagBag from /home/airflow/airflow/dags
Traceback (most recent call last):
  File "/home/airflow/.local/bin/airflow", line 37, in <module>
    args.func(args)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 75, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/bin/cli.py", line 900, in webserver
    app = cached_app_rbac(None) if settings.RBAC else cached_app(None)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/www/app.py", line 233, in cached_app
    app = create_app(config, testing)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/www/app.py", line 103, in create_app
    models.Chart, Session, name="Charts", category="Data Profiling"))
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_admin/contrib/sqla/view.py", line 330, in __init__
    menu_icon_value=menu_icon_value)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_admin/model/base.py", line 818, in __init__
    self._refresh_cache()
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_admin/model/base.py", line 913, in _refresh_cache
    self._search_supported = self.init_search()
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_admin/contrib/sqla/view.py", line 581, in init_search
    if tools.is_hybrid_property(self.model, name):
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_admin/contrib/sqla/tools.py", line 209, in is_hybrid_property
    return last_name in get_hybrid_properties(last_model)
  File "/home/airflow/.local/lib/python3.6/site-packages/flask_admin/contrib/sqla/tools.py", line 190, in get_hybrid_properties
    for key, prop in inspect(model).all_orm_descriptors.items()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/inspection.py", line 72, in inspect
    "available for object of type %s" % type_
sqlalchemy.exc.NoInspectionAvailable: No inspection system is available for object of type <class 'method'>

**What you expected to happen**: to start

<!-- What do you think went wrong? -->

**How to reproduce it**:
Install airflow with pip3 and postgres from ubuntu which is 10.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/beam/hooks/test_beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py']
Ground Truth : ['a/airflow/models/chart.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: Add autodetect arg in BQCreateExternalTable Operator
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

2.2.4 (latest released)

### Operating System

macOS Monterey

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

Autodetect parameter is missing in the create_external_table function in BQCreateExternalTableOperator because of which one cannot create external tables if schema files are missing
See function on line 1140 in this [file](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/bigquery.py)

### What you expected to happen

If autodetect argument is passed to the create_external_table function then one can create external tables without mentioning the schema for a CSV file leveraging the automatic detection functionality provided by the big query 

### How to reproduce

Simply call BigQueryCreateExternalTableOperator in the dag without mentioning schema_fields or schema_object

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/transfers/copy_into_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/transfers/test_gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataflow.py']
Ground Truth : ['a/airflow/providers/google/cloud/operators/bigquery.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: Increase typing coverage for postgres provider
This PR is about increasing typing coverage for the postgres provider. Part of: #9708

I change this file airflow/providers/postgres/hooks/postgres.py

closes: #9943
related: #9708

---
I go throw this guideline [Step 4: Prepare PR](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#step-4-prepare-pr)


## [Run the unit tests from the IDE or local virtualenv as you see fit.](https://github.com/apache/airflow/blob/master/TESTING.rst#running-unit-tests)

run the command bellow

``` bash
$ pytest tests/providers/postgres/
```
result

``` bash
============================================================ test session starts =============================================================
platform linux -- Python 3.7.0, pytest-6.0.1, py-1.9.0, pluggy-0.13.1 -- /home/salem/.pyenv/versions/3.7.0/envs/airflow-venv-370/bin/python3.7
cachedir: .pytest_cache
rootdir: /home/salem/WorkSpaces/python/airflow, configfile: pytest.ini
plugins: rerunfailures-9.0, forked-1.3.0, flaky-3.7.0, cov-2.10.1, requests-mock-1.8.0, timeouts-1.2.1, instafail-0.4.2, xdist-2.0.0
setup timeout: 0.0s, execution timeout: 0.0s, teardown timeout: 0.0s
collected 20 items                    

tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn PASSED                                            [  5%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_cursor PASSED                                     [ 10%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_from_connection PASSED                            [ 15%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_from_connection_with_schema PASSED                [ 20%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_non_default_id PASSED                             [ 25%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_rds_iam_postgres PASSED                           [ 30%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_rds_iam_redshift PASSED                           [ 35%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_with_invalid_cursor PASSED                        [ 40%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_bulk_dump SKIPPED                                              [ 45%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_bulk_load SKIPPED                                              [ 50%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_copy_expert SKIPPED                                            [ 55%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_insert_rows SKIPPED                                            [ 60%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_insert_rows_replace SKIPPED                                    [ 65%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_insert_rows_replace_missing_replace_index_arg SKIPPED          [ 70%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_insert_rows_replace_missing_target_field_arg SKIPPED           [ 75%]
tests/providers/postgres/hooks/test_postgres.py::TestPostgresHook::test_rowcount SKIPPED                                               [ 80%]
tests/providers/postgres/operators/test_postgres.py::TestPostgres::test_overwrite_schema SKIPPED                                       [ 85%]
tests/providers/postgres/operators/test_postgres.py::TestPostgres::test_postgres_operator_test SKIPPED                                 [ 90%]
tests/providers/postgres/operators/test_postgres.py::TestPostgres::test_postgres_operator_test_multi SKIPPED                           [ 95%]
tests/providers/postgres/operators/test_postgres.py::TestPostgres::test_vacuum SKIPPED                                                 [100%]
============================================================== warnings summary ==============================================================
================================================== 8 passed, 12 skipped, 1 warning in 0.91s ==================================================
``` 

## [Run the tests in Breeze.](https://github.com/apache/airflow/blob/master/TESTING.rst#running-tests-for-a-specified-target-using-breeze-from-the-host)

run the command bellow

``` bash
$ sudo ./breeze tests tests/providers/postgres
```

results

``` bash
============================================================== warnings summary ==============================================================
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn - Failed: Timeout >10.0s
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_cursor - Failed: Timeout >10.0s
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_from_connection - Failed: Timeout >10.0s
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_from_connection_with_schema - Failed: Timeout >1...
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_non_default_id - Failed: Timeout >10.0s
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_rds_iam_postgres - Failed: Timeout >10.0s
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_rds_iam_redshift - Failed: Timeout >10.0s
ERROR tests/providers/postgres/hooks/test_postgres.py::TestPostgresHookConn::test_get_conn_with_invalid_cursor - Failed: Timeout >10.0s
================================================= 12 skipped, 2 warnings, 8 errors in 15.06s =================================================

Regular tests. NOT Updating Quarantine Issue!

```

## [Running static checks](https://github.com/apache/airflow/blob/master/BREEZE.rst#running-static-checks)

I don't know at this time, which static test should I run for this Issue,  but I used the default one as shown below

``` bash
sudo ./breeze static-check mypy
```

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).

---
``` bash
    $ pytest tests/providers/postgres/

    $ sudo ./breeze tests tests/providers/postgres

    $ sudo ./breeze static-check pylint -- --all-files

    $ sudo ./breeze static-check pylint -- --files airflow/providers/postgres/

    $ sudo ./breeze static-check mypy

    $ mypy airflow/providers/postgres/hooks/postgres.py


    $ sudo ./breeze static-check pylint --show-diff-on-failure  --files airflow/providers

    $ sudo pre-commit run pylint --show-diff-on-failure --files airflow/providers

    $ sudo ./breeze static-check mypy -- --files airflow/providers/postgres/

```



``` bash
    sudo ./breeze static-check pylint -- --all-files
    sudo ./breeze static-check pylint -- --files airflow/providers/postgres/

    ./breeze static-check mypy -- --all-files
    sudo ./breeze static-check mypy -- --files airflow/providers/postgres/
```

:heart:

``` bash
pre-commit run
pre-commit run --all-files
pre-commit run mypy --all-files
pre-commit run pylint --all-files
```


```
git pull apache master
```

---
# TODO 

## Test fails on this stages

1.  ~~Static checks: except pylint [details](https://github.com/apache/airflow/pull/10864/checks?check_run_id=1116702359)~~
 > ` ./scripts/ci/static_checks/run_static_checks.sh`
2.  ~~Spell check docs [details](https://github.com/apache/airflow/pull/10864/checks?check_run_id=1116702223)~~
> `./scripts/ci/docs/ci_docs.sh --spellcheck-only`
3. CI Build / Core:Pg9.6,Py3.7 [details](https://github.com/apache/airflow/pull/10864/checks?check_run_id=1123482621)
> `./scripts/ci/testing/ci_run_airflow_testing.sh`


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_tests.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/testing_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/postgres/hooks/test_postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_mypy_folder.py']
Ground Truth : ['a/airflow/providers/postgres/hooks/postgres.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: tests/core/test_providers_manager.py::TestProviderManager::test_hooks broken on 3.6
### Apache Airflow version

main (development)

### Operating System

Any

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### What happened

`tests/core/test_providers_manager.py::TestProviderManager::test_hooks` is broken on 3.6 by #18209 since newer `importlib-resources` versions uses a different implementation and the mocks no longer works. This was actually visible before merging; all and only 3.6 checks in the PR failed. Lets be more careful identifying CI failure patterns in the future  

Not exactly sure how to fix yet. I believe the breaking changes were introduced in [importlib-resources 5.2](https://github.com/python/importlib_resources/blob/v5.2.2/CHANGES.rst#v520), but restricting `<5.2` is not a long-term fix since the same version is also in the Python 3.10 stdlib and will bite us again very soon.

### What you expected to happen

_No response_

### How to reproduce

```
$ ./breeze tests -- tests/core/test_providers_manager.py::TestProviderManager::test_hooks
...
tests/core/test_providers_manager.py F                                                                  [100%]

================================================== FAILURES ==================================================
_______________________________________ TestProviderManager.test_hooks _______________________________________

self = <test_providers_manager.TestProviderManager testMethod=test_hooks>
mock_import_module = <MagicMock name='import_module' id='139679504564520'>

    @patch('airflow.providers_manager.importlib.import_module')
    def test_hooks(self, mock_import_module):
        # Compat with importlib_resources
        mock_import_module.return_value.__spec__ = Mock()
        with pytest.warns(expected_warning=None) as warning_records:
            with self._caplog.at_level(logging.WARNING):
>               provider_manager = ProvidersManager()

tests/core/test_providers_manager.py:123:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
airflow/providers_manager.py:242: in __init__
    self._provider_schema_validator = _create_provider_info_schema_validator()
airflow/providers_manager.py:92: in _create_provider_info_schema_validator
    schema = json.loads(importlib_resources.read_text('airflow', 'provider_info.schema.json'))
/usr/local/lib/python3.6/site-packages/importlib_resources/_legacy.py:46: in read_text
    with open_text(package, resource, encoding, errors) as fp:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

package = 'airflow', resource = 'provider_info.schema.json', encoding = 'utf-8', errors = 'strict'

    def open_text(
        package: Package,
        resource: Resource,
        encoding: str = 'utf-8',
        errors: str = 'strict',
    ) -> TextIO:
        """Return a file-like object opened for text reading of the resource."""
>       return (_common.files(package) / _common.normalize_path(resource)).open(
            'r', encoding=encoding, errors=errors
        )
E       TypeError: unsupported operand type(s) for /: 'Mock' and 'str'
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_tests.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: A dag's schedule interval can no longer be an instance of dateutils.relativedelta
### Apache Airflow version

2.2.1 (latest released)

### Operating System

debian

### Versions of Apache Airflow Providers

apache-airflow==2.2.1
apache-airflow-providers-amazon==2.3.0
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==6.0.0
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-jira==2.0.1
apache-airflow-providers-mysql==2.1.1
apache-airflow-providers-postgres==2.3.0
apache-airflow-providers-redis==2.0.1
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.2.0

### Deployment

Other Docker-based deployment

### Deployment details

Dask executor, custom-built Docker images, postgres 12.7 backend

### What happened

I upgraded Airflow from 2.0.2 to 2.2.1, and some DAGs I have that used dateutils.relativedelta objects as schedule intervals stopped running

### What you expected to happen

The [code](https://github.com/apache/airflow/blob/2.2.1/airflow/models/dag.py#L101) for the schedule_interval parameter of the DAG constructor indicates that a relativedelta object is allowed, so I expected the DAG to be correctly parsed and scheduled.

### How to reproduce

Create a DAG that has a relativedelta object as its schedule interval, and it will not appear in the UI or be scheduled.

### Anything else

Here is the code that causes the failure within the PR where it was introduced: [link](https://github.com/apache/airflow/pull/17414/files#diff-ed37fe966e8247e0bfd8aa28bc2698febeec3807df5f5a00545ca80744f8aff6R267)

Here are the logs for the exception, found in the scheduler logs for the file that contains the offending DAG
<details><pre>
ERROR   | {dagbag.py:528} - 'relativedelta' object has no attribute 'total_seconds'
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 515, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 298, in process_file
    found_dags = self._process_modules(filepath, mods, file_last_changed_on_disk)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 401, in _process_modules
    dag.timetable.validate()
  File "/usr/local/lib/python3.9/site-packages/airflow/timetables/interval.py", line 274, in validate
    if self._delta.total_seconds() <= 0:
AttributeError: 'relativedelta' object has no attribute 'total_seconds'
</pre></details>

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py']
Ground Truth : ['a/airflow/timetables/interval.py']
Current Recall: 0.05460640358927298

=========================================================

ISSUE: Specify that exit code -9 is due to RAM
Related to https://github.com/apache/airflow/issues/9655

It would be nice to add a message when you get this error with some info, like 'This probably is because a lack of RAM' or something like that. 

I have found the code where the -9 is assigned but have no idea how to add a logging message. 

        self.process = None

        if self._rc is None:
            # Something else reaped it before we had a chance, so let's just "guess" at an error code.
            self._rc = -9

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/logging_mixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/container_instances.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py']
Ground Truth : ['a/airflow/task/task_runner/standard_task_runner.py']
Current Recall: 0.05674773121239932

=========================================================

ISSUE: BaseOperator type hints for retry_delay and max_retry_delay should reveal float option
### Describe the issue with documentation

`BaseOperator` type hints for `retry_delay` and `max_retry_delay` shows `timedelta` only, however the params also accept `float` seconds values.
Also, type hint for `dag` param is missing.
More precise type hints and params descriptions in the docs can help to understand the code behavior easier.

### How to solve the problem

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_prediction_summary.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py']
Ground Truth : ['a/airflow/models/baseoperator.py']
Current Recall: 0.05674773121239932

=========================================================

ISSUE: IntegrityError inserting into task_fail table with null execution_date from TI.handle_failure_with_callback
### Apache Airflow version

2.2.0 (latest released)

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon @ file:///root/.cache/pypoetry/artifacts/c9/69/16/ffa2eb7a2e6e850a7048eaf66b6c40c990ef7c58149f20d3d3f333a2e9/apache_airflow_providers_amazon-2.2.0-py3-none-any.whl                                                                                                                                                                                         
apache-airflow-providers-celery @ file:///root/.cache/pypoetry/artifacts/6e/1b/2f/f968318a7474e979af4dc53893ecafe8cd11a98a94077a9c3c27304eb7/apache_airflow_providers_celery-2.1.0-py3-none-any.whl                                                                                                                                                                                         
apache-airflow-providers-ftp @ file:///root/.cache/pypoetry/artifacts/8b/9a/dd/79a36c62bc7f37f98d0ea33652570e19272e8a7a2297db13a6785698d1/apache_airflow_providers_ftp-2.0.1-py3-none-any.whl 
apache-airflow-providers-http @ file:///root/.cache/pypoetry/artifacts/52/28/81/03a89147daf7daceb55f1218189d1c4af01c33c45849b568769ca6765f/apache_airflow_providers_http-2.0.1-py3-none-any.whl                                                                                                                                                                                             
apache-airflow-providers-imap @ file:///root/.cache/pypoetry/artifacts/1c/5d/c5/269e8a8098e7017a26a2a376eb3020e1a864775b7ff310ed39e1bd503d/apache_airflow_providers_imap-2.0.1-py3-none-any.whl                                                                                                                                                                                             
apache-airflow-providers-postgres @ file:///root/.cache/pypoetry/artifacts/fb/69/ac/e8e25a0f6a4b0daf162c81c9cfdbb164a93bef6bd652c1c00eee6e0815/apache_airflow_providers_postgres-2.3.0-py3-none-any.whl                                                                                                                                                                                     
apache-airflow-providers-redis @ file:///root/.cache/pypoetry/artifacts/cf/2b/56/75563b6058fe45b70f93886dd92541e8349918eeea9d70c703816f2639/apache_airflow_providers_redis-2.0.1-py3-none-any.whl                                                                                                                                                                                           
apache-airflow-providers-sqlite @ file:///root/.cache/pypoetry/artifacts/61/ba/e9/c0b4b7ef2599dbd902b32afc99f2620d8a616b3072122e90f591de4807/apache_airflow_providers_sqlite-2.0.1-py3-none-any.whl  
```

### Deployment

Other Docker-based deployment

### Deployment details

AWS ECS, Celery Executor, Postgres 13, S3 Logging, Sentry integration

### What happened

Noticed our Sentry getting a lot of integrity errors inserting into the task_fail table with a null execution date.

This seemed to be caused specifically by zombie task failures (We use AWS ECS Spot instances).

Specifically this callback from the dag file processor:

https://github.com/apache/airflow/blob/e6c56c4ae475605636f4a1b5ab3884383884a8cf/airflow/models/taskinstance.py#L1746

Adds a task_fail here: https://github.com/apache/airflow/blob/e6c56c4ae475605636f4a1b5ab3884383884a8cf/airflow/models/taskinstance.py#L1705

This blows up when it flushes further down the method. This i believe is because when the task instance is refreshed from the database the `self.dag_run` property is not populated. The proxy from `ti.execution_date` to `ti.dag_run.execution_date` then returns `None` causing our `NOT NULL` violation.

### What you expected to happen

Insert into task_fail successfully and trigger callback

### How to reproduce

Run this dag:

```python
import logging
import time
from datetime import datetime

from airflow import DAG
from airflow.operators.python import PythonOperator


def long_running_task():
    for i in range(60):
        time.sleep(5)
        logging.info("Slept for 5")


def log_failure_dag(*args, **kwargs):
    logging.error("Our failure callback")


dag = DAG(
    dag_id="test_null_task_fail",
    schedule_interval='@daily',
    catchup=True,
    start_date=datetime(2021, 10, 9),
    max_active_runs=1,
    max_active_tasks=1,
    on_failure_callback=log_failure_dag,
)

with dag:
    PythonOperator(
        task_id="long_running",
        python_callable=long_running_task,
        on_failure_callback=log_failure_dag
    )
```

Kill the celery worker whilst its executing the long_running tasks. Wait for the zombie reaper of the scheduler to begin and call the failure handler.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/utils/db.py', 'a/airflow/__init__.py', 'a/airflow/migrations/versions/7b2661a43ba3_taskinstance_keyed_to_dagrun.py', 'a/airflow/__main__.py', 'a/setup.py', 'a/airflow/models/xcom.py', 'a/airflow/sentry.py', 'a/airflow/utils/configuration.py', 'a/airflow/cli/commands/dag_command.py', 'a/airflow/www/views.py', 'a/airflow/models/dag.py', 'a/airflow/models/param.py', 'a/airflow/_vendor/connexion/spec.py', 'a/airflow/settings.py', 'a/airflow/task/task_runner/base_task_runner.py', 'a/airflow/models/dagrun.py', 'a/airflow/timetables/interval.py', 'a/airflow/cli/cli_parser.py', 'a/airflow/jobs/__init__.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/jobs/scheduler_job.py']
Current Recall: 0.05674773121239932

=========================================================

ISSUE: POST /api/v1/users fails with exception
### Apache Airflow version

main (development)

### Operating System

From Astronomers QA team

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

When adding a new user, The following exception is emitted:

```
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.9/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.9/site-packages/airflow/_vendor/connexion/decorators/decorator.py", line 48, in wrapper
    response = function(request)
  File "/usr/local/lib/python3.9/site-packages/airflow/_vendor/connexion/decorators/uri_parsing.py", line 144, in wrapper
    response = function(request)
  File "/usr/local/lib/python3.9/site-packages/airflow/_vendor/connexion/decorators/validation.py", line 184, in wrapper
    response = function(request)
  File "/usr/local/lib/python3.9/site-packages/airflow/_vendor/connexion/decorators/response.py", line 103, in wrapper
    response = function(request)
  File "/usr/local/lib/python3.9/site-packages/airflow/_vendor/connexion/decorators/parameter.py", line 121, in wrapper
    return function(**kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/api_connexion/security.py", line 47, in decorated
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/api_connexion/endpoints/user_endpoint.py", line 105, in post_user
    user.roles.extend(roles_to_add)
AttributeError: 'bool' object has no attribute 'roles'
```

The immediate cause to this exception is F.A.B. returns `False` when it fails to add a new user. The problem, however, is _why_ excactly it failed. This is the payload used:

```json
{
    "username": "username6",
    "password": "password1",
    "email": "username5@example.com",
    "first_name": "user2",
    "last_name": "test1",
    "roles":[{"name":"Admin"},{"name":"Viewer"}]
}
```

This went through validation, therefore we know

1. The POST-ing user has permission to create a new user.
2. The format is correct (including the nested roles).
3. There is not already an existing `username6` in the database.
4. All listed roles exist.

(All these are already covered by unit tests.)

Further complicating the issue is F.A.B.s security manager swallows an exception when this happens, and only logs the exception to the server. And were having trouble locating that line of log. Its quite difficult to diagnose further, so Im posting this hoping someone has better luck reproducing this.

I will submit a fix to correct the immediate issue, making the API emit 500 with something like Failed to create user for unknown reason to make the failure _slightly_ less confusing.

### What you expected to happen

_No response_

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py']
Ground Truth : ['a/airflow/api_connexion/endpoints/user_endpoint.py']
Current Recall: 0.05674773121239932

=========================================================

ISSUE: ExasolHook get_pandas_df does not return pandas dataframe but None

When calling the exasol hooks get_pandas_df function (https://github.com/apache/airflow/blob/main/airflow/providers/exasol/hooks/exasol.py) I noticed that it does not return a pandas dataframe. It returns None. In fact the function definition type hint explicitly states that None is returned. But the name of the function suggests otherwise. The name get_pandas_df implies that it should return a dataframe and not None.

I think that it would make more sense if get_pandas_df would indeed return a dataframe as the name is alluring to. So the code should be like this:

`def get_pandas_df(self, sql: Union[str, list], parameters: Optional[dict] = None, **kwargs) -> pd.DataFrame:
... some code ...
with closing(self.get_conn()) as conn:
df=conn.export_to_pandas(sql, query_params=parameters, **kwargs)
return df`

INSTEAD OF:

`def get_pandas_df(self, sql: Union[str, list], parameters: Optional[dict] = None, **kwargs) -> None:
... some code ...
with closing(self.get_conn()) as conn:
conn.export_to_pandas(sql, query_params=parameters, **kwargs)`

**Apache Airflow version**: 2.1.0


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): Not using Kubernetes

**Environment**:Official Airflow-Docker Image

- **Cloud provider or hardware configuration**: no cloud - docker host (DELL Server with 48 Cores, 512GB RAM and many TB storage)
- **OS** (e.g. from /etc/os-release):Official Airflow-Docker Image on CentOS 7 Host
- **Kernel** (e.g. `uname -a`): Linux cad18b35be00 3.10.0-1160.21.1.el7.x86_64 #1 SMP Tue Mar 16 18:28:22 UTC 2021 x86_64 GNU/Linux
- **Install tools**: only docker
- **Others**:

**What happened**:
You can replicate the findings with following dag file:

import datetime

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.providers.exasol.operators.exasol import ExasolHook
import pandas as pd


default_args = {"owner": "airflow"}


def call_exasol_hook(**kwargs):
    #Make connection to Exasol
    hook = ExasolHook(exasol_conn_id='Exasol QA')
    sql = 'select 42;'    
    df = hook.get_pandas_df(sql = sql) 
    return df
    
with DAG(
    dag_id="exasol_hook_problem",
    start_date=datetime.datetime(2021, 5, 5),
    schedule_interval="@once",
    default_args=default_args,
    catchup=False,
) as dag:
      
    set_variable = PythonOperator(
        task_id='call_exasol_hook',
        python_callable=call_exasol_hook
    )

Sorry for the strange code formatting. I do not know how to fix this in the github UI form. 
Sorry also in case I missed something.
 
When testing or executing the task via CLI:
` airflow tasks test exasol_hook_problem call_exasol_hook 2021-07-20`

the logs show:
`[2021-07-21 12:53:19,775] {python.py:151} INFO - Done. Returned value was: None`

None was returned - although get_pandas_df was called. A pandas df should have been returned instead.



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/exasol/hooks/exasol.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/sql_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/base_sql_to_slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/presto/hooks/presto.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/sql_to_slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/trino/hooks/trino.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py']
Ground Truth : ['a/airflow/providers/exasol/hooks/exasol.py']
Current Recall: 0.058889058835525664

=========================================================

ISSUE: API Endpoint - CRUD - DAG
Hello 

We need to create several endpoints that perform basic CRUD operations on **DAG**. We need the following endpoints:

- PATCH /dags/{dag_id}

It depends on https://github.com/apache/airflow/issues/8128

Detailed information is available in the issue:
https://github.com/apache/airflow/issues/8118

LOVE,

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_extra_links.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataprep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/non_caching_file_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/natural_language/example_natural_language.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/conftest.py']
Ground Truth : ['a/airflow/api_connexion/schemas/common_schema.py', 'a/airflow/api_connexion/endpoints/dag_endpoint.py', 'a/airflow/api_connexion/schemas/dag_schema.py']
Current Recall: 0.058889058835525664

=========================================================

ISSUE: secrets_masker RecursionError with nested TriggerDagRunOperators
**Apache Airflow version**: 2.1.0


**Environment**: tested on Windows docker-compose envirnoment and on k8s (both with celery executor).

**What happened**:

```
[2021-06-16 07:56:32,682] {taskinstance.py:1481} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1137, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/trigger_dagrun.py", line 134, in execute
    replace_microseconds=False,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/experimental/trigger_dag.py", line 123, in trigger_dag
    replace_microseconds=replace_microseconds,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/experimental/trigger_dag.py", line 48, in _trigger_dag
    dag = dag_bag.get_dag(dag_id)  # prefetch dag if it is stored serialized
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 186, in get_dag
    self._add_dag_from_db(dag_id=dag_id, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 252, in _add_dag_from_db
    dag = row.dag
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/serialized_dag.py", line 175, in dag
    dag = SerializedDAG.from_dict(self.data)  # type: Any
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py", line 792, in from_dict
    return cls.deserialize_dag(serialized_obj['dag'])
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py", line 716, in deserialize_dag
    v = {task["task_id"]: SerializedBaseOperator.deserialize_operator(task) for task in v}
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py", line 716, in <dictcomp>
    v = {task["task_id"]: SerializedBaseOperator.deserialize_operator(task) for task in v}
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py", line 493, in deserialize_operator
    op_predefined_extra_links = cls._deserialize_operator_extra_links(v)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py", line 600, in _deserialize_operator_extra_links
    if _operator_link_class_path in get_operator_extra_links():
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py", line 86, in get_operator_extra_links
    _OPERATOR_EXTRA_LINKS.update(ProvidersManager().extra_links_class_names)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers_manager.py", line 400, in extra_links_class_names
    self.initialize_providers_manager()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers_manager.py", line 129, in initialize_providers_manager
    self._discover_all_providers_from_packages()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers_manager.py", line 151, in _discover_all_providers_from_packages
    log.debug("Loading %s from package %s", entry_point, package_name)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1366, in debug
    self._log(DEBUG, msg, args, **kwargs)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1514, in _log
    self.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1524, in handle
    self.callHandlers(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 1586, in callHandlers
    hdlr.handle(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 890, in handle
    rv = self.filter(record)
  File "/usr/local/lib/python3.7/logging/__init__.py", line 751, in filter
    result = f.filter(record)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 157, in filter
    record.__dict__[k] = self.redact(v)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in redact
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in <genexpr>
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in redact
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in <genexpr>
    return tuple(self.redact(subval) for subval in item)
  ....
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in redact
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in <genexpr>
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in redact
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 203, in <genexpr>
    return tuple(self.redact(subval) for subval in item)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/secrets_masker.py", line 201, in redact
    elif isinstance(item, (tuple, set)):
RecursionError: maximum recursion depth exceeded in __instancecheck__ 
```

**What you expected to happen**:
I think new masker is not able to handle TriggerDagRunOperator running dag with TriggerDagRunOperator

**How to reproduce it**:
```
from datetime import datetime, timedelta

from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

def pprint(**kwargs):
    print(1)

with DAG("test",
         catchup=False,
         max_active_runs=1,
         start_date=datetime(2021, 1, 1),
         is_paused_upon_creation=False,
         schedule_interval=None) as dag:

    task_observe_pea_data = PythonOperator(
        task_id="test_task",
        python_callable=pprint,
        provide_context=True
    )

with DAG("test_1",
         catchup=False,
         max_active_runs=1,
         start_date=datetime(2021, 1, 1),
         is_paused_upon_creation=False,
         schedule_interval=None) as dag:

    task_observe_pea_data = TriggerDagRunOperator(
        task_id="test_trigger_1",
        trigger_dag_id="test"
    )

with DAG("test_2",
         catchup=False,
         max_active_runs=1,
         start_date=datetime(2021, 1, 1),
         is_paused_upon_creation=False,
         schedule_interval=None) as dag:

    task_observe_pea_data = TriggerDagRunOperator(
        task_id="test_trigger_2",
        trigger_dag_id="test_1"
    )
```

**Anything else we need to know**:

How often does this problem occur? Every time
I have tried hide_sensitive_var_conn_fields=False but error still occurs.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/openlineage/utils/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py']
Ground Truth : ['a/airflow/utils/log/secrets_masker.py']
Current Recall: 0.061030386458652004

=========================================================

ISSUE: scheduler gets stuck without a trace


**Apache Airflow version**:


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:
**What happened**:

The scheduler gets stuck without a trace or error. When this happens, the CPU usage of scheduler service is at 100%. No jobs get submitted and everything comes to a halt. Looks it goes into some kind of infinite loop. 
The only way I could make it run again is by manually restarting the scheduler service. But again, after running some tasks it gets stuck. I've tried with both Celery and Local executors but same issue occurs. I am using the -n 3 parameter while starting scheduler. 

Scheduler configs,
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
executor = LocalExecutor
parallelism = 32

Please help. I would be happy to provide any other information needed

**What you expected to happen**:


**How to reproduce it**:


**Anything else we need to know**:

Moved here from https://issues.apache.org/jira/browse/AIRFLOW-401
    

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/_cron.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py']
Ground Truth : ['a/airflow/utils/dag_processing.py']
Current Recall: 0.061030386458652004

=========================================================

ISSUE: Add a flag to list all Upgrade Check Rules
It would be nice to have a list of all the available uprade-checks rules:

```
 airflow upgrade_check --help
usage: airflow upgrade_check [-h] [-s SAVE] [-i IGNORE] [-c CONFIG]

optional arguments:
  -h, --help            show this help message and exit
  -s SAVE, --save SAVE  Saves the result to the indicated file. The file
                        format is determined by the file extension.
  -i IGNORE, --ignore IGNORE
                        Ignore a rule. Can be used multiple times.
  -c CONFIG, --config CONFIG
                        Path to upgrade check config yaml file.
```

example (Ideally it should have 2 columns: Check_ID / Class Name and Summary):

```
 airflow upgrade_check --list

- Check for latest versions of apache-airflow and checker
- Remove airflowAirflowMacroPlugin class
- Chain between DAG and operator not allowed
- Connectionconn_id is not unique
- Connectionconn_type is not nullable
- Ensure users are not using custom metaclasses in custom operators
- Fernet is enabled by default
- GCP service account key deprecation
- Unify hostname_callable option in core section
- Changes in import paths of hooks, operators, sensors and others
- Users must delete deprecated configs for KubernetesExecutor
- Legacy UI is deprecated by default
- Logging configuration has been moved to new section
- Removal of Mesos Executor
- No additional argument allowed in BaseOperator
- Users must set a kubernetespod_template_file value
- SendGrid email uses old airflowcontrib module
- Changes in import path of remote task handlers
- Jinja Template Variables cannot be undefined
```

Related Slack conversation: https://apache-airflow.slack.com/archives/CCQB40SQJ/p1609259241092000

This allows users to know the progress of their checks and an easy way to ignore a check beforehand (if we list an id / class name too, example `PodTemplateFileRule`).

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py']
Ground Truth : ['a/airflow/upgrade/checker.py', 'a/airflow/upgrade/rules/__init__.py']
Current Recall: 0.061030386458652004

=========================================================

ISSUE: is not bound to a Session; lazy load operation of attribute 'dag_model' 
### Apache Airflow version

2.2.3 (latest released)

### What happened

 An error occurred while browsing the page (/rendered-k8s?dag_id=xxx&task_id=yyy&execution_date=zzz):
```
Parent instance <TaskInstance at 0x7fb0b01df940> is not bound to a Session; lazy load operation of attribute 'dag_model' cannot proceed (Background on this error at: http://sqlalche.me/e/13/bhk3) Traceback (most recent call last):
    args=self.command_as_list(),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 527, in command_as_list
    dag = self.dag_model
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py", line 294, in __get__
    return self.impl.get(instance_state(instance), dict_)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/attributes.py", line 730, in get
    value = self.callable_(state, passive)
  File "/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/strategies.py", line 717, in _load_for_state
    raise orm_exc.DetachedInstanceError(
sqlalchemy.orm.exc.DetachedInstanceError: Parent instance <TaskInstance at 0x7fb0b01df940> is not bound to a Session; lazy load operation of attribute 'dag_model' cannot proceed (Background on this error at: http://sqlalche.me/e/13/bhk3)
```


### What you expected to happen

_No response_

### How to reproduce

_No response_

### Operating System

NAME="CentOS Stream" VERSION="8"

### Versions of Apache Airflow Providers

```
apache-airflow-providers-cncf-kubernetes==2.2.0
apache-airflow-providers-elasticsearch==2.0.3
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-mysql==2.1.1
apache-airflow-providers-postgres==2.3.0
apache-airflow-providers-sftp==2.1.1
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.2.0
```

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.061030386458652004

=========================================================

ISSUE: Scheduler Pickling - conflict with 'session' parameter defined in both args and kwargs
Dear Airflow Maintainers,

Before I tell you about my issue, let me describe my environment:
# Environment
- **Version of Airflow:** master (1db892b)
- **Example code to reproduce the bug:** Using airflow cli: `airflow scheduler -p`
- **Stack trace:**

```
[2016-03-28 19:11:19,952] {jobs.py:656} ERROR - pickle() got multiple values for keyword argument 'session'
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/airflow-1.6.2-py2.7.egg/airflow/jobs.py", line 653, in _execute
    self.process_dag(dag, executor)
  File "/usr/local/lib/python2.7/dist-packages/airflow-1.6.2-py2.7.egg/airflow/jobs.py", line 459, in process_dag
    pickle_id = dag.pickle(session).id
  File "/usr/local/lib/python2.7/dist-packages/airflow-1.6.2-py2.7.egg/airflow/utils.py", line 143, in wrapper
    result = func(*args, **kwargs)
TypeError: pickle() got multiple values for keyword argument 'session'
```
- **Operating System:** Ubuntu 14.04
- **Python Version:** 2.7.6

Now that you know a little about me, let me tell you about the issue I am having:

The Airflow Scheduler is unable to pickle DAGs using `airflow scheduler -p`. The reason is as follows:

For each DAG, the pickle-enabled scheduler attempts to pickle the DAG:

```
def process_dag(self, dag, executor):  # shortened for brevity 
        session = settings.Session()

        # picklin'
        pickle_id = None
        if self.do_pickle and self.executor.__class__ not in (
                executors.LocalExecutor, executors.SequentialExecutor):
            pickle_id = dag.pickle(session).id
```

Here is the header for the `pickle` function of the DAG class:

```
@provide_session
def pickle(self, session=None):
```

Note that `pickle`
1. has an optional argument named 'session' which is used as a positional argument in `process_dag`
2. uses a decorator function `provide_session`, which is seen below

```
def provide_session(func):  # shortened for brevity
    @wraps(func)
    def wrapper(*args, **kwargs):
        needs_session = False
        if 'session' not in kwargs:  # args is not checked!
            needs_session = True
            session = settings.Session()
            kwargs['session'] = session
        result = func(*args, **kwargs)
        if needs_session:
            session.expunge_all()
            session.commit()
            session.close()
        return result
    return wrapper
```

Because `session` is used as a positional argument in `process_dag`, the `provide_session` wrapper does not find a 'session' argument among the keyword arguments and therefore provides one. When the interior function is called, the positional and keyword arguments are reconciled and the two respective `session`s collide to produce the error shown above.

For reference, see [this stack overflow post](http://stackoverflow.com/questions/21764770/typeerror-got-multiple-values-for-argument)
- **What did you expect to happen?** No errors, DAGs should be scheduled normally.
- **What happened instead?** An error with aforementioned stack trace.
## Reproduction Steps
1. Modify airflow.cfg to use `MesosExecutor`
2. from the CLI, input `airflow scheduler -p`


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/session.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py']
Ground Truth : ['a/airflow/utils/db.py']
Current Recall: 0.061030386458652004

=========================================================

ISSUE: Exception within  LocalTaskJob._run_mini_scheduler_on_child_tasks brakes Sentry Handler
### Apache Airflow version

2.1.3 (latest released)

### Operating System

Debian GNU/Linux 10 (buster)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon @ file:///root/.cache/pypoetry/artifacts/7f/f7/23/fc7fd3543aa486275ef0385c29063ff0dc391b0fc95dc5aa6cab2cf4e5/apache_airflow_providers_amazon-2.2.0-py3-none-any.whl
apache-airflow-providers-celery @ file:///root/.cache/pypoetry/artifacts/14/80/39/0d9d57205da1d24189ac9c18eb3477664ed2c2618c1467c9809b9a2fbf/apache_airflow_providers_celery-2.0.0-py3-none-any.whl
apache-airflow-providers-ftp @ file:///root/.cache/pypoetry/artifacts/a5/13/da/bf14abc40193a1ee1b82bbd800e3ac230427d7684b9d40998ac3684bef/apache_airflow_providers_ftp-2.0.1-py3-none-any.whl
apache-airflow-providers-http @ file:///root/.cache/pypoetry/artifacts/fc/d7/d2/73c89ef847bbae1704fa403d7e92dba1feead757aae141613980db40ff/apache_airflow_providers_http-2.0.0-py3-none-any.whl
apache-airflow-providers-imap @ file:///root/.cache/pypoetry/artifacts/af/5d/de/21c10bfc7ac076a415dcc3fc909317547e77e38c005487552cf40ddd97/apache_airflow_providers_imap-2.0.1-py3-none-any.whl
apache-airflow-providers-postgres @ file:///root/.cache/pypoetry/artifacts/50/27/e0/9b0d8f4c0abf59967bb87a04a93d73896d9a4558994185dd8bc43bb67f/apache_airflow_providers_postgres-2.2.0-py3-none-any.whl
apache-airflow-providers-redis @ file:///root/.cache/pypoetry/artifacts/7d/95/03/5d2a65ace88ae9a9ce9134b927b1e9639c8680c13a31e58425deae55d1/apache_airflow_providers_redis-2.0.1-py3-none-any.whl
apache-airflow-providers-sqlite @ file:///root/.cache/pypoetry/artifacts/ec/e6/a3/e0d81fef662ccf79609e7d2c4e4440839a464771fd2a002d252c9a401d/apache_airflow_providers_sqlite-2.0.1-py3-none-any.whl
```


### Deployment

Other Docker-based deployment

### Deployment details

We are using the Sentry integration

### What happened

An exception within LocalTaskJobs mini scheduler was handled incorrectly by the Sentry integrations 'enrich_errors' method. This is because it assumes its applied to a method of a TypeInstance task

```
TypeError: cannot pickle 'dict_keys' object
  File "airflow/sentry.py", line 166, in wrapper
    return func(task_instance, *args, **kwargs)
  File "airflow/jobs/local_task_job.py", line 241, in _run_mini_scheduler_on_child_tasks
    partial_dag = task.dag.partial_subset(
  File "airflow/models/dag.py", line 1487, in partial_subset
    dag.task_dict = {
  File "airflow/models/dag.py", line 1488, in <dictcomp>
    t.task_id: copy.deepcopy(t, {id(t.dag): dag})  # type: ignore
  File "copy.py", line 153, in deepcopy
    y = copier(memo)
  File "airflow/models/baseoperator.py", line 970, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "copy.py", line 161, in deepcopy
    rv = reductor(4)

AttributeError: 'LocalTaskJob' object has no attribute 'task'
  File "airflow", line 8, in <module>
    sys.exit(main())
  File "airflow/__main__.py", line 40, in main
    args.func(args)
  File "airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "airflow/utils/cli.py", line 91, in wrapper
    return f(*args, **kwargs)
  File "airflow/cli/commands/task_command.py", line 238, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "airflow/cli/commands/task_command.py", line 64, in _run_task_by_selected_method
    _run_task_by_local_task_job(args, ti)
  File "airflow/cli/commands/task_command.py", line 121, in _run_task_by_local_task_job
    run_job.run()
  File "airflow/jobs/base_job.py", line 245, in run
    self._execute()
  File "airflow/jobs/local_task_job.py", line 128, in _execute
    self.handle_task_exit(return_code)
  File "airflow/jobs/local_task_job.py", line 166, in handle_task_exit
    self._run_mini_scheduler_on_child_tasks()
  File "airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "airflow/sentry.py", line 168, in wrapper
    self.add_tagging(task_instance)
  File "airflow/sentry.py", line 119, in add_tagging
    task = task_instance.task
```

### What you expected to happen

The error to be handled correctly and passed on to Sentry without raising another exception within the error handling system

### How to reproduce

In this case we were trying to backfill task for a DAG that at that point had a compilation error. This is quite an edge case yes :-)

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

Chart version 1.1.0 does not gracefully shutdown workers
### Official Helm Chart version

1.1.0 (latest released)

### Apache Airflow version

2.1.3 (latest released)

### Kubernetes Version

1.19.13

### Helm Chart configuration

```yaml
executor: "CeleryExecutor"
workers:
  # Number of airflow celery workers in StatefulSet
  replicas: 1
  # Below is the default value, it does not work
  command: ~
  args:
     - "bash"
     - "-c"
     - |-
       exec \
       airflow celery worker
```

### Docker Image customisations

```dockerfile
FROM apache/airflow:2.1.3-python3.7

ENV AIRFLOW_HOME=/opt/airflow

USER root

RUN set -ex \
    && buildDeps=' \
        python3-dev \
        libkrb5-dev \
        libssl-dev \
        libffi-dev \
        build-essential \
        libblas-dev \
        liblapack-dev \
        libpq-dev \
        gcc \
        g++ \
    ' \
    && apt-get update -yqq \
    && apt-get upgrade -yqq \
    && apt-get install -yqq --no-install-recommends \
        $buildDeps \
        libsasl2-dev \
        libsasl2-modules \
        apt-utils \
        curl \
        vim \
        rsync \
        netcat \
        locales \
        sudo \
        patch \
        libpq5 \
    && apt-get autoremove -yqq  --purge\
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

USER airflow

COPY --chown=airflow:root requirements*.txt /tmp/

RUN pip install -U pip setuptools wheel cython \
    && pip install -r /tmp/requirements_providers.txt \
    && pip install -r /tmp/requirements.txt

COPY --chown=airflow:root setup.py /tmp/custom_operators/
COPY --chown=airflow:root custom_operators/ /tmp/custom_operators/custom_operators/

RUN pip install /tmp/custom_operatos

COPY --chown=airflow:root entrypoint*.sh /
COPY --chown=airflow:root config/ ${AIRFLOW_HOME}/config/
COPY --chown=airflow:root airflow.cfg ${AIRFLOW_HOME}/
COPY --chown=airflow:root dags/ ${AIRFLOW_HOME}/dags
```

### What happened

Using CeleryExecutor whenever I kill a worker pod that is running a task with `kubectl delete pod` or a `helm upgrade` the pod gets instantly killed and does not wait for the task to finish or the end of terminationGracePeriodSeconds.

### What you expected to happen

I expect the worker to finish all it's tasks inside the grace period before being killed

Killing the pod when it's running a task throws this
```bash
k logs -f airflow-worker-86d78f7477-rjljs

 * Serving Flask app "airflow.utils.serve_logs" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
[2021-09-07 16:26:25,612] {_internal.py:113} INFO -  * Running on http://0.0.0.0:8793/ (Press CTRL+C to quit)
/home/airflow/.local/lib/python3.7/site-packages/celery/platforms.py:801 RuntimeWarning: You're running the worker with superuser privileges: this is
absolutely not recommended!

Please specify a different user using the --uid option.

User information: uid=50000 euid=50000 gid=0 egid=0

[2021-09-07 16:28:11,021: WARNING/ForkPoolWorker-1] Running <TaskInstance: test-long-running.long-long 2021-09-07T16:28:09.148524+00:00 [queued]> on host airflow-worker-86d78f7477-rjljs

worker: Warm shutdown (MainProcess)
[2021-09-07 16:28:32,919: ERROR/MainProcess] Process 'ForkPoolWorker-2' pid:20 exited with 'signal 15 (SIGTERM)'
[2021-09-07 16:28:32,930: ERROR/MainProcess] Process 'ForkPoolWorker-1' pid:19 exited with 'signal 15 (SIGTERM)'
[2021-09-07 16:28:33,183: ERROR/MainProcess] Task handler raised error: WorkerLostError('Worker exited prematurely: signal 15 (SIGTERM) Job: 0.')
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/worker/worker.py", line 208, in start
    self.blueprint.start(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/bootsteps.py", line 119, in start
    step.start(parent)
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/bootsteps.py", line 369, in start
    return self.obj.start()
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/worker/consumer/consumer.py", line 318, in start
    blueprint.start(self)
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/bootsteps.py", line 119, in start
    step.start(parent)
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/worker/consumer/consumer.py", line 599, in start
    c.loop(*c.loop_args())
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/worker/loops.py", line 83, in asynloop
    next(loop)
  File "/home/airflow/.local/lib/python3.7/site-packages/kombu/asynchronous/hub.py", line 303, in create_loop
    poll_timeout = fire_timers(propagate=propagate) if scheduled else 1
  File "/home/airflow/.local/lib/python3.7/site-packages/kombu/asynchronous/hub.py", line 145, in fire_timers
    entry()
  File "/home/airflow/.local/lib/python3.7/site-packages/kombu/asynchronous/timer.py", line 68, in __call__
    return self.fun(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/kombu/asynchronous/timer.py", line 130, in _reschedules
    return fun(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/worker/consumer/gossip.py", line 167, in periodic
    for worker in values(workers):
  File "/home/airflow/.local/lib/python3.7/site-packages/kombu/utils/functional.py", line 109, in _iterate_values
    for k in self:
  File "/home/airflow/.local/lib/python3.7/site-packages/kombu/utils/functional.py", line 95, in __iter__
    def __iter__(self):
  File "/home/airflow/.local/lib/python3.7/site-packages/celery/apps/worker.py", line 285, in _handle_request
    raise exc(exitcode)
celery.exceptions.WorkerShutdown: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/billiard/pool.py", line 1267, in mark_as_worker_lost
    human_status(exitcode), job._job),
billiard.exceptions.WorkerLostError: Worker exited prematurely: signal 15 (SIGTERM) Job: 0.

 -------------- celery@airflow-worker-86d78f7477-rjljs v4.4.7 (cliffs)
--- ***** -----
-- ******* ---- Linux-5.4.129-63.229.amzn2.x86_64-x86_64-with-debian-10.10 2021-09-07 16:26:26
- *** --- * ---
- ** ---------- [config]
- ** ---------- .> app:         airflow.executors.celery_executor:0x7ff517d78d90
- ** ---------- .> transport:   redis://:**@airflow-redis:6379/0
- ** ---------- .> results:     postgresql+psycopg2://airflow:**@db-host:5432/airflow
- *** --- * --- .> concurrency: 2 (prefork)
-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
--- ***** -----
 -------------- [queues]
                .> default          exchange=default(direct) key=default
```

### How to reproduce

Run a dag with this airflow configuration
```yaml
executor: "CeleryExecutor"
workers:
  replicas: 1
  command: ~
  args:
    - "bash"
    - "-c"
    # The format below is necessary to get `helm lint` happy
    - |-
      exec \
      airflow celery worker
```

and kill the worker pod

### Anything else

Overwriting the official entrypoint seems to solve the issue

```yaml
workers:
  # To gracefully shutdown workers I have to overwrite the container entrypoint
  command: ["airflow"]
  args: ["celery", "worker"]
```

When the worker gets killed another worker pod comes online and the old one stays in status `Terminating`, all new tasks go to the new worker.

Below are the logs when the worker gets killed
 
```bash
k logs -f airflow-worker-5ff95df84f-fznk7
 * Serving Flask app "airflow.utils.serve_logs" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
[2021-09-07 16:42:42,399] {_internal.py:113} INFO -  * Running on http://0.0.0.0:8793/ (Press CTRL+C to quit)
/home/airflow/.local/lib/python3.7/site-packages/celery/platforms.py:801 RuntimeWarning: You're running the worker with superuser privileges: this is
absolutely not recommended!

Please specify a different user using the --uid option.

User information: uid=50000 euid=50000 gid=0 egid=0

[2021-09-07 16:42:53,133: WARNING/ForkPoolWorker-1] Running <TaskInstance: test-long-running.long-long 2021-09-07T16:28:09.148524+00:00 [queued]> on host airflow-worker-5ff95df84f-fznk7

worker: Warm shutdown (MainProcess)

 -------------- celery@airflow-worker-5ff95df84f-fznk7 v4.4.7 (cliffs)
--- ***** -----
-- ******* ---- Linux-5.4.129-63.229.amzn2.x86_64-x86_64-with-debian-10.10 2021-09-07 16:42:43
- *** --- * ---
- ** ---------- [config]
- ** ---------- .> app:         airflow.executors.celery_executor:0x7f69aaa90d50
- ** ---------- .> transport:   redis://:**@airflow-redis:6379/0
- ** ---------- .> results:     postgresql+psycopg2://airflow:**@db-host:5432/airflow
- *** --- * --- .> concurrency: 2 (prefork)
-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
--- ***** -----
 -------------- [queues]
                .> default          exchange=default(direct) key=default


rpc error: code = Unknown desc = Error: No such container: efe5ce470f5bd5b7f84479c1a8f5dc1d5d92cb1ad6b16696fa5a1ca9610602ee%
```
There is no timestamp but it waits for the task to finish before writing `worker: Warm shutdown (MainProcess)`



Another option I tried was using this as the entrypoint and it also works

```bash
#!/usr/bin/env bash

handle_worker_term_signal() {
    # Remove worker from queue
    celery -b $AIRFLOW__CELERY__BROKER_URL -d celery@$HOSTNAME control cancel_consumer default

    while [ $(airflow jobs check --hostname $HOSTNAME | grep "Found one alive job." | wc -l) -eq 1 ]; do
        echo 'Finishing jobs!'
        airflow jobs check --hostname $HOSTNAME --limit 100 --allow-multiple
        sleep 60
    done
    echo 'All jobs finished! Terminating worker'

    kill $pid
    exit 0
}

trap handle_worker_term_signal SIGTERM

airflow celery worker &
pid="$!"

wait $pid
```

Got the idea from this post: https://medium.com/flatiron-engineering/upgrading-airflow-with-zero-downtime-8df303760c96


Thanks!

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py']
Ground Truth : ['a/setup.py', 'a/airflow/models/taskinstance.py', 'a/airflow/www/security.py', 'a/airflow/configuration.py', 'a/docs/build_docs.py', 'a/docs/exts/operators_and_hooks_ref.py', 'a/airflow/sentry.py', '/dev/null', 'a/airflow/api/common/experimental/delete_dag.py', 'a/airflow/www/views.py', 'a/airflow/www/auth.py', 'a/docs/conf.py', 'a/docs/exts/exampleinclude.py', 'a/airflow/api_connexion/endpoints/task_instance_endpoint.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dag.py', 'a/airflow/models/dagrun.py']
Current Recall: 0.061030386458652004

=========================================================

ISSUE: Celery executor should support custom database backend models
Currently, the celery executor imports the `Task` model for the Celery database backend directly:

https://github.com/apache/airflow/blob/2dd4e96045d4a7f45cc8c06df3d25c4f1479392c/airflow/executors/celery_executor.py#L38

However, Celery, being highly customisable, uses `self.task_cls` in the backend implementation; this defaults to either `Task` or `TaskExtended` depending on the Celery configuration, see [`celery.backends.database:66`](https://github.com/celery/celery/blob/406f04a082949ac42ec7a4af94fed896c515aaa4/celery/backends/database/__init__.py#L66):

> ```python
>     task_cls = Task
> ```

and [`celery.backends.database:76,77`](https://github.com/celery/celery/blob/406f04a082949ac42ec7a4af94fed896c515aaa4/celery/backends/database/__init__.py#L76-L77):

> ```python
>         if self.extended_result:
>             self.task_cls = TaskExtended
> ```

Airflow should support this flexibility, and instead of importing a fixed class just use `backend.task_cls`, so in `_get_many_from_db_backend` just use:

```python
        Task = app.backend.task_cls
        with session_cleanup(session):
            tasks = session.query(Task).filter(Task.task_id.in_(task_ids)).all()
```

I suppose there is one downside to this: `TaskExtended` fetches a few more columns per task, but given that the result of `task.to_dict()` is passed to `backend.meta_from_decoded()`, which could also have been customised to make use of information contained in a custom model, that's neither here nor there. It would be the price anyone using the `result_extended` option would need to pay elsewhere anyway.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/integration/executors/test_celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/celery/executors/test_celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/default_celery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py']
Ground Truth : ['a/airflow/executors/celery_executor.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: `retry_exponential_backoff` algorithm does not account for case when `retry_delay` is zero
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 2.1.2


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:
When `retry_exponential_backoff` is enabled and `retry_interval` is inadvertently set to zero, a divide by zero error occurs in the `modded_hash` calculation of the exponential backoff algorithm, causing the scheduler to crash.

<!-- (please include exact error messages if you can) -->

**What you expected to happen**:
Scheduler should treat it as a task with `retry_delay` of zero.

<!-- What do you think went wrong? -->

**How to reproduce it**:
Create a task with `retry_delay=timedelta()` and `retry_exponential_backoff=True`.
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:
Willing to submit a PR; opened #17003 (WIP) with possible fix.

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Handle TODO: .first() is not None can be changed to .scalar()
### Body

The TODO part of:
https://github.com/apache/airflow/blob/d67ac5932dabbf06ae733fc57b48491a8029b8c2/airflow/models/serialized_dag.py#L156-L158

can now be addressed since we are on sqlalchemy 1.4+ and https://github.com/sqlalchemy/sqlalchemy/issues/5481 is resolved


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/expandinput.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py']
Ground Truth : ['a/airflow/models/serialized_dag.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: MongoToS3Operator failed when running with a single query (not aggregate pipeline)
**Apache Airflow version**: 2.0.2

**What happened**:

`MongoToS3Operator` failed when running with a single query (not aggregate pipeline):

```sh
Traceback (most recent call last):
  File "/home/airflow//bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow//lib/python3.8/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/home/airflow//lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow//lib/python3.8/site-packages/airflow/utils/cli.py", line 89, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow//lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 385, in task_test
    ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)
  File "/home/airflow//lib/python3.8/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow//lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1413, in run
    self._run_raw_task(
  File "/home/airflow//lib/python3.8/site-packages/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow//lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1138, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow//lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1311, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow//lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1341, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow//lib/python3.8/site-packages/airflow/providers/amazon/aws/transfers/mongo_to_s3.py", line 116, in execute
    results = MongoHook(self.mongo_conn_id).find(
  File "/home/airflow//lib/python3.8/site-packages/airflow/providers/mongo/hooks/mongo.py", line 144, in find
    return collection.find(query, **kwargs)
  File "/home/airflow//lib/python3.8/site-packages/pymongo/collection.py", line 1523, in find
    return Cursor(self, *args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'allowDiskUse'
```

**What you expected to happen**:

I expect the data from MongoDB to be exported to a file in S3 with no errors.

**How to reproduce it**:

Run the following operator with a single  `mongo_query` (no aggregate pipeline):

```python
export_to_s3 = MongoToS3Operator(
    task_id='export_to_s3',
    mongo_conn_id=Variable.get('mongo_conn_id'),
    s3_conn_id=Variable.get('aws_conn_id'),
    mongo_collection='my_mongo_collection',
    mongo_query={},
    s3_bucket=Variable.get('s3_bucket'),
    s3_key="my_data.json",
    replace=True,
    dag=dag,
)
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/mongo_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py']
Ground Truth : ['a/airflow/providers/amazon/aws/transfers/mongo_to_s3.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Airflow database upgrade fails with "psycopg2.errors.NotNullViolation: column "map_index" of relation "task_instance" contains null value"s
### Apache Airflow version

main (development)

### What happened

I currently have Airflow 2.2.3 and due to this [issue](https://github.com/apache/airflow/issues/19699) I have tried to upgrade Airflow to this [commit](https://github.com/apache/airflow/commit/14ee831c7ad767e31a3aeccf3edbc519b3b8c923).

When I run `airflow db upgrade` I get the following error:
```
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 587bdf053233 -> e655c0453f75, Add TaskMap and map_index on TaskInstance.
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/usr/local/lib/python3.7/dist-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.NotNullViolation: column "map_index" of relation "task_instance" contains null values
```

The map_index column was introduced with this [PR](https://github.com/apache/airflow/pull/20286).

Could you please advise? 

### What you expected to happen

_No response_

### How to reproduce

_No response_

### Operating System

Ubuntu 18.04.6 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

Kubernetes

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py']
Ground Truth : ['a/airflow/models/taskreschedule.py', 'a/airflow/migrations/versions/e655c0453f75_add_taskmap_and_map_id_on_taskinstance.py', 'a/airflow/models/taskinstance.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Migrations fail to apply when there are null run_id's in the dag_run table
### Apache Airflow version

2.2.0b2 (beta snapshot)

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon @ file:///root/.cache/pypoetry/artifacts/c9/69/16/ffa2eb7a2e6e850a7048eaf66b6c40c990ef7c58149f20d3d3f333a2e9/apache_airflow_providers_amazon-2.2.0-py3-none-any.whl
apache-airflow-providers-celery @ file:///root/.cache/pypoetry/artifacts/6e/1b/2f/f968318a7474e979af4dc53893ecafe8cd11a98a94077a9c3c27304eb7/apache_airflow_providers_celery-2.1.0-py3-none-any.whl
apache-airflow-providers-ftp @ file:///root/.cache/pypoetry/artifacts/8b/9a/dd/79a36c62bc7f37f98d0ea33652570e19272e8a7a2297db13a6785698d1/apache_airflow_providers_ftp-2.0.1-py3-none-any.whl
apache-airflow-providers-http @ file:///root/.cache/pypoetry/artifacts/52/28/81/03a89147daf7daceb55f1218189d1c4af01c33c45849b568769ca6765f/apache_airflow_providers_http-2.0.1-py3-none-any.whl
apache-airflow-providers-imap @ file:///root/.cache/pypoetry/artifacts/1c/5d/c5/269e8a8098e7017a26a2a376eb3020e1a864775b7ff310ed39e1bd503d/apache_airflow_providers_imap-2.0.1-py3-none-any.whl
apache-airflow-providers-postgres @ file:///root/.cache/pypoetry/artifacts/fb/69/ac/e8e25a0f6a4b0daf162c81c9cfdbb164a93bef6bd652c1c00eee6e0815/apache_airflow_providers_postgres-2.3.0-py3-none-any.whl
apache-airflow-providers-redis @ file:///root/.cache/pypoetry/artifacts/cf/2b/56/75563b6058fe45b70f93886dd92541e8349918eeea9d70c703816f2639/apache_airflow_providers_redis-2.0.1-py3-none-any.whl
apache-airflow-providers-sqlite @ file:///root/.cache/pypoetry/artifacts/61/ba/e9/c0b4b7ef2599dbd902b32afc99f2620d8a616b3072122e90f591de4807/apache_airflow_providers_sqlite-2.0.1-py3-none-any.whl
```

### Deployment

Other Docker-based deployment

### Deployment details

AWS ECS + Fargate, RabbitMQ, Celery executor, Postgres 13 backend

### What happened

Hey so we just hit an issue with the 2.2.0b2 migration `airflow/migrations/versions/7b2661a43ba3_taskinstance_keyed_to_dagrun.py`
 . We have dag_runs without a run_id (as it is still nullable) on the dag_run table. This caused the FK constraint on TI not to apply.
Noticeably this wasn't caught by https://github.com/apache/airflow/blob/ca93ff4ef05e68369b4fd42fcda3b40f63682d4a/airflow/utils/db.py#L700 before the migrations apply. Is it worth adding an additional check here and making dag_run.run_id NOT NULL?

### What you expected to happen

The data check before applying migrations should have spotted the NULL run_ids on the dag_run table. Going forward perhaps we can make this column NOT NULL?

### How to reproduce

NULL some run_id columns on the dag_run table. As to how this occurs I am not sure.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0093_2_2_0_taskinstance_keyed_to_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/dbt/cloud/operators/dbt.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py']
Ground Truth : ['a/airflow/migrations/versions/7b2661a43ba3_taskinstance_keyed_to_dagrun.py', 'a/airflow/utils/db.py', 'a/airflow/models/taskinstance.py', 'a/airflow/models/dagrun.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Unable to fetch CloudWatch Logs of previous run attempts
### Apache Airflow version

2.7.0

### What happened

After upgrading to `apache-airflow-providers-amazon==8.5.1`, I am no longer able to view logs from previous run attempts. 

Airflow is able to find the log stream successfully, but there's no content viewable (even though there are logs in the actual streams):
```
REDACTED.us-west-2.compute.internal
*** Reading remote log from Cloudwatch log_group: REDACTED log_stream: dag_id=REDACTED/run_id=REDACTED/task_id=REDACTED/attempt=1.log.
```

I believe this issue occurred from #33231 -  Looking at the [CloudWatch Logs code](https://github.com/apache/airflow/blob/providers-amazon/8.5.1/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py#L109-L133), I think `task_instance.start_date` and `task_instance.end_date` somehow refer to its __latest__ run, and so the log contents are getting filtered out on previous attempts.


### What you think should happen instead

_No response_

### How to reproduce

1. Configure environment with remote CloudWatch logging
2. Run task
3. Clear task and re-run
4. The logs for the first attempt now no longer show

### Operating System

Debian 11

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.5.1

### Deployment

Other

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py']
Ground Truth : ['a/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Can't clear all upstream tasks when DAG Serialization is enabled

**Apache Airflow version**: 1.10.10



**To Reproduce**
Steps to reproduce the behavior with Astronomer Image:
1. Enable DAG Serialization (Put `AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True`
Once the Webserver boots up, run any DAG
1. Once the DAG completes execution, click on the task that has few upstream tasks in Graph View
1. One the **Task Instance Modal**, select **Upstream** -> click on **Clear**. You will see a list of tasks but that would not have all the upstream tasks.

**Screenshots**
![Apr-22-2020 15-56-14](https://user-images.githubusercontent.com/8811558/79997805-d96b7f00-84b1-11ea-8328-9d24c4a77ad6.gif)




Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/mark_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_experimental/common/test_mark_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/skipmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/providers/google/cloud/example_dags/example_gcs.py', 'a/airflow/serialization/serialized_objects.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Limit all google-cloud api to <2.0.0 as it introduces breaking changes
**Description**

Google Cloud APIs introduced breaking changes in 2.0.0 (https://github.com/googleapis/python-container/blob/master/UPGRADING.md) and they already caused a number of changes. We should (for now - before we migrate to 2.0+ ) limit all our google-cloud deps to <2.0.0

**Use case / motivation**

Make sure that further releases of google cloud APIs are not breaking our builds.



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/compute.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Don't crash when a getpass.getuser() call fails
**Apache Airflow version**: 1.10.11


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): 1.18.4

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): Ubuntu 20.04
- **Kernel** (e.g. `uname -a`): 
- **Install tools**:
- **Others**:

**What happened**:

There are many call occurrences to `getpass.getuser()` in the code. None are fenced with a try/except to protect them from failures. Though, in a containerized environment, using a process with an uid that is not defined in `/etc/passwd` is not uncommon when using a existing image. This makes airflow crash with errors like the one below, though the call to `getpass.getuser()` serves no critical purposes (it is sometimes used to annotate metrics...) AFAICT.

```
Traceback (most recent call last):
  File "/usr/local/bin/airflow", line 37, in <module>
    args.func(args)
  File "/usr/local/lib/python3.8/site-packages/airflow/utils/cli.py", line 76, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/airflow/bin/cli.py", line 1189, in scheduler
    job = jobs.SchedulerJob(
  File "<string>", line 4, in __init__
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/state.py", line 433, in _initialize_instance
    manager.dispatch.init_failure(self, args, kwargs)
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.raise_(
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 178, in raise_
    raise exception
  File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/state.py", line 430, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/usr/local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 387, in __init__
    super(SchedulerJob, self).__init__(*args, **kwargs)
  File "<string>", line 6, in __init__
  File "/usr/local/lib/python3.8/site-packages/airflow/jobs/base_job.py", line 92, in __init__
    self.unixname = getpass.getuser()
  File "/usr/local/lib/python3.8/getpass.py", line 169, in getuser
    return pwd.getpwuid(os.getuid())[0]
KeyError: 'getpwuid(): uid not found: 1000'
```
I think i many cases it could be simply replaced with:
```python
try:
    <var> = getpass.getuser()
except KeyError:
    <var> = str(os.getuid())
```


**What you expected to happen**:

I expected Airflow to not crash because the uid is not mapped to a user defined in `/etc/passwd`

**How to reproduce it**:

Deploy the official airflow image in docker with `--user=9999` (or any other uid that is not 50000) or in k8s with a security context like:
```
securityContext:
  runAsUser: 9999
```
**Anything else we need to know**:

It even looks like the value returned by `getuser()` is hardly used, e.g. jobs see their unixname attribute assigned the value, but use of that attribute seem hardly critical:
```
 grep -HInr "\.unixname" "${VIRTUAL_ENV}"/lib/python3.8/site-packages/airflow{,_exporter}
airflow/migrations/versions/e3a246e0dc1_current_schema.py:105:            sa.Column('unixname', sa.String(length=1000), nullable=True),
airflow/migrations/versions/e3a246e0dc1_current_schema.py:166:            sa.Column('unixname', sa.String(length=1000), nullable=True),
airflow/migrations/versions/6e96a59344a4_make_taskinstance_pool_not_nullable.py:71:    unixname = Column(String(1000))
airflow/jobs/base_job.py:67:    unixname = Column(String(1000))
airflow/jobs/base_job.py:92:        self.unixname = getpass.getuser()
airflow/www/views.py:2809:        'unixname', 'hostname', 'start_date', 'end_date', 'latest_heartbeat')
airflow/www/views.py:3015:        'unixname', 'priority_weight', 'queue', 'queued_dttm', 'try_number',
airflow/www_rbac/views.py:2491:                    'executor_class', 'hostname', 'unixname']
airflow/www_rbac/views.py:2494:                      'hostname', 'unixname']
airflow/www_rbac/views.py:2689:                    'unixname', 'priority_weight', 'queue', 'queued_dttm', 'try_number',
airflow/models/taskinstance.py:157:    unixname = Column(String(1000))
airflow/models/taskinstance.py:201:        self.unixname = getpass.getuser()
airflow/models/taskinstance.py:487:            self.unixname = ti.unixname
```
(there is a `run_as_user` attribute that is)

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ssh/hooks/ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py']
Ground Truth : ['a/airflow/providers/ssh/hooks/ssh.py', 'a/airflow/task/task_runner/cgroup_task_runner.py', 'a/airflow/task/task_runner/base_task_runner.py', 'a/airflow/models/taskinstance.py', 'a/airflow/utils/cli.py', 'a/airflow/utils/platform.py', 'a/airflow/providers/microsoft/winrm/hooks/winrm.py', 'a/airflow/jobs/base_job.py', 'a/airflow/cli/commands/info_command.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Make sure that local installation works with installing airflow + all providers.
We have to double-check that providers work and can be tested locally when installing airflow in a local virtualenv.

After we've changed to separate providers and core package, pip install . will not install those packages, but with local -e it will likely work as providers packages are in the same folder. 

We still need to check how local virtualenv work when we install without -e though.

Most likely solution will be dynamic selection of packages in setup.py based on the fact whether you are installing airflow from packages or sources. 

This is connected with #11464 


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/install_airflow_and_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/reinstall.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Ensure all tests that create TaskInstances in DB also create DagRuns
As part of the work on AIP-39 we want to make it so that _every_ TaskInstance in the DB has DagRun.

Now at "run time" this is already the case (baring the case where someone might delete a DagRun. Rare, now only possible by direct DB? I can't remember if Browse -> DagRuns allows deleting anymore or not) but in our tests this is far from the case

There are countless examples of where the tests just directly create TaskInstances, without a matching DagRun.

We need to fix that so that the tests reflect reality.

- [x] https://github.com/apache/airflow/pull/16889 is part of this -- creating a pytest fixture to reduce amount of duplication in tests
- [x] #17556
- [ ] We probably also want to enforce foreign key constraints between DagRun and TaskInstance once we've done this (see also https://github.com/apache/airflow/pull/17030) 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/skipmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/ti_deps/deps/runnable_exec_date_dep.py', '/dev/null', 'a/airflow/models/taskreschedule.py', 'a/airflow/jobs/local_task_job.py', 'a/airflow/www/auth.py', 'a/airflow/kubernetes/pod_generator.py', 'a/airflow/ti_deps/deps/not_previously_skipped_dep.py', 'a/airflow/api/common/experimental/mark_tasks.py', 'a/airflow/api_connexion/schemas/task_instance_schema.py', 'a/airflow/www/decorators.py', 'a/airflow/sentry.py', 'a/airflow/www/views.py', 'a/airflow/cli/commands/dag_command.py', 'a/airflow/sensors/smart_sensor.py', 'a/airflow/jobs/backfill_job.py', 'a/airflow/api_connexion/endpoints/task_instance_endpoint.py', 'a/airflow/models/dag.py', 'a/airflow/cli/commands/task_command.py', 'a/airflow/api_connexion/endpoints/log_endpoint.py', 'a/airflow/kubernetes/kubernetes_helper_functions.py', 'a/airflow/dag_processing/processor.py', 'a/airflow/ti_deps/deps/dagrun_id_dep.py', 'a/airflow/models/dagrun.py', 'a/airflow/ti_deps/dep_context.py', 'a/airflow/utils/callback_requests.py', 'a/airflow/models/skipmixin.py', 'a/airflow/ti_deps/deps/dagrun_exists_dep.py', 'a/airflow/models/baseoperator.py', 'a/airflow/executors/kubernetes_executor.py', 'a/airflow/cli/commands/kubernetes_command.py', 'a/airflow/ti_deps/deps/trigger_rule_dep.py', 'a/airflow/providers/google/cloud/operators/bigquery.py', 'a/airflow/www/utils.py', 'a/airflow/sensors/base.py', 'a/airflow/jobs/scheduler_job.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Error on logging empty line to Cloudwatch
**Apache Airflow version**: 2.0.1

**Environment**:

- **Cloud provider or hardware configuration**: AWS

**What happened**:
I have Airflow with Cloudwatch-based remote logging running. I also have `BashOperator` that does, for example, `rsync` with invalid parameters, for example `rsync -av test test`. The output of the `rsync` error is formatted and contains empty line. Once that empty line is logged to the Cloudwatch, i receive an error:
```
2021-04-06 19:29:22,318] /home/airflow/.local/lib/python3.6/site-packages/watchtower/__init__.py:154 WatchtowerWarning: Failed to deliver logs: Parameter validation failed:
Invalid length for parameter logEvents[5].message, value: 0, valid range: 1-inf
[2021-04-06 19:29:22,320] /home/airflow/.local/lib/python3.6/site-packages/watchtower/__init__.py:158 WatchtowerWarning: Failed to deliver logs: None
```

So basically empty lines can't be submitted to the Cloudwatch and as result the whole output of the process doesn't appear in logs.

**What you expected to happen**:

I expect to have an output of the bash command in logs. Empty lines can be skipped or replaced with something.

**How to reproduce it**:

For example: run `BashOperator` with `rsync` command that fails on Airflow with Cloudwatch-based remote logging. It could be any other command that produces empty line in the output.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py']
Ground Truth : ['a/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', 'a/setup.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: DagRuns are created beyond dag end_date
To replicate, setup a simple dag as follows

``` python
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators import DummyOperator

delta = timedelta(hours=1)
runs = 3
start_date = datetime(2015, 1, 1)
end_date = start_date + (runs - 1) * delta

dag = DAG(
    'test-dag',
    start_date=start_date,
    end_date=end_date,
    schedule_interval=delta,
)

task = DummyOperator(
    task_id='test-operator',
    owner='someone',
    dag=dag,
)
```

Refreshing the DagRuns view periodically reveals an endlessly increasing list of DagRuns even though and `end_date` was specified.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_core.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/dates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_trigger_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_time_delta.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py']
Ground Truth : ['a/airflow/models.py', 'a/airflow/jobs.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Integration list synchronization with airflow-site
Hello friends.

Today I am coming to you with another ticket. This time, while working on the website, I discovered that we have some information that is slightly outdated.

[`landing-pages/site/static/integrations.json`](https://github.com/apache/airflow-site/blob/master/landing-pages/site/static/integrations.json
) file have lists all the integrations that have been added to Airflow, but unfortunately it is updated manually.  Recently, new services were added there on Oct 2019

We can think of a tool that will update this list more automatically by reading the integration list from [provider.yaml](https://github.com/apache/airflow/find/master) files, and then creating an integration.json file that can be published to the airflow-site.

For now, we need a manual run script, but in the future, we can also run it on Github Action or during updates and documentation.

CC: @ryw 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py']
Ground Truth : ['a/scripts/ci/pre_commit/pre_commit_check_provider_yaml_files.py', '/dev/null']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: CLI: `airflow dags test { dag w/ schedule_interval=None } ` error: "No run dates were found"
### Apache Airflow version

2.2.0b2 (beta snapshot)

### Operating System

ubuntu 20.04

### Versions of Apache Airflow Providers

n/a

### Deployment

Virtualenv installation

### Deployment details

pip install /path/to/airflow/src

### What happened

Given any DAG initialized with: `schedule_interval=None`

Run `airflow dags test mydagname $(date +%Y-%m-%d)` and get an error:

```
INFO - No run dates were found for the given dates and dag interval.
```

This behavior changed in https://github.com/apache/airflow/pull/15397, it used to trigger a backfill dagrun at the given date.

### What you expected to happen

I expected a backfill dagrun with the given date, regardless of whether it fit into the `schedule_interval`.

If AIP-39 made that an unrealistic expectation, then I'd hope for some way to define unscheduled dags which can still be tested from the command line (which, so far as I know, is the fastest way to iterate on a DAG.).

As it is, I keep changing `schedule_interval` back and forth depending on whether I want to iterate via `astro dev start` (which tolerates `None` but does superfluous work if the dag is scheduled) or via `airflow dags test ...` (which doesn't tolerate `None`).

### How to reproduce

Initialize a DAG with: `schedule_interval=None` and run it via `airflow dags test mydagname $(date +%Y-%m-%d)`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py']
Ground Truth : ['a/airflow/jobs/backfill_job.py', 'a/airflow/cli/commands/dag_command.py', 'a/airflow/models/dag.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: [AIRFLOW-726] Adding Vnomics to Airflow users
Dear Airflow Maintainers,

Please accept this PR that addresses the following issues:
- https://issues.apache.org/jira/browse/AIRFLOW-726

Per Apache guidelines you need to create a [Jira issue](https://issues.apache.org/jira/browse/AIRFLOW/).

Testing Done:
- Unittests are required, if you do not include new unit tests please
specify why you think this is not required. We like to improve our
coverage so a non existing test is even a better reason to include one.

Reminders for contributors (REQUIRED!):
* Your PR's title must reference an issue on 
[Airflow's JIRA](https://issues.apache.org/jira/browse/AIRFLOW/). 
For example, a PR called "[AIRFLOW-1] My Amazing PR" would close JIRA 
issue #1. Please open a new issue if required!

* For all PRs with UI changes, you must provide screenshots. If the UI changes are not obvious, either annotate the images or provide before/after screenshots.

* Please squash your commits when possible and follow the [How to write a good git commit message](http://chris.beams.io/posts/git-commit/). 
Summarized as follows:
  1. Separate subject from body with a blank line
  2. Limit the subject line to 50 characters
  3. Do not end the subject line with a period
  4. Use the imperative mood in the subject line (add, not adding)
  5. Wrap the body at 72 characters
  6. Use the body to explain what and why vs. how



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/minor_release_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['/dev/null', 'a/airflow/settings.py', 'a/airflow/operators/sensors.py', 'a/airflow/executors/base_executor.py', 'a/airflow/jobs.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: current_state method of TaskInstance fails for mapped task instance
### Apache Airflow version

2.4.3

### What happened

`current_state` method on TaskInstance doesn't filter by `map_index` so calling this method on mapped task instance fails.

https://github.com/apache/airflow/blob/fb7c6afc8cb7f93909bd2e654ea185eb6abcc1ea/airflow/models/taskinstance.py#L708-L726

### What you think should happen instead

map_index should also be filtered in the query to return single TaskInstance object.

### How to reproduce

```python
with create_session() as session:
    print(session.query(TaskInstance).filter(TaskInstance.dag_id == "divide_by_zero", 
                                             TaskInstance.map_index == 1, 
                                             TaskInstance.run_id == 'scheduled__2022-11-22T00:00:00+00:00')
                                     .scalar().current_state())

 
---------------------------------------------------------------------------
MultipleResultsFound                      Traceback (most recent call last)
Input In [7], in <cell line: 1>()
      1 with create_session() as session:
----> 2     print(session.query(TaskInstance).filter(TaskInstance.dag_id == "divide_by_zero", TaskInstance.map_index == 1, TaskInstance.run_id == 'scheduled__2022-11-22T00:00:00+00:00').scalar().current_state())

File ~/stuff/python/airflow/airflow/utils/session.py:75, in provide_session.<locals>.wrapper(*args, **kwargs)
     73 else:
     74     with create_session() as session:
---> 75         return func(*args, session=session, **kwargs)

File ~/stuff/python/airflow/airflow/models/taskinstance.py:725, in TaskInstance.current_state(self, session)
    708 @provide_session
    709 def current_state(self, session: Session = NEW_SESSION) -> str:
    710     """
    711     Get the very latest state from the database, if a session is passed,
    712     we use and looking up the state becomes part of the session, otherwise
   (...)
    715     :param session: SQLAlchemy ORM Session
    716     """
    717     return (
    718         session.query(TaskInstance.state)
    719         .filter(
    720             TaskInstance.dag_id == self.dag_id,
    721             TaskInstance.task_id == self.task_id,
    722             TaskInstance.run_id == self.run_id,
    723
    724         )
--> 725         .scalar()
    726     )

File ~/stuff/python/airflow/.env/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2803, in Query.scalar(self)
   2801 # TODO: not sure why we can't use result.scalar() here
   2802 try:
-> 2803     ret = self.one()
   2804     if not isinstance(ret, collections_abc.Sequence):
   2805         return ret

File ~/stuff/python/airflow/.env/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2780, in Query.one(self)
   2762 def one(self):
   2763     """Return exactly one result or raise an exception.
   2764 
   2765     Raises ``sqlalchemy.orm.exc.NoResultFound`` if the query selects
   (...)
   2778 
   2779     """
-> 2780     return self._iter().one()

File ~/stuff/python/airflow/.env/lib/python3.10/site-packages/sqlalchemy/engine/result.py:1162, in Result.one(self)
   1134 def one(self):
   1135     # type: () -> Row
   1136     """Return exactly one row or raise an exception.
   1137 
   1138     Raises :class:`.NoResultFound` if the result returns no
   (...)
   1160 
   1161     """
-> 1162     return self._only_one_row(True, True, False)

File ~/stuff/python/airflow/.env/lib/python3.10/site-packages/sqlalchemy/engine/result.py:620, in ResultInternal._only_one_row(self, raise_for_second_row, raise_for_none, scalar)
    618     if next_row is not _NO_ROW:
    619         self._soft_close(hard=True)
--> 620         raise exc.MultipleResultsFound(
    621             "Multiple rows were found when exactly one was required"
    622             if raise_for_none
    623             else "Multiple rows were found when one or none "
    624             "was required"
    625         )
    626 else:
    627     next_row = _NO_ROW

MultipleResultsFound: Multiple rows were found when exactly one was required

```

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: References to the 'kubernetes' section cause parse errors
### Apache Airflow version

main (development)

### What happened

Here's a dag:

```python3
from airflow import DAG
from airflow.decorators import task
import airflow.configuration as conf
from datetime import datetime

@task
def print_this(this):
    print(this)

with DAG(dag_id="config_ref", schedule=None, start_date=datetime(1970, 1, 1)) as dag:
    namespace = conf.get("kubernetes", "NAMESPACE")
    print_this(namespace)
```

In 2.4.3 it parses without error, but in main (as of 2e7a4bcb550538283f28550208b01515d348fb51) the reference to the "kubernetes" section breaks.  Likely because of this: https://github.com/apache/airflow/pull/26873
```
 airflow dags list-import-errors
filepath                              | error
======================================+========================================================================================================
/Users/matt/2022/11/29/dags/config.py | Traceback (most recent call last):
                                      |   File "/Users/matt/src/airflow/airflow/configuration.py", line 595, in get
                                      |     return self._get_option_from_default_config(section, key, **kwargs)
                                      |   File "/Users/matt/src/airflow/airflow/configuration.py", line 605, in _get_option_from_default_config
                                      |     raise AirflowConfigException(f"section/key [{section}/{key}] not found in config")
                                      | airflow.exceptions.AirflowConfigException: section/key  not found in config
                                      |

 python dags/config.py
[2022-11-29 21:30:05,300] {configuration.py:603} WARNING - section/key [kubernetes/namespace] not found in config
Traceback (most recent call last):
  File "/Users/matt/2022/11/29/dags/config.py", line 13, in <module>
    namespace = conf.get("kubernetes", "NAMESPACE")
  File "/Users/matt/src/airflow/airflow/configuration.py", line 1465, in get
    return conf.get(*args, **kwargs)
  File "/Users/matt/src/airflow/airflow/configuration.py", line 595, in get
    return self._get_option_from_default_config(section, key, **kwargs)
  File "/Users/matt/src/airflow/airflow/configuration.py", line 605, in _get_option_from_default_config
    raise AirflowConfigException(f"section/key [{section}/{key}] not found in config")
airflow.exceptions.AirflowConfigException: section/key [kubernetes/namespace] not found in config
```

To quote @jedcunningham :
>  The backcompat layer only expects you to use the new section name.




### What you think should happen instead

The recent section name change should be registered so that the old name still works.

### How to reproduce

See above

### Operating System

Mac OS / venv

### Versions of Apache Airflow Providers

n/a

### Deployment

Virtualenv installation

### Deployment details

`pip install -e ~/src/airflow` into a fresh venv

### Anything else

Also, it's kind of weird that the important part of the error message (which section?) is missing from `list-import-errors`.  I had to run the dag def like a script to realize that it was the kubernetes section that it was complaining about.


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/config_endpoint.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: Snowflake connection, boolean value of extra__snowflake__insecure_mode interpreted as string
### Apache Airflow version

2.2.3 (latest released)

### What happened

Error thrown when using SnowflakeOperator with a Snowflake Connection.

After creating a Snowflake Connection, the "Extra" field was automatically filled with a dictionary containing the values entered in the other input fields. Note: the value for the key "extra__snowflake__insecure_mode" is a boolean. 

![image](https://user-images.githubusercontent.com/82085639/150988536-002c78d7-b405-49c8-af10-ddf3d050ce09.png)

A task using SnowflakeOperator fails, throwing following error:
```
[2022-01-25, 14:39:54 UTC] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/operators/snowflake.py", line 129, in execute
    execution_info = hook.run(self.sql, autocommit=self.autocommit, parameters=self.parameters)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 293, in run
    with closing(self.get_conn()) as conn:
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 236, in get_conn
    conn_config = self._get_conn_params()
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 170, in _get_conn_params
    insecure_mode = to_boolean(
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/utils/strings.py", line 30, in to_boolean
    return False if astring is None else astring.lower() in ['true', 't', 'y', 'yes', '1']
AttributeError: 'bool' object has no attribute 'lower'
[2022-01-25, 14:39:54 UTC] {taskinstance.py:1267} INFO - Marking task as FAILED. dag_id=test, task_id=snowflake_task, execution_date=20220123T000000, start_date=20220125T133954, end_date=20220125T133954
[2022-01-25, 14:39:55 UTC] {standard_task_runner.py:89} ERROR - Failed to execute job 4 for task snowflake_task
Traceback (most recent call last):
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 180, in _run_raw_task
    ti._run_raw_task(
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/operators/snowflake.py", line 129, in execute
    execution_info = hook.run(self.sql, autocommit=self.autocommit, parameters=self.parameters)
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 293, in run
    with closing(self.get_conn()) as conn:
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 236, in get_conn
    conn_config = self._get_conn_params()
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/providers/snowflake/hooks/snowflake.py", line 170, in _get_conn_params
    insecure_mode = to_boolean(
  File "/home/test_airflow2/.local/lib/python3.8/site-packages/airflow/utils/strings.py", line 30, in to_boolean
    return False if astring is None else astring.lower() in ['true', 't', 'y', 'yes', '1']
AttributeError: 'bool' object has no attribute 'lower'
[2022-01-25, 14:39:55 UTC] {local_task_job.py:154} INFO - Task exited with return code 1
[2022-01-25, 14:39:55 UTC] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check

```

This error seems to arise because the boolean value of extra__snowflake__insecure_mode is interpreted as a string. 
Manually modifying the boolean extra__snowflake__insecure_mode value to be string in the "Extra" dictionary solves this problem:
**false->"false"**
![image](https://user-images.githubusercontent.com/82085639/150990538-b58a641e-cc16-4c49-88a6-1d32c4a3fc79.png)


### What you expected to happen

Be able to create a usable Snowflake Connection, by filling in fields other than "Extra". The "Extra" field should be automatically filled with a correct/usable connection dictionary.
I can then use this Snowflake Connection for SnowflakeOperators in DAGs. 

### How to reproduce

Create a new Connection of type **Snowflake**, set an arbitrary Connection ID. The rest of the fields can be left empty (doesn't affect error).
![image](https://user-images.githubusercontent.com/82085639/150990006-87c9e388-872b-4323-b34b-3ea816f024bb.png)


Create a DAG with a SnowflakeOperator task, which uses the created Snowflake Connection:
```
from airflow import DAG
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from airflow.utils.dates import days_ago

with DAG('test', start_date=days_ago(2)) as dag:
    
    snowflake_task = SnowflakeOperator(task_id='snowflake_task',
                                            sql='select 1;',
                                            snowflake_conn_id='snowflake_conn')
```

Execute the DAG, the task will fail and throw the above error.

### Operating System

Ubuntu 20.04.2 LTS

### Versions of Apache Airflow Providers

apache-airflow-providers-snowflake==2.4.0

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/hooks/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py']
Ground Truth : ['a/airflow/providers/snowflake/hooks/snowflake.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: API Endpoint - CRUD - Dag Runs
Hello 

We need to create several endpoints that perform basic CRUD operations on **DAG Run**. We need the following endpoints:

DELETE /dags/{dag_id}/dagRuns/{dag_run_id}
PATCH /dags/{dag_id}/dagRuns/{dag_run_id}
POST /dags/{dag_id}/dagRuns/{dag_run_id}

It depends on https://github.com/apache/airflow/issues/8129

Detailed information is available in the issue:
https://github.com/apache/airflow/issues/8118

LOVE,

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_docker_compose_quick_start.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/mark_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/schemas/dag_run_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_xcom_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/api_connexion/endpoints/dag_run_endpoint.py', 'a/airflow/api_connexion/schemas/dag_run_schema.py']
Current Recall: 0.06317171408177834

=========================================================

ISSUE: DockerSwarmOperator can not mount a volume
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 1.10.10


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): Ubuntu 16.04
- **Kernel** (e.g. `uname -a`): Linux 4.4.0-179-generic
- **Install tools**: pip inside virtualenv
- **Others**:

**What happened**:


`with dag as dag:
    t1 = DockerSwarmOperator(
        api_version='auto',              
        command='***',          
        image='***',             
        auto_remove=False,        
        task_id='airflow_test',     
        volumes=['/tmp/.X11-unix:/tmp/.X11-unix'],
        environment = {'QT_X11_NO_MITSHM':1, 'DISPLAY':0.0}
    )`
Mounting of a volume is not succesfull with Docker Swarm Operator.

<!-- (please include exact error messages if you can) -->

**What you expected to happen**:
I am able to create a service with Docker swarm operator but I can not mount a volume to the container. I understand the problem when I run "docker service inspect <service_name>", there is no volume mounted to the host PC. 

**Solution proposal**:
Inside _run_image() function in docker_swarm_operator.py file we create a service but it has a missing argument for the volumes.
I simply fixed the problem when I add `mounts=self.volumes` as an argument to a `ContainerSpec` function.

New syntax to mount Docker volumes with --mount
I had this after reading #12537 and #9047. Currently `DockerOperator`s `volumes` argument is passed directly to docker-pys `bind` (aka `docker -v`). But `-v`s behaviour has long been problematic, and [Docker has been pushing users to the new `--mount` syntax instead](https://docs.docker.com/storage/bind-mounts/#choose-the--v-or---mount-flag). With #12537, it seems like `-v`s behaviour is also confusing to some Airflow users, so I want to migrate Airflows internals to `--mount`.

However, `--mount` has a different syntax to `-v`, and the behaviour is also slightly different, so for compatibility reasons we cant just do it under the hood. I can think of two possible solutions to this:

A. Deprecate `volumes` altogether and introduce `DockerOperator(mounts=...)`

This will emit a deprecation warning when the user passes `DockerOperator(volumes=...)` to tell them to convert to `DockerOperator(mounts=...)` instead. `volumes` will stay unchanged otherwise, and continue to be passed to bind mounts.

`mounts` will take a list of [`docker.types.Mount`](https://docker-py.readthedocs.io/en/stable/api.html#docker.types.Mount) to describe the mounts. They will be passed directly to the mounts API. Some shorthands could be useful as well, for example:

```python
DockerOperator(
    ...
    mounts=[
        ('/root/data1', './data1'),  # Source and target, default to volume mount.
        ('/root/data2', './data2', 'bind'),  # Bind mount.
    ],
)
```

B. Reuse `volumes` and do introspection to choose between binds and mounts

The `volumes` argument can be augmented to also accept `docker.types.Mount` instances, and internally well do something like

```python
binds = []
mounts = []
for vol in volumes:
    if isinstance(vol, str):
        binds.append(vol)
    elif isintance(vol, docker.types.Mount):
        mounts.append(vol)
    else:
        raise ValueError('...')
if binds:
    warnings.warn('...', DeprecationWarning)
```

and pass the collected lists to binds and mounts respectively.

Im very interested in hearing thoughts on this.


**Are you willing to submit a PR?**
Yes

**Related Issues**

* #12537: Confusing on the bind syntax.
* #9047: Implement mounting in `DockerSwarmOperator` (its a subclass of `DockerOperator`, but the `volumes` option is currently unused).

Mounting directories using docker operator on airflow is not working
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: apache-airflow==1.10.12


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): Does not apply

**Environment**: 

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): Ubuntu 18.04.5 LTS bionic
- **Kernel** (e.g. `uname -a`): Linux letyndr-letyndr 4.15.0-123-generic #126-Ubuntu SMP Wed Oct 21 09:40:11 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**:
- **Others**:

**What happened**:

I'm trying to use the docker operator to automate the execution of some scripts using airflow.

What I want to do is to "copy" all my project's files (with folders and files) to the container using this code.

The following file ml-intermediate.py is in this directory ~/airflow/dags/ml-intermediate.py:



```
"""
Template to convert a Ploomber DAG to Airflow
"""
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago

from ploomber.spec import DAGSpec
from soopervisor.script.ScriptConfig import ScriptConfig

script_cfg = ScriptConfig.from_path('/home/letyndr/airflow/dags/ml-intermediate')
# Replace the project root to reflect the new location - or maybe just
# write a soopervisor.yaml, then we can we rid of this line
script_cfg.paths.project = '/home/letyndr/airflow/dags/ml-intermediate'

# TODO: use lazy_import from script_cfg
dag_ploomber = DAGSpec('/home/letyndr/airflow/dags/ml-intermediate/pipeline.yaml',
                       lazy_import=True).to_dag()
dag_ploomber.name = "ML Intermediate"

default_args = {
    'start_date': days_ago(0),
}

dag_airflow = DAG(
    dag_ploomber.name.replace(' ', '-'),
    default_args=default_args,
    description='Ploomber dag',
    schedule_interval=None,
)

script_cfg.save_script()

from airflow.operators.docker_operator import DockerOperator
for task_name in dag_ploomber:
    DockerOperator(task_id=task_name,
        image="continuumio/miniconda3",
        api_version="auto",
        auto_remove=True,
        # command="sh /home/letyndr/airflow/dags/ml-intermediate/script.sh",
        command="sleep 600",
        docker_url="unix://var/run/docker.sock",
        volumes=[
            "/home/letyndr/airflow/dags/ml-intermediate:/home/letyndr/airflow/dags/ml-intermediate:rw",
            "/home/letyndr/airflow-data/ml-intermediate:/home/letyndr/airflow-data/ml-intermediate:rw"
        ],
        working_dir=script_cfg.paths.project,
        dag=dag_airflow,
        container_name=task_name,
    )



for task_name in dag_ploomber:
    task_ploomber = dag_ploomber[task_name]
    task_airflow = dag_airflow.get_task(task_name)

    for upstream in task_ploomber.upstream:
        task_airflow.set_upstream(dag_airflow.get_task(upstream))

dag = dag_airflow
```

When I execute this DAG using Airflow, I get the error that the docker does not find the `/home/letyndr/airflow/dags/ml-intermediate/script.sh` script. I changed the execution command of the docker operator `sleep 600` to enter to the container and check the files in the container with the corrects paths.

**What you expected to happen**: Basically to share the files of the host with the docker container to execute a shell script within the container.

When I'm in the container I can go to this path /home/letyndr/airflow/dags/ml-intermediate/ for example, but I don't see the files that are supposed to be there.

**What do you think went wrong?** 

I tried to replicate how Airflow implements Docker SDK for Python 

This is my one replication of the docker implementation:

```
import docker

client = docker.APIClient()

# binds = {
#         "/home/letyndr/airflow/dags": {
#             "bind": "/home/letyndr/airflow/dags",
#             "mode": "rw"
#         },
#         "/home/letyndr/airflow-data/ml-intermediate": {
#             "bind": "/home/letyndr/airflow-data/ml-intermediate",
#             "mode": "rw"
#         }
#     }

binds = ["/home/letyndr/airflow/dags:/home/letyndr/airflow/dags:rw",
"/home/letyndr/airflow-data/ml-intermediate:/home/letyndr/airflow-data/ml-intermediate:rw"]

container = client.create_container(
    image="continuumio/miniconda3",
    command="sleep 600",
    volumes=["/home/letyndr/airflow/dags", "/home/letyndr/airflow-data/ml-intermediate"],
    host_config=client.create_host_config(binds=binds),
    working_dir="/home/letyndr/airflow/dags",
    name="simple_example",
)

client.start(container=container.get("Id"))

```
What I found was that mounting volumes only works if it's set `host_config` and `volumes`, the problem is that the implementation on Airflow just set `host_config` but not `volumes`. I added the parameter on the method `create_container`, it worked.



**How to reproduce it**:
Mount a volume from a host and use the files inside the directory in the docker container.



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/singularity/operators/singularity.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/docker/operators/test_docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/container_instances.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/docker/example_docker_copy_data.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/setup_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/visuals.py']
Ground Truth : ['a/airflow/providers/docker/operators/docker_swarm.py', 'a/airflow/providers/docker/operators/docker.py', 'a/docs/exts/docs_build/third_party_inventories.py', 'a/docs/conf.py', 'a/airflow/providers/docker/example_dags/example_docker_copy_data.py']
Current Recall: 0.06359997960640361

=========================================================

ISSUE: Make conn_id unique in Connections table
s we discussed in the thread on Spec thread for our new API, the idea
came up of making conn_id unique in Airflow 

<https://lists.apache.org/thread.html/rfb6f7d95b2754fda9dd09b08444214dfad12d10f143d32de0fcf4104%40%3Cdev.airflow.apache.org%3E>

The people in that thread seemed to be in favour of it:

- It is often confusing to users (there have been a few bug reports about it over the years)
- It's questionable if it actually works well or not
- There are better/smarter tools for loadbalancing connections to a DB than picking one of a random list
- For Hive at least it has been implemented another way - allowing two host's in a single connection string https://github.com/apache/airflow/pull/4708 (available in 1.10.6)
- It makes the HTTP API confusing (needing a integer connection ID, and a string conn_id field)

Given the downsides/work arounds, and the confusion this causes we should remove this (mis)feature from Airflow 2.0.

We should take some care in the migration that does this to capture the unique constraint error and report it a a meaninful way to the user.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py']
Ground Truth : ['a/airflow/exceptions.py', '/dev/null', 'a/airflow/models/connection.py', 'a/airflow/secrets/local_filesystem.py']
Current Recall: 0.06359997960640361

=========================================================

ISSUE: Broken Tree View
We are using version 1.5.1 of Airflow.

I am seeing the following issue with Tree View. Steps:
1. Go to http://my-airflow-instance.com/admin/
2. Click on Tree View for any DAG.
3. In the Tree View, click on `Go` button in the Tree form.
4. 500 page with this stack trace:

```
2015-09-22_23:19:02.61708 2015-09-22 23:19:02,616 - tornado.access - ERROR - 500 GET /admin/airflow/tree?base_date=2015-09-23+17%3A00%3A00&num_runs=25&root=None&dag_id=summarize_hourly (172.17.42.1) 15.79ms
2015-09-22_23:24:43.09539 2015-09-22 23:24:43,095 - airflow.www.app - ERROR - Exception on /admin/airflow/tree [GET]
2015-09-22_23:24:43.09552 Traceback (most recent call last):
2015-09-22_23:24:43.09557   File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 1817, in wsgi_app
2015-09-22_23:24:43.09561     response = self.full_dispatch_request()
2015-09-22_23:24:43.09565   File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 1477, in full_dispatch_request
2015-09-22_23:24:43.09568     rv = self.handle_user_exception(e)
2015-09-22_23:24:43.09570   File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 1381, in handle_user_exception
2015-09-22_23:24:43.09570     reraise(exc_type, exc_value, tb)
2015-09-22_23:24:43.09570   File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 1475, in full_dispatch_request
2015-09-22_23:24:43.09571     rv = self.dispatch_request()
2015-09-22_23:24:43.09571   File "/usr/local/lib/python2.7/dist-packages/flask/app.py", line 1461, in dispatch_request
2015-09-22_23:24:43.09572     return self.view_functions[rule.endpoint](**req.view_args)
2015-09-22_23:24:43.09575   File "/usr/local/lib/python2.7/dist-packages/flask_admin/base.py", line 68, in inner
2015-09-22_23:24:43.09576     return self._run_view(f, *args, **kwargs)
2015-09-22_23:24:43.09578   File "/usr/local/lib/python2.7/dist-packages/flask_admin/base.py", line 359, in _run_view
2015-09-22_23:24:43.09579     return fn(self, *args, **kwargs)
2015-09-22_23:24:43.09581   File "/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py", line 96, in view_func
2015-09-22_23:24:43.09583     return f(*args, **kwargs)
2015-09-22_23:24:43.09584   File "/usr/local/lib/python2.7/dist-packages/airflow/www/app.py", line 1070, in tree
2015-09-22_23:24:43.09585     node_limit = 5000 / len(dag.roots)
2015-09-22_23:24:43.09587 ZeroDivisionError: division by zero
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/dates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0004_1_5_0_more_logging_into_task_isntance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/flink/sensors/test_flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/sensors/test_spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/sensors/test_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/sftp/decorators/sensors/test_sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/redis/log/redis_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/macros/test_macros.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py']
Ground Truth : ['a/airflow/www/app.py']
Current Recall: 0.06359997960640361

=========================================================

ISSUE: Fix "Attempt 2 out of 1" messages
It's possible to get strange "Attempt 2 out of 1" status messages when Airflow forces a task to run after it's completed or previously run.

For example, this unit test in CoreTests prints such a message (https://github.com/airbnb/airflow/blob/aee3cacb1042dbdc26d9ba06d481c81948f418ec/tests/core.py#L467):

``` python
    def test_raw_job(self):
        TI = models.TaskInstance
        ti = TI(
            task=self.runme_0, execution_date=DEFAULT_DATE)
        ti.dag = self.dag_bash
        ti.run(force=True)
```

The "Attempt" message is built as 

``` python
            tot_tries = task.retries + 1
            msg = "Attempt {} out of {}".format(self.try_number+1,
                                                tot_tries)
            self.try_number += 1
```

So every time the task gets loaded, the number of attempts is incremented. If the task is run an unexpected number of times, either due to prior failures or `force` runs, then this message becomes nonsensical. I think this is straightforward to fix by resetting self.try_number any time the task is run that ISN'T a retry.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/executors/test_base_executor.py']
Ground Truth : ['a/airflow/models.py']
Current Recall: 0.06359997960640361

=========================================================

ISSUE: ExternalPythonOperator: AttributeError: 'python_path' is configured as a template field but ExternalPythonOperator does not have this attribute.
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

Using the ExternalPythonOperator directly in v2.4.2 as opposed to via the @task.external decorator described in https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#externalpythonoperator causes the following error: 

```
AttributeError: 'python_path' is configured as a template field but ExternalPythonOperator does not have this attribute.
```

This seems to be due to https://github.com/apache/airflow/blob/main/airflow/operators/python.py#L624 having 'python_path' as an additional template field, instead of 'python', which is the correct additional keyword argument for the operator

### What you think should happen instead

We should change https://github.com/apache/airflow/blob/main/airflow/operators/python.py#L624 to 
read:

```
template_fields: Sequence[str] = tuple({'python'} | set(PythonOperator.template_fields))
```

instead of 
```
template_fields: Sequence[str] = tuple({'python_path'} | set(PythonOperator.template_fields))
```

This has been verified by adding:
```
ExternalPythonOperator.template_fields = tuple({'python'} | set(PythonOperator.template_fields))
```
in the sample DAG code below, which causes the DAG to run successfully 

### How to reproduce

```
import airflow
from airflow.models import DAG
from airflow.operators.python import ExternalPythonOperator

args = dict(
    start_date=airflow.utils.dates.days_ago(3),
    email=["x@y.com"],
    email_on_failure=False,
    email_on_retry=False,
    retries=0
)

dag = DAG(
    dag_id='test_dag',
    default_args=args,
    schedule_interval='0 20 * * *',
    catchup=False,
)

def print_kwargs(*args, **kwargs):
    print('args', args)
    print('kwargs', kwargs)


with dag:

    def print_hello():
        print('hello')

    # Due to a typo in the airflow library :(
    # ExternalPythonOperator.template_fields = tuple({'python'} | set(PythonOperator.template_fields))

    t1 = ExternalPythonOperator(
        task_id='test_task',
        python='/opt/airflow/miniconda/envs/nexus/bin/python',
        python_callable=print_kwargs
    )
```



### Operating System

Ubuntu 18.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/external_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py']
Ground Truth : ['a/airflow/operators/python.py']
Current Recall: 0.06574130722952995

=========================================================

ISSUE: Mask secrets in stdout for `airflow tasks test ...` CLI command (#17476)
The problem with `airflow tasks test ...` is that any `stdout` that comes from a task inside an operator or other callable does not go through a logger.  It runs in the main process.  This results in secrets that are printed to `stdout` to not be masked according to the `airflow.task` logger.

I wanted a way to capture and filter stdout without having to route everything through a secondary logger.  I decided a context manager that filters `stdout` would be the best way to go about this.  The context manager redacts all secrets according to the filters on the `airflow.task` logger.

Closes #17476 

Sensitive variables don't get masked when rendered with airflow tasks test
**Apache Airflow version**: 2.1.2


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): No

**Environment**:

- **Cloud provider or hardware configuration**: No
- **OS** (e.g. from /etc/os-release): MacOS Big Sur 11.4
- **Kernel** (e.g. `uname -a`):  -
- **Install tools**: -
- **Others**: -

**What happened**:

With the following code:
```
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator

from datetime import datetime, timedelta

def _extract():
    partner = Variable.get("my_dag_partner_secret")
    print(partner)

with DAG("my_dag", start_date=datetime(2021, 1 , 1), schedule_interval="@daily") as dag:

    extract = PythonOperator(
        task_id="extract",
        python_callable=_extract
    )
```

By executing the command

`
airflow tasks test my_dag extract 2021-01-01
`

The value of the variable my_dag_partner_secret gets rendered in the logs whereas it shouldn't

```
[2021-08-06 19:05:30,088] {taskinstance.py:1303} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=my_dag
AIRFLOW_CTX_TASK_ID=extract
AIRFLOW_CTX_EXECUTION_DATE=2021-01-01T00:00:00+00:00
partner_a
[2021-08-06 19:05:30,091] {python.py:151} INFO - Done. Returned value was: None
[2021-08-06 19:05:30,096] {taskinstance.py:1212} INFO - Marking task as SUCCESS. dag_id=my_dag, task_id=extract, execution_date=20210101T000000, start_date=20210806T131013, end_date=20210806T190530
```
**What you expected to happen**:

The value should be masked like on the UI or in the logs

**How to reproduce it**:

DAG given above


**Anything else we need to know**:

Nop


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py', 'a/airflow/utils/log/secrets_masker.py']
Current Recall: 0.06574130722952995

=========================================================

ISSUE: airflow task test failing due to mini scheduler implementation not respecting test mode
**Apache Airflow version**: 2.0.0b3

**Environment**: Python3.7-slim running on docker

**What happened**:

Error when running `airflow tasks test <dag> <task> <date>` there is an error with the task rescheduler, which should not happen as we are testing the tasks and not running via scheduler. 

**What you expected to happen**:

No error message should be displayed as the task is success and it is running on test mode

**How to reproduce it**:

just run the `airflow test <dag> <task> <date>` after a vanilla airflow installation using pip install.

**Anything else we need to know**:
this is the log
```
root@add8b3f038cf:/# airflow  tasks test docker_test d 2020-11-24 
[2020-11-24 08:13:00,796] {dagbag.py:440} INFO - Filling up the DagBag from /opt/airflow/dags/test
[2020-11-24 08:13:01,072] {taskinstance.py:827} INFO - Dependencies all met for <TaskInstance: docker_test.d 2020-11-24T00:00:00+00:00 [None]>
[2020-11-24 08:13:01,077] {taskinstance.py:827} INFO - Dependencies all met for <TaskInstance: docker_test.d 2020-11-24T00:00:00+00:00 [None]>
[2020-11-24 08:13:01,077] {taskinstance.py:1018} INFO - 
--------------------------------------------------------------------------------
[2020-11-24 08:13:01,077] {taskinstance.py:1019} INFO - Starting attempt 1 of 4
[2020-11-24 08:13:01,077] {taskinstance.py:1020} INFO - 
--------------------------------------------------------------------------------
[2020-11-24 08:13:01,078] {taskinstance.py:1039} INFO - Executing <Task(PythonOperator): d> on 2020-11-24T00:00:00+00:00
[2020-11-24 08:13:01,109] {taskinstance.py:1232} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=adilson@zookal.com
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=docker_test
AIRFLOW_CTX_TASK_ID=d
AIRFLOW_CTX_EXECUTION_DATE=2020-11-24T00:00:00+00:00
d
[2020-11-24 08:13:01,110] {python.py:118} INFO - Done. Returned value was: None
[2020-11-24 08:13:01,115] {taskinstance.py:1143} INFO - Marking task as SUCCESS. dag_id=docker_test, task_id=d, execution_date=20201124T000000, start_date=20201124T081301, end_date=20201124T081301
Traceback (most recent call last):
  File "/usr/local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.7/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 50, in command
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/cli.py", line 86, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 379, in task_test
    ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/session.py", line 63, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1350, in run
    mark_success=mark_success, test_mode=test_mode, job_id=job_id, pool=pool, session=session
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/session.py", line 59, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1152, in _run_raw_task
    self._run_mini_scheduler_on_child_tasks(session)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/session.py", line 59, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1165, in _run_mini_scheduler_on_child_tasks
    execution_date=self.execution_date,
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 3473, in one
    raise orm_exc.NoResultFound("No row was found for one()")
sqlalchemy.orm.exc.NoResultFound: No row was found for one()
```



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/sensors/test_spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.06574130722952995

=========================================================

ISSUE: Deprecate adding Operators via plugins
This is the companion issue to https://github.com/apache/airflow/issues/9498, but for adding a deprecation warning to 1.10.12 so we can remove it in Airflow 2.0


```
    from airflow.operators.my_plugin import MyOperator
```

can become

```
    from my_plugin import MyOperator
```

In 1.10.12 we should issue a FutureDeprecationWarning in the plugin if it includes anything in `operators = []`, and additionally when anything imports from `airflow.operators.my_plugin` (this last part may already by happening. We should double check and confirm.)

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/_cron.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py']
Ground Truth : ['a/setup.py', 'a/airflow/sensors/__init__.py', 'a/airflow/operators/__init__.py', 'a/airflow/plugins_manager.py']
Current Recall: 0.06574130722952995

=========================================================

ISSUE: Enhance Airflow Logs API to fetch logs from Amazon Cloudwatch with time range
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

MWAA Version: 2.4.3
Airflow Version: 2.4.3

Airflow Logs currently do not fetch logs from Cloudwatch without time range, so when the cloudwatch logs are large and CloudWatch log streams are OLD, the airflow UI cannot display logs with error message:

```
*** Reading remote log from Cloudwatch log_group: airflow-cdp-airflow243-XXXX-Task log_stream: dag_id=<DAG_NAME>/run_id=scheduled__2023-07-27T07_25_00+00_00/task_id=<TASK_ID>/attempt=1.log.
Could not read remote logs from log_group: airflow-cdp-airflow243-XXXXXX-Task log_group: airflow-cdp-airflow243-XXXX-Task log_stream: dag_id=<DAG_NAME>/run_id=scheduled__2023-07-27T07_25_00+00_00/task_id=<TASK_ID>/attempt=1.log
```

The Airflow API need to pass start and end timestamps to GetLogEvents API from Amazon CloudWatch to resolve this error and it also improves performance of fetching logs.

This is critical issue for customers when they would like to fetch logs to investigate failed pipelines form few days to weeks old

### What you think should happen instead

The Airflow API need to pass start and end timestamps to GetLogEvents API from Amazon CloudWatch to resolve this error.
This should also improve performance of fetching logs.


### How to reproduce

This issue is intermittent and happens mostly on FAILD tasks.

1. Log onto Amazon MWAA Service
2. Open Airflow UI
3. Select DAG
4. Select the Failed Tasks 
5. Select Logs
You should see error message like below in the logs:
```
*** Reading remote log from Cloudwatch log_group: airflow-cdp-airflow243-XXXX-Task log_stream: dag_id=<DAG_NAME>/run_id=scheduled__2023-07-27T07_25_00+00_00/task_id=<TASK_ID>/attempt=1.log.
Could not read remote logs from log_group: airflow-cdp-airflow243-XXXXXX-Task log_group: airflow-cdp-airflow243-XXXX-Task log_stream: dag_id=<DAG_NAME>/run_id=scheduled__2023-07-27T07_25_00+00_00/task_id=<TASK_ID>/attempt=1.log
```

### Operating System

Running with Amazon MWAA

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.3.1
apache-airflow==2.4.3

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/triggers/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py']
Ground Truth : ['a/airflow/providers/amazon/aws/log/cloudwatch_task_handler.py', 'a/airflow/providers/amazon/aws/hooks/logs.py', 'a/airflow/providers/amazon/aws/utils/__init__.py']
Current Recall: 0.06574130722952995

=========================================================

ISSUE: Improve filtering for invalid schemas in Hive hook
### Description

#27647 has introduced filtering for invalid schemas in Hive hook based on the characters `;` and `!`. I'm wondering if a more generic filtering could be introduced, e.g. one that adheres to the regex `[^a-z0-9_]`, since Hive schemas (and table names) can only contain alphanumeric characters and the character `_`.

Note: since the Hive metastore [stores schemas and tables in lowercase](https://stackoverflow.com/questions/57181316/how-to-keep-column-names-in-camel-case-in-hive/57183048#57183048), checking against `[^a-z0-9_]` is probably better than `[^a-zA-Z0-9_]`.

### Use case/motivation

Ensure that Hive schemas used in `apache-airflow-providers-apache-hive` hooks contain no invalid characters.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/operators/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/glue_catalog.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/chime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/transfers/mysql_to_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/macros/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator_deprecated.py']
Ground Truth : ['a/airflow/providers/apache/hive/hooks/hive.py']
Current Recall: 0.06788263485265629

=========================================================

ISSUE: API Endpoint - Dags details/Task
**Description**

We need to have endpoints that allow us to read information about workflow - DAG, Task. Endpoints should properly support serialization.

- GET /dags/{dag_id}/details
- GET /dags/{dag_id}/tasks
- GET /dags/{dag_id}/tasks/{task_id}

More details about API Endpoints:
https://github.com/apache/airflow/issues/8118

**Use case / motivation**

N/A

**Related Issues**

N/A

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/client/json_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_log_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataprep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/http/sensors/test_http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py']
Ground Truth : ['/dev/null', 'a/setup.py', 'a/airflow/api_connexion/endpoints/dag_endpoint.py', 'a/airflow/api_connexion/endpoints/task_endpoint.py']
Current Recall: 0.06788263485265629

=========================================================

ISSUE: Display DAGs dependencies  in CLI
### Description

Recently, we added [a new DAG dependencies view](https://github.com/apache/airflow/pull/13199) to the webserver. It would be helpful if a similar diagram could also be displayed/generated using the CLI. For now, only [one DAG can be displayed](http://airflow.apache.org/docs/apache-airflow/stable/usage-cli.html#exporting-dag-structure-as-an-image) using CLI.
![image](https://user-images.githubusercontent.com/12058428/137945580-6a33f919-2648-4980-bd2c-b40cfcacc9fe.png)

If anyone is interested, I will be happy to help with the review.

### Use case/motivation

* Keep parity between CLI and web serwer.
* Enable the writing of scripts that use these diagrams, e.g. for attaching in the documentation.

### Related issues

https://github.com/apache/airflow/pull/13199/files

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_bulk_issues.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py']
Ground Truth : ['a/airflow/utils/dot_renderer.py', 'a/airflow/cli/commands/dag_command.py', 'a/airflow/cli/cli_parser.py']
Current Recall: 0.06788263485265629

=========================================================

ISSUE: Unnecessary warnings during airflow pods starts
### Apache Airflow version

2.2.2 (latest released)

### What happened

Warnings `{providers_manager.py:156} WARNING - Exception when importing 'airflow.providers.google.leveldb.hooks.leveldb.LevelDBHook' from 'apache-airflow-providers-google' package: No module named 'plyvel'` appear in `wait-for-airflow-migrations` init containers and during webserver pod start.

### What you expected to happen

No such warnings, they were not present in 2.1.3 version.

### How to reproduce

Install Airflow using airflow helm chart with 2.2.2 airflow docker image and check `wait-for-airflow-migrations` init containers logs. Warnings can also be seen in webserver container.

### Operating System

Debian buster

### Versions of Apache Airflow Providers

Providers that come with the official Airflow docker image.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

Looking at commit history, isn't this supposed to be a debug level log as per https://github.com/apache/airflow/commit/2f32df7b711cf63d18efd5e0023b22a79040cc86#diff-c56df299b30e60d690494057a4e8721d36406c0cca266961ff2ae6504993c8cb ?

Seems like this logic was changed in https://github.com/apache/airflow/commit/29aab6434ffe0fb8c83b6fd6c9e44310966d496a#diff-c56df299b30e60d690494057a4e8721d36406c0cca266961ff2ae6504993c8cbL288 since `_sanity_check`  doesn't handle `ImportError`s as debug messages . Should these warnings be ignored for now?

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py']
Ground Truth : ['a/airflow/providers_manager.py']
Current Recall: 0.06788263485265629

=========================================================

ISSUE: CSRF configuration is missing the WTF_ prefix
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**:


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**: AWS ECS
- **OS** (e.g. from /etc/os-release): `Ubuntu 18.04 bionic`
- **Kernel** (e.g. `uname -a`): `4.15.0-1065-aws`
- **Install tools**:
- **Others**:

**What happened**:
I have been trying to update a certain CSRF configuration (`WTF_CSRF_TIME_LIMIT`) because I've been annoyed by the `CSRF token has expired` error message whenever I stayed on a page for more than 1 hour and wanted to `refresh`.
<!-- (please include exact error messages if you can) -->

**What you expected to happen**:
In `webserver_config.py` there is a `CSRF_ENABLED = True`. So I added `CSRF_TIME_LIMIT = None` after that line. But it didn't work. After reading https://github.com/lepture/flask-wtf/blob/v0.14.2/flask_wtf/csrf.py, I realized that we needed `WTF_CSRF_TIME_LIMIT` in my `webserver_config.py`. The `WTF_` prefix cannot be omitted. 
<!-- What do you think went wrong? -->

**How to reproduce it**:
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md sytle of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->
1. Add `CSRF_TIME_LIMIT = None` after `CSRF_ENABLED = True` in `webserver_config.py`. 
2. Re-deploy. 
3. Go to any DAG's tree view.
4. Stay there for more than 1 hour.
5. Click the `Refresh` button.
6. See the `CSRF token has expired` error message

**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->
I already forked Airflow. I guess I'll submit a simple PR to add the `WTF_` prefix

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/config_templates/default_webserver_config.py']
Current Recall: 0.06788263485265629

=========================================================

ISSUE: CI: Rewrite `free_space` script in Python
In our build/ci.yml (maybe other?) files we are using a "free_space" script that performs cleanup of the machine before running the tasks. This allows us to reclaim memory and disk space for our tasks.

Example: https://github.com/apache/airflow/blob/eaa8ac72fc901de163b912a94dbe675045d2a009/.github/workflows/ci.yml#L334

This should be written in Python and our ci.yml and build.yml should be updated. We shoudl also be able to remove free_space script from the repo.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/scheduler_dag_execution_timing.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/non_caching_file_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/main_command.py']
Ground Truth : ['/dev/null']
Current Recall: 0.06788263485265629

=========================================================

ISSUE: Dynamic task mapping does not always create mapped tasks
### Apache Airflow version

2.5.1

### What happened

Same problem as https://github.com/apache/airflow/issues/28296, but seems to happen nondeterministically, and still happens when ignoring `depends_on_past=True`.

I've got a task that retrieves some filenames, which then creates dynamically mapped tasks to move the files, one per task.
I'm using a similar task across multiple DAGs. However, task mapping fails on some DAG runs: it inconsistently happens per DAG run, and some DAGs do not seem to be affected at all. These seem to be the DAGs where no task was ever mapped, so that the mapped task instance ended up in a Skipped state.

What happens is that multiple files will be found, but only a single dynamically mapped task will be created. This task never starts and has map_index of -1. It can be found under the "List instances, all runs" menu, but says "No Data found." under the "Mapped Tasks" tab.

![Screenshot 2023-02-14 at 13 29 15](https://user-images.githubusercontent.com/64646000/218742434-c132d3c1-8013-446f-8fd0-9b485506f43e.png)

![Screenshot 2023-02-14 at 13 29 25](https://user-images.githubusercontent.com/64646000/218742461-fb0114f6-6366-403b-841e-03b0657e3561.png)

When I press the "Run" button when the mapped task is selected, the following error appears:

```
Could not queue task instance for execution, dependencies not met: Previous Dagrun State: depends_on_past is true for this task's DAG, but the previous task instance has not run yet., Task has been mapped: The task has yet to be mapped!
```

The previous task _has_ run however. No errors appeared in my Airflow logs.

When I try to run the task with **Ignore All Deps** enabled, I get the error:

```
Could not queue task instance for execution, dependencies not met: Previous Dagrun State: depends_on_past is true for this task's DAG, but the previous task instance has not run yet., Task has been mapped: The task has yet to be mapped!
```

This last bit is a contradiction, the task cannot be mapped and not mapped simultaneously.

If the amount of mapped tasks is 0 while in this erroneous state, the mapped tasks will not be marked as skipped as expected.

### What you think should happen instead

The mapped tasks should not get stuck with "no status".

The mapped tasks should be created and ran successfully, or in the case of a 0-length list output of the upstream task they should be skipped.

### How to reproduce

Run the below DAG, if it runs successfully clear several tasks out of order. This may not immediately reproduce the bug, but after some task clearing, for me it always ends up in the faulty state described above.

```
from airflow import DAG
from airflow.decorators import task
import datetime as dt

from airflow.operators.python import PythonOperator
import random


@task
def get_filenames_kwargs():
    return [
        {"file_name": i}
        for i in range(random.randint(0, 2))
    ]

def print_filename(file_name):
    print(file_name)

with DAG(
        dag_id="dtm_test_2",
        start_date=dt.datetime(2023, 2, 10),
        default_args={
            "owner": "airflow",
            "depends_on_past": True,
        },
        schedule="@daily",
) as dag:
    get_filenames_task = get_filenames_kwargs.override(task_id="get_filenames_task")()

    print_filename_task = PythonOperator.partial(
        task_id="print_filename_task",
        python_callable=print_filename,
    ).expand(op_kwargs=get_filenames_task)
```

### Operating System

Amazon Linux v2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/dep_context.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py']
Ground Truth : ['a/airflow/ti_deps/deps/prev_dagrun_dep.py']
Current Recall: 0.06788263485265629

=========================================================

ISSUE: Improve idempodency in CloudDataTransferServiceCreateJobOperator
**Description**

We should improve idempodency in [CloudDataTransferServiceCreateJobOperator](https://github.com/apache/airflow/blob/42eef38/airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py#L157). If a Job ID is passed and an AlreadyExist exception raises, then the task should be successful.

This issue is due to [the March 9 service update](https://cloud.google.com/storage-transfer/docs/release-notes#March_09_2020).

> FEATURE: Storage Transfer Service supports custom job IDs.

To complete this task you must complete the following steps:
- [ ] Update the operator
- [ ] Update the example DAG
- [ ] Update the documentation
- [ ] Run system tests (final check)

If you haven't used the GCP yet, after creating the account you will [get $300](https://cloud.google.com/free), which will allow you to get to know these services better.

The implementation of this task will allow a better understanding of GCP services, as well as learn methods of testing that is required by the community. If anyone is interested in this task, I am willing to provide all the necessary tips and information.

**Use case / motivation**

N/A

**Related Issues**

https://github.com/apache/airflow/issues/8284

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_storage_transfer_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_memorystore.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py']
Ground Truth : ['a/airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py', 'a/airflow/providers/google/cloud/hooks/cloud_storage_transfer_service.py', 'a/airflow/providers/google/cloud/example_dags/example_cloud_storage_transfer_service_aws.py']
Current Recall: 0.06859641072703174

=========================================================

ISSUE: Mapped TriggerDagRunOperator causes SerializationError due to operator_extra_links 'property' object is not iterable 
### Apache Airflow version

2.3.2 (latest released)

### What happened

Hi, I have a kind of issue with launching several subDags via mapping TriggerDagRunOperator (mapping over `conf` parameter). Here is the demo example of my typical DAG:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow import XComArg
from datetime import datetime

with DAG(
    'triggerer',
    schedule_interval=None,
    catchup=False,
    start_date=datetime(2019, 12, 2)
) as dag:

    t1 = PythonOperator(
        task_id='first',
        python_callable=lambda : list(map(lambda i: {"x": i}, list(range(10)))),
    )

    t2 = TriggerDagRunOperator.partial(
        task_id='second',
        trigger_dag_id='mydag'
    ).expand(conf=XComArg(t1))

    t1 >> t2
```

But when Airflow tries to import such DAG it throws the following SerializationError (which I observed both in UI and in $AIRFLOW_HOME/logs/scheduler/latest/<my_dag_name>.py.log):

```
Broken DAG: [/home/aliona/airflow/dags/triggerer_dag.py] Traceback (most recent call last):
  File "/home/aliona/airflow/venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 638, in _serialize_node
    serialize_op['_operator_extra_links'] = cls._serialize_operator_extra_links(
  File "/home/aliona/airflow/venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 933, in _serialize_operator_extra_links
    for operator_extra_link in operator_extra_links:
TypeError: 'property' object is not iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/aliona/airflow/venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1106, in to_dict
    json_dict = {"__version": cls.SERIALIZER_VERSION, "dag": cls.serialize_dag(var)}
  File "/home/aliona/airflow/venv/lib/python3.10/site-packages/airflow/serialization/serialized_objects.py", line 1014, in serialize_dag
    raise SerializationError(f'Failed to serialize DAG {dag.dag_id!r}: {e}')
airflow.exceptions.SerializationError: Failed to serialize DAG 'triggerer': 'property' object is not iterable
```

How it appears in the UI:
![image](https://user-images.githubusercontent.com/23297330/175775674-f3375c0e-7ea7-4b6a-84e8-b02ee8f02062.png)


### What you think should happen instead

I think that TriggerDagRunOperator mapped over `conf` parameter should serialize and work well by default. 

During the debugging process and trying to make everything work I found out that simple non-mapped TriggerDagRunOperator has value `['Triggered DAG']` in `operator_extra_links` field, so, it is Lisr. But as for mapped TriggerDagRunOperator, it is 'property'. I don't have any idea why during the serialization process Airflow cannot get value of this property, but I tried to reinitialize this field with `['Triggered DAG']` value and finally I fixed this issue in a such way.

For now, for every case of using mapped TriggerDagRunOperator I also use such code at the end of my dag file:
```python
# here 'second' is the name of corresponding mapped TriggerDagRunOperator task (see demo code above)
t2_patch = dag.task_dict['second']
t2_patch.operator_extra_links=['Triggered DAG']
dag.task_dict.update({'second': t2_patch})
```

So, for every mapped TriggerDagRunOperator task I manually change value of operator_extra_links property to `['Triggered DAG']` and as a result there is no any SerializationError. I have a lot of such cases, and all of them are working good with this fix, all subDags are launched, mapped configuration is passed correctly. Also I can wait for end of their execution or not, all this options also work correctly. 

### How to reproduce

1. Create dag with mapped TriggerDagRunOperator tasks (main parameters such as task_id, trigger_dag_id and others are in `partial section`, in `expand` section use conf parameter with non-empty iterable value), as, for example:
```python
t2 = TriggerDagRunOperator.partial(
        task_id='second',
        trigger_dag_id='mydag'
    ).expand(conf=[{'x': 1}])
```

2. Try to serialize dag, and error will appear.

The full example of failing dag file:
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow import XComArg
from datetime import datetime

with DAG(
    'triggerer',
    schedule_interval=None,
    catchup=False,
    start_date=datetime(2019, 12, 2)
) as dag:

    t1 = PythonOperator(
        task_id='first',
        python_callable=lambda : list(map(lambda i: {"x": i}, list(range(10)))),
    )

    t2 = TriggerDagRunOperator.partial(
        task_id='second',
        trigger_dag_id='mydag'
    ).expand(conf=[{'a': 1}])

    t1 >> t2

# uncomment these lines to fix an error
# t2_patch = dag.task_dict['second']
# t2_patch.operator_extra_links=['Triggered DAG']
# dag.task_dict.update({'second': t2_patch})
```
As subDag ('mydag') I use these DAG:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

with DAG(
    'mydag',
    schedule_interval=None,
    catchup=False,
    start_date=datetime(2019, 12, 2)
) as dag:
    t1 = PythonOperator(
        task_id='first',
        python_callable=lambda : print("first"),
    )
    t2 = PythonOperator(
        task_id='second',
        python_callable=lambda : print("second"),
    )
    t1 >> t2
```

### Operating System

Ubuntu 22.04 LTS

### Versions of Apache Airflow Providers

apache-airflow-providers-ftp==2.1.2
apache-airflow-providers-http==2.1.2
apache-airflow-providers-imap==2.2.3
apache-airflow-providers-sqlite==2.1.3


### Deployment

Virtualenv installation

### Deployment details

Python 3.10.4
pip 22.0.2

### Anything else

Currently for demonstration purposes I am using fully local Airflow installation: single node, SequentialExecutor and SQLite database backend. But such issue also appeared for multi-node installation with either CeleryExecutor or LocalExecutor and PostgreSQL database in the backend.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py']
Ground Truth : ['a/airflow/operators/trigger_dagrun.py']
Current Recall: 0.06859641072703174

=========================================================

ISSUE: IntegrityError inserting into task_fail table with null execution_date from TI.handle_failure_with_callback
### Apache Airflow version

2.2.0 (latest released)

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon @ file:///root/.cache/pypoetry/artifacts/c9/69/16/ffa2eb7a2e6e850a7048eaf66b6c40c990ef7c58149f20d3d3f333a2e9/apache_airflow_providers_amazon-2.2.0-py3-none-any.whl                                                                                                                                                                                         
apache-airflow-providers-celery @ file:///root/.cache/pypoetry/artifacts/6e/1b/2f/f968318a7474e979af4dc53893ecafe8cd11a98a94077a9c3c27304eb7/apache_airflow_providers_celery-2.1.0-py3-none-any.whl                                                                                                                                                                                         
apache-airflow-providers-ftp @ file:///root/.cache/pypoetry/artifacts/8b/9a/dd/79a36c62bc7f37f98d0ea33652570e19272e8a7a2297db13a6785698d1/apache_airflow_providers_ftp-2.0.1-py3-none-any.whl 
apache-airflow-providers-http @ file:///root/.cache/pypoetry/artifacts/52/28/81/03a89147daf7daceb55f1218189d1c4af01c33c45849b568769ca6765f/apache_airflow_providers_http-2.0.1-py3-none-any.whl                                                                                                                                                                                             
apache-airflow-providers-imap @ file:///root/.cache/pypoetry/artifacts/1c/5d/c5/269e8a8098e7017a26a2a376eb3020e1a864775b7ff310ed39e1bd503d/apache_airflow_providers_imap-2.0.1-py3-none-any.whl                                                                                                                                                                                             
apache-airflow-providers-postgres @ file:///root/.cache/pypoetry/artifacts/fb/69/ac/e8e25a0f6a4b0daf162c81c9cfdbb164a93bef6bd652c1c00eee6e0815/apache_airflow_providers_postgres-2.3.0-py3-none-any.whl                                                                                                                                                                                     
apache-airflow-providers-redis @ file:///root/.cache/pypoetry/artifacts/cf/2b/56/75563b6058fe45b70f93886dd92541e8349918eeea9d70c703816f2639/apache_airflow_providers_redis-2.0.1-py3-none-any.whl                                                                                                                                                                                           
apache-airflow-providers-sqlite @ file:///root/.cache/pypoetry/artifacts/61/ba/e9/c0b4b7ef2599dbd902b32afc99f2620d8a616b3072122e90f591de4807/apache_airflow_providers_sqlite-2.0.1-py3-none-any.whl  
```

### Deployment

Other Docker-based deployment

### Deployment details

AWS ECS, Celery Executor, Postgres 13, S3 Logging, Sentry integration

### What happened

Noticed our Sentry getting a lot of integrity errors inserting into the task_fail table with a null execution date.

This seemed to be caused specifically by zombie task failures (We use AWS ECS Spot instances).

Specifically this callback from the dag file processor:

https://github.com/apache/airflow/blob/e6c56c4ae475605636f4a1b5ab3884383884a8cf/airflow/models/taskinstance.py#L1746

Adds a task_fail here: https://github.com/apache/airflow/blob/e6c56c4ae475605636f4a1b5ab3884383884a8cf/airflow/models/taskinstance.py#L1705

This blows up when it flushes further down the method. This i believe is because when the task instance is refreshed from the database the `self.dag_run` property is not populated. The proxy from `ti.execution_date` to `ti.dag_run.execution_date` then returns `None` causing our `NOT NULL` violation.

### What you expected to happen

Insert into task_fail successfully and trigger callback

### How to reproduce

Run this dag:

```python
import logging
import time
from datetime import datetime

from airflow import DAG
from airflow.operators.python import PythonOperator


def long_running_task():
    for i in range(60):
        time.sleep(5)
        logging.info("Slept for 5")


def log_failure_dag(*args, **kwargs):
    logging.error("Our failure callback")


dag = DAG(
    dag_id="test_null_task_fail",
    schedule_interval='@daily',
    catchup=True,
    start_date=datetime(2021, 10, 9),
    max_active_runs=1,
    max_active_tasks=1,
    on_failure_callback=log_failure_dag,
)

with dag:
    PythonOperator(
        task_id="long_running",
        python_callable=long_running_task,
        on_failure_callback=log_failure_dag
    )
```

Kill the celery worker whilst its executing the long_running tasks. Wait for the zombie reaper of the scheduler to begin and call the failure handler.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.06859641072703174

=========================================================

ISSUE: Improve audit log 
### Discussed in https://github.com/apache/airflow/discussions/25638

See the discussion. There are a couple of improvements that can be done:

* add atribute to download the log rather than open it in-browser
* add .log or similar (.txt?) extension
* sort the output
* possibly more

<div type='discussions-op-text'>

<sup>Originally posted by **V0lantis** August 10, 2022</sup>
### Apache Airflow version

2.3.3

### What happened

The audit log link crashes because there is too much data displayed.


### What you think should happen instead

The windows shouldn't crash

### How to reproduce

Display a dag audit log with thousand or millions lines should do the trick

### Operating System
```
NAME="Amazon Linux" VERSION="2" ID="amzn" ID_LIKE="centos rhel fedora" VERSION_ID="2" PRETTY_NAME="Amazon Linux 2" ANSI_COLOR="0;33" CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2" HOME_URL="https://amazonlinux.com/"
```
### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==4.0.0
apache-airflow-providers-celery==3.0.0
apache-airflow-providers-cncf-kubernetes==4.1.0
apache-airflow-providers-datadog==3.0.0
apache-airflow-providers-docker==3.0.0
apache-airflow-providers-ftp==3.0.0
apache-airflow-providers-github==2.0.0
apache-airflow-providers-google==8.1.0
apache-airflow-providers-grpc==3.0.0
apache-airflow-providers-hashicorp==3.0.0
apache-airflow-providers-http==3.0.0
apache-airflow-providers-imap==3.0.0
apache-airflow-providers-jira==3.0.0
apache-airflow-providers-mysql==3.0.0
apache-airflow-providers-postgres==5.0.0
apache-airflow-providers-redis==3.0.0
apache-airflow-providers-sftp==3.0.0
apache-airflow-providers-slack==5.0.0
apache-airflow-providers-sqlite==3.0.0
apache-airflow-providers-ssh==3.0.0
apache-airflow-providers-tableau==3.0.0
apache-airflow-providers-zendesk==4.0.0
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

k8s

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.06859641072703174

=========================================================

ISSUE: Update slackapiclient / slack_sdk to v3
Hello,

Slack has released updates to its library and we can start using it for it. 

We especially like one change.

> slack_sdk has no required dependencies. This means aiohttp is no longer automatically resolved.

I've looked through the documentation and it doesn't look like a difficult task, but I think it's still worth testing.
More info: https://slack.dev/python-slack-sdk/v3-migration/index.html#from-slackclient-2-x

Best regards,
Kamil Bregua

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/scheduler_dag_execution_timing.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/providers/slack/hooks/slack.py', 'a/setup.py', 'a/docs/conf.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: Scheduler deadlock error when mapping over empty list
### Apache Airflow version

2.3.0b1 (pre-release)

### What happened

manually triggered this dag:
```python
from datetime import datetime
from airflow import DAG

with DAG(
    dag_id="null_mapped_2",
    start_date=datetime(1970, 1, 1),
    schedule_interval=None,
) as dag:

    @dag.task
    def empty():
        return []

    @dag.task
    def print_it(thing):
        print(thing)

    print_it.expand(thing=empty())

```

scheduler logs (whitespace added for emphasis):
```
   ____________       _____________
  ____    |__( )_________  __/__  /________      __
 ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
 ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
 [2022-04-20 20:46:24,760] {scheduler_job.py:697} INFO - Starting the scheduler
 [2022-04-20 20:46:24,761] {scheduler_job.py:702} INFO - Processing each file at most -1 times
 [2022-04-20 20:46:24 +0000] [28] [INFO] Starting gunicorn 20.1.0
 [2022-04-20 20:46:24 +0000] [28] [INFO] Listening at: http://0.0.0.0:8793 (28)
 [2022-04-20 20:46:24 +0000] [28] [INFO] Using worker: sync
 [2022-04-20 20:46:24 +0000] [29] [INFO] Booting worker with pid: 29
 [2022-04-20 20:46:24,782] {executor_loader.py:106} INFO - Loaded executor: LocalExecutor
 [2022-04-20 20:46:24 +0000] [78] [INFO] Booting worker with pid: 78
 [2022-04-20 20:46:24,953] {manager.py:156} INFO - Launched DagFileProcessorManager with pid: 166
 [2022-04-20 20:46:24,962] {settings.py:55} INFO - Configured default timezone Timezone('UTC')
 /usr/local/lib/python3.9/site-packages/airflow/configuration.py:466 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
 [2022-04-20 20:46:24,988] {scheduler_job.py:1231} INFO - Resetting orphaned tasks for active dag runs
 /usr/local/lib/python3.9/site-packages/airflow/configuration.py:466 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
 [2022-04-20 20:46:32,124] {update_checks.py:128} INFO - Checking for new version of Astronomer Certified Airflow, previous check was performed at None
 [2022-04-20 20:46:32,441] {update_checks.py:84} INFO - Check finished, next check in 86400.0 seconds
 /usr/local/lib/python3.9/site-packages/airflow/configuration.py:466 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
 [2022-04-20 20:47:45,296] {scheduler_job.py:354} INFO - 1 tasks up for execution:
 	<TaskInstance: null_mapped_2.empty manual__2022-04-20T20:47:45.075321+00:00 [scheduled]>
 [2022-04-20 20:47:45,297] {scheduler_job.py:419} INFO - DAG null_mapped_2 has 0/16 running and queued tasks
 [2022-04-20 20:47:45,297] {scheduler_job.py:505} INFO - Setting the following tasks to queued state:
 	<TaskInstance: null_mapped_2.empty manual__2022-04-20T20:47:45.075321+00:00 [scheduled]>
 [2022-04-20 20:47:45,300] {scheduler_job.py:547} INFO - Sending TaskInstanceKey(dag_id='null_mapped_2', task_id='empty', run_id='manual__2022-04-20T20:47:45.075321+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
 [2022-04-20 20:47:45,300] {base_executor.py:88} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'null_mapped_2', 'empty', 'manual__2022-04-20T20:47:45.075321+00:00', '--local', '--subdir', 'DAGS_FOLDER/null_mapped_2.py']
 [2022-04-20 20:47:45,303] {local_executor.py:79} INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'null_mapped_2', 'empty', 'manual__2022-04-20T20:47:45.075321+00:00', '--local', '--subdir', 'DAGS_FOLDER/null_mapped_2.py']
 [2022-04-20 20:47:45,340] {dagbag.py:507} INFO - Filling up the DagBag from /usr/local/airflow/dags/null_mapped_2.py
 /usr/local/lib/python3.9/site-packages/airflow/configuration.py:466 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
 [2022-04-20 20:47:45,432] {task_command.py:369} INFO - Running <TaskInstance: null_mapped_2.empty manual__2022-04-20T20:47:45.075321+00:00 [queued]> on host 7ec8e95b149d


 [2022-04-20 20:47:46,796] {dagrun.py:583} ERROR - Deadlock; marking run <DagRun null_mapped_2 @ 2022-04-20 20:47:45.075321+00:00: manual__2022-04-20T20:47:45.075321+00:00, externally triggered: True> failed


 [2022-04-20 20:47:46,797] {dagrun.py:607} INFO - DagRun Finished: dag_id=null_mapped_2, execution_date=2022-04-20 20:47:45.075321+00:00, run_id=manual__2022-04-20T20:47:45.075321+00:00, run_start_date=2022-04-20 20:47:45.254176+00:00, run_end_date=2022-04-20 20:47:46.797390+00:00, run_duration=1.543214, state=failed, external_trigger=True, run_type=manual, data_interval_start=2022-04-20 20:47:45.075321+00:00, data_interval_end=2022-04-20 20:47:45.075321+00:00, dag_hash=8476a887126cf6d52573ee41fa81c637
 [2022-04-20 20:47:46,801] {dag.py:2894} INFO - Setting next_dagrun for null_mapped_2 to None, run_after=None
 [2022-04-20 20:47:46,818] {scheduler_job.py:600} INFO - Executor reports execution of null_mapped_2.empty run_id=manual__2022-04-20T20:47:45.075321+00:00 exited with status success for try_number 1
 [2022-04-20 20:47:46,831] {scheduler_job.py:644} INFO - TaskInstance Finished: dag_id=null_mapped_2, task_id=empty, run_id=manual__2022-04-20T20:47:45.075321+00:00, map_index=-1, run_start_date=2022-04-20 20:47:45.491953+00:00, run_end_date=2022-04-20 20:47:45.760654+00:00, run_duration=0.268701, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-04-20 20:47:45.297857+00:00, queued_by_job_id=1, pid=250
```

The dagrun fails, even though none of its tasks get set to failed:

<img width="319" alt="Screen Shot 2022-04-20 at 2 43 15 PM" src="https://user-images.githubusercontent.com/5834582/164320377-663fcc0a-0bc4-4edc-8cc3-91884b84748d.png">



### What you think should happen instead

Mapping over a 0 length list should create no map_index'ed tasks, but the parent task should succeed because none of the 0 tasks failed.

### How to reproduce

Run the dag above

### Operating System

debian (docker)

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

`astrocloud dev start`, image contains version 2.3.0.dev20220414

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/flink/sensors/test_flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_xcom_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/triggers/test_external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/hooks/test_dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_mapped_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_base.py']
Ground Truth : ['a/airflow/models/mappedoperator.py', 'a/airflow/models/dagrun.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: API server /plugin crashes
### Apache Airflow version

2.3.3 (latest released)

### What happened

The `/plugins` endpoint returned a 500 http status code.

```
curl -X GET http://localhost:8080/api/v1/plugins\?limit\=1  \
   -H 'Cache-Control: no-cache' \
   --user "admin:admin"
{
  "detail": "\"{'name': 'Test View', 'category': 'Test Plugin', 'view': 'test.appbuilder_views.TestAppBuilderBaseView'}\" is not of type 'object'\n\nFailed validating 'type' in schema['allOf'][0]['properties']['plugins']['items']['properties']['appbuilder_views']['items']:\n    {'nullable': True, 'type': 'object'}\n\nOn instance['plugins'][0]['appbuilder_views'][0]:\n    (\"{'name': 'Test View', 'category': 'Test Plugin', 'view': \"\n     \"'test.appbuilder_views.TestAppBuilderBaseView'}\")",
  "status": 500,
  "title": "Response body does not conform to specification",
  "type": "http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/stable-rest-api-ref.html#section/Errors/Unknown"
}
```

The error message in the webserver is as followed

```
[2022-08-03 17:07:57,705] {validation.py:244} ERROR - http://localhost:8080/api/v1/plugins?limit=1 validation error: "{'name': 'Test View', 'category': 'Test Plugin', 'view': 'test.appbuilder_views.TestAppBuilderBaseView'}" is not of type 'object'

Failed validating 'type' in schema['allOf'][0]['properties']['plugins']['items']['properties']['appbuilder_views']['items']:
    {'nullable': True, 'type': 'object'}

On instance['plugins'][0]['appbuilder_views'][0]:
    ("{'name': 'Test View', 'category': 'Test Plugin', 'view': "
     "'test.appbuilder_views.TestAppBuilderBaseView'}")
172.18.0.1 - admin [03/Aug/2022:17:10:17 +0000] "GET /api/v1/plugins?limit=1 HTTP/1.1" 500 733 "-" "curl/7.79.1"
```

### What you think should happen instead

The response should contain all the plugins integrated with Airflow.

### How to reproduce

Create a simple plugin in the plugin directory.

`appbuilder_views.py`

```
from flask_appbuilder import expose, BaseView as AppBuilderBaseView


# Creating a flask appbuilder BaseView
class TestAppBuilderBaseView(AppBuilderBaseView):
    @expose("/")
    def test(self):
        return self.render_template("test_plugin/test.html", content="Hello galaxy!")

```

`plugin.py`

```
from airflow.plugins_manager import AirflowPlugin
from test.appbuilder_views import TestAppBuilderBaseView


class TestPlugin(AirflowPlugin):
    name = "test"

    appbuilder_views = [
        {
            "name": "Test View",
            "category": "Test Plugin",
            "view": TestAppBuilderBaseView()
        }
    ]

```

Call the `/plugin` endpoint.

```
curl -X GET http://localhost:8080/api/v1/plugins\?limit\=1  \
   -H 'Cache-Control: no-cache' \
   --user "admin:admin"
```

### Operating System

N/A

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/plugins/test_plugin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/plugins/test_plugins_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_plugin_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow/empty_plugin/empty_plugin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_plugins_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/plugins_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_plugin_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_extra_link_endpoint.py']
Ground Truth : ['a/airflow/api_connexion/endpoints/plugin_endpoint.py', 'a/airflow/api_connexion/schemas/plugin_schema.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: "Not a valid timetable" when returning None from next_dagrun_info in a custom timetable
### Apache Airflow version

2.2.0 (latest released)

### Operating System

Mac

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

Getting an exception when returning None from next_dagrun_info in a custom timetable. The timetable protocol says that when None is returned a DagRun will not happen but right now it throws an exception.

Exception:
```
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 623, in _execute
    self._run_scheduler_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 704, in _run_scheduler_loop
    num_queued_tis = self._do_scheduling(session)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 787, in _do_scheduling
    callback_to_run = self._schedule_dag_run(dag_run, session)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 1039, in _schedule_dag_run
    self._update_dag_next_dagruns(dag, dag_model, active_runs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py", line 930, in _update_dag_next_dagruns
    data_interval = dag.get_next_data_interval(dag_model)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 629, in get_next_data_interval
    return self.infer_automated_data_interval(dag_model.next_dagrun)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 671, in infer_automated_data_interval
    raise ValueError(f"Not a valid timetable: {self.timetable!r}")
ValueError: Not a valid timetable: <my_timetables.workday_timetable.WorkdayTimetable object at 0x7f42b1f02430>
```

### What you expected to happen

DagRun to not happen.

### How to reproduce

Create a custom timetable and return None in next_dagrun_info after a few DagRuns are created by that timetable

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py']
Ground Truth : ['a/airflow/models/dag.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: Display task documentation when hovering on the task
### Description

It would be great if task documentation wouldn't be hidden within the task instance page, but rather displayed when hovering over the task.

### Use case/motivation

In complex DAGs, having access to this allows access to documentation with a few less clicks per task

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py']
Ground Truth : ['a/airflow/www/utils.py', 'a/airflow/www/views.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: AzureContainerInstancesOperator is not working due to argument error
### Apache Airflow Provider(s)

microsoft-azure

### Versions of Apache Airflow Providers

3.1.0

### Apache Airflow version

2.1.3 (latest released)

### Operating System

Ubuntu

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

I'm getting this error when trying to run AzureContainerInstancesOperator. I believe its due to AzureContainerInstanceHook initialization when passing self.ci_conn_id as a positional argument.

https://github.com/apache/airflow/blob/79d85573591f641db4b5f89a12213e799ec6dea1/airflow/providers/microsoft/azure/operators/azure_container_instances.py#L202

Should this be?
```py
        self._ci_hook = AzureContainerInstanceHook(conn_id=self.ci_conn_id)
```

Error:
```
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1164, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1282, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1312, in _execute_task
    result = task_copy.execute(context=context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/microsoft/azure/operators/azure_container_instances.py", line 202, in execute
    self._ci_hook = AzureContainerInstanceHook(self.ci_conn_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/microsoft/azure/hooks/azure_container_instance.py", line 45, in __init__
    super().__init__(sdk_client=ContainerInstanceManagementClient, *args, **kwargs)
TypeError: __init__() got multiple values for argument 'sdk_client'
```




### What you expected to happen

_No response_

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/container_instances.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/providers/microsoft/azure/operators/container_instances.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: Could not get scheduler_job_id
**Apache Airflow version:**

2.0.0

**Kubernetes version (if you are using kubernetes) (use kubectl version):**

1.18.3

**Environment:**

Cloud provider or hardware configuration: AWS

**What happened:**

When trying to run a DAG, it gets scheduled, but task is never run. When attempting to run task manually, it shows an error:

```
Something bad has happened.
Please consider letting us know by creating a bug report using GitHub.

Python version: 3.8.7
Airflow version: 2.0.0
Node: airflow-web-ffdd89d6-h98vj
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/usr/local/lib/python3.8/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.8/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.8/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.8/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.8/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.8/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.8/site-packages/airflow/www/auth.py", line 34, in decorated
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/airflow/www/decorators.py", line 60, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.8/site-packages/airflow/www/views.py", line 1366, in run
    executor.start()
  File "/usr/local/lib/python3.8/site-packages/airflow/executors/kubernetes_executor.py", line 493, in start
    raise AirflowException("Could not get scheduler_job_id")
airflow.exceptions.AirflowException: Could not get scheduler_job_id
```

**What you expected to happen:**

The task to be run successfully without 

**How to reproduce it:**

Haven't pinpointed what causes the issue, besides an attempted upgrade from Airflow 1.10.14 to Airflow 2.0.0

**Anything else we need to know:**

This error is encountered in an upgrade of Airflow from 1.10.14 to Airflow 2.0.0

EDIT: Formatted to fit the issue template

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_mypy_folder.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: [Airflow 2.2.2] execution_date Proxy object - str formatting error 
### Apache Airflow version

2.2.2 (latest released)

### Operating System

Ubuntu 18.04.6

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

The deprecated variable `execution_date` raises an error when used in an f string template with date string formatting.

```python
In [1]: execution_date
DeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'logical_date' or 'data_interval_start' instead.
Out[1]: <Proxy at 0x7fb6f9af81c0 wrapping DateTime(2021, 11, 18, 0, 0, 0, tzinfo=Timezone('UTC')) at 0x7fb6f9aeff90 with factory <function TaskInstance.get_template_context.<locals>.deprecated_proxy.<locals>.deprecated_func at 0x7fb6f98699d0>>

In [2]: f"{execution_date:%Y-%m-%d}"
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
----> 1 f"{execution_date:%Y-%m-%d}"

TypeError: unsupported format string passed to Proxy.__format__
```



### What you expected to happen

Executing `f"{execution_date:%Y-%m-%d}"` should return a string and not raise an error. 

### How to reproduce

```python
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator


def test_str_fmt(execution_date: datetime):
    return f"{execution_date:%Y-%m-%d}"


dag = DAG(
    dag_id="Test_Date_String",
    schedule_interval="@daily",
    catchup=False,
    default_args={
        "depends_on_past": False,
        "start_date": datetime(2021, 11, 1),
        "email": None,
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 0,
    },
)

with dag:
    test_task = PythonOperator(
        task_id="test_task",
        python_callable=test_str_fmt,
    )
```

### Anything else


```python
from datetime import datetime
...
datetime.fromisoformat(next_ds)
TypeError: fromisoformat: argument must be str
```


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

Custom deprecation proxy implementing __format__
Fix #19716.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py']
Ground Truth : ['a/airflow/models/taskinstance.py', '/dev/null', 'a/airflow/macros/__init__.py', 'a/airflow/operators/python.py', 'a/airflow/utils/operator_helpers.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: Update source_code field of dag_code table to MEDIUMTEXT
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

**Description**

Update source_code field of dag_code table to MEDIUMTEXT
<!-- A short description of your feature -->

**Use case / motivation**

Lot of dags exceed the limit of 65K characters limit giving error `"Data too long for column 'source_code' at row 1"` when enabling webserver to fetch dag_code from db. 
<!-- What do you want to happen?

Rather than telling us how you might implement this solution, try to take a
step back and describe what you are trying to achieve.

-->

**Related Issues**

<!-- Is there currently another issue associated with this? -->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagcode.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py']
Ground Truth : ['/dev/null', 'a/airflow/models/dagcode.py', 'a/airflow/migrations/versions/4addfa1236f1_add_fractional_seconds_to_mysql_tables.py', 'a/airflow/migrations/versions/d2ae31099d61_increase_text_size_for_mysql.py']
Current Recall: 0.06931018660140718

=========================================================

ISSUE: SAWarning: TypeDecorator UtcDateTime(timezone=True) will not produce a cache key because the ``cache_ok`` attribute is not set to True
### Apache Airflow version

2.2.4 (latest released)

### What happened

Error 

```
[2022-03-31, 11:47:06 UTC] {warnings.py:110} WARNING - /home/ec2-user/.local/lib/python3.7/site-packages/airflow/models/xcom.py:437: SAWarning: TypeDecorator UtcDateTime(timezone=True) will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)
  return query.delete()

[2022-03-31, 11:47:06 UTC] {warnings.py:110} WARNING - /home/ec2-user/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py:2214: SAWarning: TypeDecorator UtcDateTime(timezone=True) will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)
  for result in query.with_entities(XCom.task_id, XCom.value)

[2022-03-31, 11:47:06 UTC] {warnings.py:110} WARNING - /home/ec2-user/.local/lib/python3.7/site-packages/airflow/models/renderedtifields.py:126: SAWarning: TypeDecorator UtcDateTime(timezone=True) will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)
  session.merge(self)

[2022-03-31, 11:47:06 UTC] {warnings.py:110} WARNING - /home/ec2-user/.local/lib/python3.7/site-packages/airflow/models/renderedtifields.py:162: SAWarning: Coercing Subquery object into a select() for use in IN(); please pass a select() construct explicitly
  tuple_(cls.dag_id, cls.task_id, cls.execution_date).notin_(subq1),

[2022-03-31, 11:47:06 UTC] {warnings.py:110} WARNING - /home/ec2-user/.local/lib/python3.7/site-packages/airflow/models/renderedtifields.py:163: SAWarning: TypeDecorator UtcDateTime(timezone=True) will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)
  ).delete(synchronize_session=False)
```

### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Operating System

NAME="Amazon Linux" VERSION="2" ID="amzn" ID_LIKE="centos rhel fedora" VERSION_ID="2" PRETTY_NAME="Amazon Linux 2" ANSI_COLOR="0;33" CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2" HOME_URL="https://amazonlinux.com/"

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==3.0.0
apache-airflow-providers-celery==2.1.0
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-http==2.0.3
apache-airflow-providers-imap==2.2.0
apache-airflow-providers-postgres==3.0.0
apache-airflow-providers-redis==2.0.1
apache-airflow-providers-sqlite==2.1.0

### Deployment

Other

### Deployment details

Pip package

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py']
Ground Truth : ['a/airflow/utils/sqlalchemy.py']
Current Recall: 0.07145151422453352

=========================================================

ISSUE: Task preceeding PythonVirtualenvOperator fails: "cannot pickle 'module' object"
**Apache Airflow version**

13faa6912f7cd927737a1dc15630d3bbaf2f5d4d

**Environment**

- **Configuration**: Local Executor
- **OS** (e.g. from /etc/os-release): Mac OS 11.3
- **Kernel**: Darwin Kernel Version 20.4.0
- **Install tools**: `pip install -e .`

**The DAG**

```python
def callable():
    print("hi")

with DAG(dag_id="two_virtualenv") as dag:

    a = PythonOperator(
        task_id="a",
        python_callable=callable,
    )

    # b = PythonOperator(          # works
    b = PythonVirtualenvOperator(  # doesn't work
        task_id="b",
        python_callable=callable,
    )

    a >> b
```

**What happened**:
Failure somewhere between first task and second:
```
INFO - Marking task as SUCCESS. dag_id=two_virtualenv, task_id=a
ERROR - Failed to execute task: cannot pickle 'module' object.
Traceback (most recent call last):
  File "/Users/matt/src/airflow/airflow/executors/debug_executor.py", line 79, in _run_task
    ti._run_raw_task(job_id=ti.job_id, **params)  # pylint: disable=protected-access
  File "/Users/matt/src/airflow/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Users/matt/src/airflow/airflow/models/taskinstance.py", line 1201, in _run_raw_task
    self._run_mini_scheduler_on_child_tasks(session)
  File "/Users/matt/src/airflow/airflow/utils/session.py", line 67, in wrapper
    return func(*args, **kwargs)
  File "/Users/matt/src/airflow/airflow/models/taskinstance.py", line 1223, in _run_mini_scheduler_on_child_tasks
    partial_dag = self.task.dag.partial_subset(
  File "/Users/matt/src/airflow/airflow/models/dag.py", line 1490, in partial_subset
    dag.task_dict = {
  File "/Users/matt/src/airflow/airflow/models/dag.py", line 1491, in <dictcomp>
    t.task_id: copy.deepcopy(t, {id(t.dag): dag})  # type: ignore
  File "/usr/local/Cellar/python@3.9/3.9.4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/Users/matt/src/airflow/airflow/models/baseoperator.py", line 961, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))  # noqa
  File "/usr/local/Cellar/python@3.9/3.9.4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/copy.py", line 161, in deepcopy
    rv = reductor(4)
TypeError: cannot pickle 'module' object
ERROR - Task instance <TaskInstance: two_virtualenv.a 2021-05-11 00:00:00+00:00 [failed]> failed
```

**What you expected to happen**:

Both tasks say "hi" and succeed

**To Replicate**

The DAG and output above are shortened for brevity.  A more complete story: https://gist.github.com/MatrixManAtYrService/6b27378776470491eb20b60e01cfb675

Ran it like this:
```
 $ airflow dags test two_virtualenv $(date "+%Y-%m-%d")
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py']
Ground Truth : ['a/airflow/operators/python.py', 'a/airflow/models/dag.py']
Current Recall: 0.07145151422453352

=========================================================

ISSUE: A dag's schedule interval can no longer be an instance of dateutils.relativedelta
### Apache Airflow version

2.2.1 (latest released)

### Operating System

debian

### Versions of Apache Airflow Providers

apache-airflow==2.2.1
apache-airflow-providers-amazon==2.3.0
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==6.0.0
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-jira==2.0.1
apache-airflow-providers-mysql==2.1.1
apache-airflow-providers-postgres==2.3.0
apache-airflow-providers-redis==2.0.1
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.2.0

### Deployment

Other Docker-based deployment

### Deployment details

Dask executor, custom-built Docker images, postgres 12.7 backend

### What happened

I upgraded Airflow from 2.0.2 to 2.2.1, and some DAGs I have that used dateutils.relativedelta objects as schedule intervals stopped running

### What you expected to happen

The [code](https://github.com/apache/airflow/blob/2.2.1/airflow/models/dag.py#L101) for the schedule_interval parameter of the DAG constructor indicates that a relativedelta object is allowed, so I expected the DAG to be correctly parsed and scheduled.

### How to reproduce

Create a DAG that has a relativedelta object as its schedule interval, and it will not appear in the UI or be scheduled.

### Anything else

Here is the code that causes the failure within the PR where it was introduced: [link](https://github.com/apache/airflow/pull/17414/files#diff-ed37fe966e8247e0bfd8aa28bc2698febeec3807df5f5a00545ca80744f8aff6R267)

Here are the logs for the exception, found in the scheduler logs for the file that contains the offending DAG
<details><pre>
ERROR   | {dagbag.py:528} - 'relativedelta' object has no attribute 'total_seconds'
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 515, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 298, in process_file
    found_dags = self._process_modules(filepath, mods, file_last_changed_on_disk)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 401, in _process_modules
    dag.timetable.validate()
  File "/usr/local/lib/python3.9/site-packages/airflow/timetables/interval.py", line 274, in validate
    if self._delta.total_seconds() <= 0:
AttributeError: 'relativedelta' object has no attribute 'total_seconds'
</pre></details>

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py']
Ground Truth : ['a/airflow/models/param.py', 'a/airflow/utils/cli.py', 'a/airflow/www/security.py', 'a/setup.py', 'a/airflow/ti_deps/deps/pool_slots_available_dep.py', 'a/airflow/utils/db.py', 'a/airflow/timetables/interval.py', 'a/airflow/www/views.py', 'a/airflow/api/common/experimental/trigger_dag.py', 'a/airflow/migrations/versions/7b2661a43ba3_taskinstance_keyed_to_dagrun.py', 'a/airflow/models/variable.py', 'a/airflow/executors/kubernetes_executor.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/models/dagrun.py', 'a/airflow/dag_processing/processor.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dag.py', 'a/airflow/cli/commands/standalone_command.py']
Current Recall: 0.07145151422453352

=========================================================

ISSUE: Dag disappears when DAG tag is longer than 100 char limit
### Apache Airflow version

2.2.5

### What happened

We added new DAG tags to a couple of our DAGs. In the case when the tag was longer than the 100 character limit the DAG was not showing in the UI and wasn't scheduled. It was however possible to reach it by typing in the URL to the DAG.
Usually when DAGs are broken there will be an error message in the UI, but this problem did not render any error message.

This problem occurred to one of our templated DAGs. Only one DAG broke and it was the one with a DAG tag which was too long. When we fixed the length, the DAG was scheduled and was visible in the UI again.

### What you think should happen instead

Exclude the dag if it is over the 100 character limit or show an error message in the UI.

### How to reproduce

Add a DAG tag which is longer than 100 characters.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

Running Airflow in Kubernetes.
Syncing DAGs from S3 with https://tech.scribd.com/blog/2020/breaking-up-the-dag-repo.html

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py']
Ground Truth : ['a/airflow/models/dag.py']
Current Recall: 0.07145151422453352

=========================================================

ISSUE: AIP-44 Migrate DagModel.get_paused_dag_ids to Internal API


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_internal/internal_api_call.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/dag_run.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_pydantic_models.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_internal/endpoints/rpc_api_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_internal/test_internal_api_call.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/test_parameters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_internal/endpoints/test_rpc_api_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/pydantic/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/pydantic.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py']
Ground Truth : ['a/airflow/api_internal/internal_api_call.py', 'a/airflow/models/dag.py', 'a/airflow/api_internal/endpoints/rpc_api_endpoint.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Trying to use a nonexistent pool fails silently
Looking at the `pool_full()` method of `TaskInstance` (line 818 of models.py) -- if the TI has a pool specified but the pool doesn't exist in the database, it returns False (and the run continues).

I think maybe if a pool was specified but doesn't exist, it should raise an error instead? After all, the point of a pool is to restrict execution and this behavior lets it go on (silently).


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/api/experimental/test_endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/subdag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/batch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/pool.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/experimental/pool.py']
Ground Truth : ['a/airflow/models.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: TypeError: unsupported operand type(s) for +=: 'NoneType' and 'datetime.timedelta'
### Apache Airflow version

2.2.0 (latest released)

### Operating System

Debian

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

I've upgraded to Airflow 2.2.0 and added a new operator to the existing DAG:

```
delay_sensor = TimeDeltaSensorAsync(task_id="wait", delta=timedelta(hours=24))
```

I've started getting an error:

```
[2021-10-21, 20:01:17 UTC] {taskinstance.py:1686} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1324, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1443, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1499, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/sensors/time_delta.py", line 54, in execute
    target_dttm += self.delta
TypeError: unsupported operand type(s) for +=: 'NoneType' and 'datetime.timedelta'
[2021-10-21, 20:01:17 UTC] {taskinstance.py:1270} INFO - Marking task as UP_FOR_RETRY. dag_id=integration-ups-mpos-2215, task_id=wait, execution_date=20211020T070000, start_date=20211021T200117, end_date=20211021T200117
[2021-10-21, 20:01:17 UTC] {standard_task_runner.py:88} ERROR - Failed to execute job 1458 for task wait
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 292, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py", line 180, in _run_raw_task
    ti._run_raw_task(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1324, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1443, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1499, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/sensors/time_delta.py", line 54, in execute
    target_dttm += self.delta
TypeError: unsupported operand type(s) for +=: 'NoneType' and 'datetime.timedelta'
```

It looks like 2 new fields that were introduced in "dag_run" table:

* data_interval_start
* data_interval_end

I see that these fields are populated correctly for new DAG-s but for the old DAG-s the data is not back-filled.

![image](https://user-images.githubusercontent.com/131281/138353823-aaede2dc-47fd-4caf-a519-ff758ead8943.png)


### What you expected to happen

The `TimeDeltaSensorAsync` sensor should work correctly even in older DAG-s.

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Can't open task log from Gantt view
### Apache Airflow version

2.2.1

### Operating System

Linux 5.4.149-73.259.amzn2.x86_64

### Versions of Apache Airflow Providers

default for 2.2.1

### Deployment

Other 3rd-party Helm chart

### Deployment details

aws eks using own-developed helm chart

### What happened

When trying to open log from gantt view - receiving an exception
```
  File "/home/airflow/.local/lib/python3.9/site-packages/pendulum/parsing/__init__.py", line 177, in _parse_common
    return date(year, month, day)
ValueError: year 0 is out of range
```

due to incorrect query parameter push: no value for `execution_date` pushed
```
/log?dag_id=report_generator_daily&task_id=check_quints_earnings&execution_date=
```

### What you expected to happen

Logs should be available

### How to reproduce

Open dag's `gantt` chart
click on task ribbon
click on `log`
observe an error

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_acl.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: DAGs deleted from zips aren't deactivated
### Apache Airflow version

2.5.3

### What happened

When a DAG is removed from a zip in the DAGs directory, but the zip file remains, it is not marked correctly as inactive. It is still visible in the UI, and attempting to open the DAG results in an `DAG "mydag" seems to be missing from DagBag.` error in the UI.

The DAG is removed from the SerializedDag table, resulting in the scheduler repeatedly erroring with `[2023-04-12T12:43:51.165+0000] {scheduler_job.py:1063} ERROR - DAG 'mydag' not found in serialized_dag table`.

I have done some minor investigating and it appears that [this piece of code](https://github.com/apache/airflow/blob/2.5.3/airflow/dag_processing/manager.py#L748-L772) may be the cause.

`dag_filelocs` provides the path to a specific python file within a zip, so `SerializedDagModel.remove_deleted_dags` is able to remove the missing DAG.

However, `self._file_paths` only contains the top-level zip name, so `DagModel.deactivate_deleted_dags` will only deactivate DAGs where the zip they are contained in is deleted, regardless of whether the DAG is still inside the zip.

I can see there are [other methods that handle DAG deactivation](https://github.com/apache/airflow/blob/2.5.3/airflow/models/dag.py#L2945-L2968) and I'm not sure how these all interact but this does seem to cause this specific issue.

### What you think should happen instead

DAGS that are no longer in the DagBag are marked as inactive

### How to reproduce

Running airflow locally with docker-compose:
- Create a zipfile with 2 DAG py files in in ./dags
- Wait for the DAGs to be parsed by the scheduler and appear in the UI
- Overwrite the existing DAG zip, with a new zip containing only 1 of the original DAG py files
- Wait for scheduler loop to parse the new zip
- Attempt to open the removed DAG in the UI, you will see an error



### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

If I replace the docker image in the docker compose with an image built from this Dockerfile:

```
FROM apache/airflow:2.5.3
RUN sed -i '772s/self._file_paths/dag_filelocs/' /home/airflow/.local/lib/python3.7/site-packages/airflow/dag_processing/manager.py
RUN sed -i '3351s/correct_maybe_zipped(dag_model.fileloc)/dag_model.fileloc/' /home/airflow/.local/lib/python3.7/site-packages/airflow/models/dag.py
```

The DAG is deactivated as expected

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/file.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_home.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py']
Ground Truth : ['a/airflow/dag_processing/manager.py', 'a/airflow/models/dag.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Manual versioninig with semver for the first release of providers
The first release can be manually versioned with 0.1.0 version for all packages. No automation is yet needed.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/hatch_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docs_publisher.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_mypy_folder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/events.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py']
Ground Truth : ['a/provider_packages/setup_provider_packages.py', 'a/provider_packages/refactor_provider_packages.py', 'a/setup.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Remove duplicate line of code for configuration copy
This is code improvement PR to remove a duplicate line. The code copies config in either cases of IF statement hence duplicate call. 
Moving that before IF makes it unique and sufficient for the purpose.

I have moved first call above IF and removed ELSE block

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/main/UPDATING.md).


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py']
Ground Truth : ['a/airflow/task/task_runner/base_task_runner.py', 'a/airflow/utils/configuration.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Dynamic Task Mapping skips tasks before upstream has started
### Apache Airflow version

2.5.0

### What happened

In some cases we are seeing dynamic mapped task being skipped before upstream tasks have started & the dynamic count for the task can be calculated. We see this both locally in a with the `LocalExecutor` & on our cluster with the `KubernetesExecutor`.

To trigger the issue we need multiple dynamic tasks merging into a upstream task, see the images below for example. If there is no merging the tasks run as expected. The tasks also need to not know the number of dynamic tasks that will be created on DAG start, for example by chaining in an other dynamic task output.

![screenshot_2023-01-16_at_14-57-23_test_skip_-_graph_-_airflow](https://user-images.githubusercontent.com/1442084/212699549-8bfc80c6-02c7-4187-8dad-91020c94616f.png)
![screenshot_2023-01-16_at_14-56-44_test_skip_-_graph_-_airflow](https://user-images.githubusercontent.com/1442084/212699551-428c7efd-d044-472c-8fc3-92c9b146a6da.png)


If the DAG, task, or upstream tasks are cleared the skipped task runs as expected. 

The issue exists both on airflow 2.4.x & 2.5.0.

Happy to help debug this further & answer any questions!

### What you think should happen instead

The tasks should run after upstream tasks are done.

### How to reproduce

The following code is able to reproduce the issue on our side:

```python
from datetime import datetime

from airflow import DAG
from airflow.decorators import task
from airflow.utils.task_group import TaskGroup
from airflow.operators.empty import EmptyOperator

# Only one chained tasks results in only 1 of the `skipped_tasks` skipping.
# Add in extra tasks results in both `skipped_tasks` skipping, but
# no earlier tasks are ever skipped.
CHAIN_TASKS = 1


@task()
def add(x, y):
    return x, y


with DAG(
    dag_id="test_skip",
    schedule=None,
    start_date=datetime(2023, 1, 13),
) as dag:

    init = EmptyOperator(task_id="init_task")
    final = EmptyOperator(task_id="final")

    for i in range(2):
        with TaskGroup(f"task_group_{i}") as tg:
            chain_task = [i]
            for j in range(CHAIN_TASKS):
                chain_task = add.partial(x=j).expand(y=chain_task)
            skipped_task = (
                add.override(task_id="skipped").partial(x=i).expand(y=chain_task)
            )

        # Task isn't skipped if final (merging task) is removed.
        init >> tg >> final
```

### Operating System

MacOS

### Versions of Apache Airflow Providers

This can be reproduced without any extra providers installed.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py']
Ground Truth : ['a/airflow/models/xcom_arg.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: MySQL Not Using Correct Index for Scheduler Critical Section Query
### Apache Airflow version

Other Airflow 2 version

### What happened

Airflow Version: 2.2.5
MySQL Version: 8.0.18

In the Scheduler, we are coming across instances where MySQL is inefficiently optimizing the [critical section task queuing query](https://github.com/apache/airflow/blob/2.2.5/airflow/jobs/scheduler_job.py#L294-L303). When a large number of task instances are scheduled, MySQL failing to use the `ti_state` index to filter the `task_instance` table, resulting in a full table scan (about 7.3 million rows).

Normally,  when running the critical section query the index on `task_instance.state` is used to filter scheduled `task_instances`.
```bash
| -> Limit: 512 row(s)  (actual time=5.290..5.413 rows=205 loops=1)
    -> Sort row IDs: <temporary>.tmp_field_0, <temporary>.execution_date, limit input to 512 row(s) per chunk  (actual time=5.289..5.391 rows=205 loops=1)
        -> Table scan on <temporary>  (actual time=0.003..0.113 rows=205 loops=1)
            -> Temporary table  (actual time=5.107..5.236 rows=205 loops=1)
                -> Nested loop inner join  (cost=20251.99 rows=1741) (actual time=0.100..4.242 rows=205 loops=1)
                    -> Nested loop inner join  (cost=161.70 rows=12) (actual time=0.071..2.436 rows=205 loops=1)
                        -> Index lookup on task_instance using ti_state (state='scheduled')  (cost=80.85 rows=231) (actual time=0.051..1.992 rows=222 loops=1)
                        -> Filter: ((dag_run.run_type <> 'backfill') and (dag_run.state = 'running'))  (cost=0.25 rows=0) (actual time=0.002..0.002 rows=1 loops=222)
                            -> Single-row index lookup on dag_run using dag_run_dag_id_run_id_key (dag_id=task_instance.dag_id, run_id=task_instance.run_id)  (cost=0.25 rows=1) (actual time=0.001..0.001 rows=1 loops=222)
                    -> Filter: ((dag.is_paused = 0) and (task_instance.dag_id = dag.dag_id))  (cost=233.52 rows=151) (actual time=0.008..0.008 rows=1 loops=205)
                        -> Index range scan on dag (re-planned for each iteration)  (cost=233.52 rows=15072) (actual time=0.008..0.008 rows=1 loops=205)
1 row in set, 1 warning (0.03 sec)
```

When a large number of task_instances are in scheduled state at the same time, the index on `task_instance.state` is not being used to filter scheduled `task_instances`.

```bash
| -> Limit: 512 row(s)  (actual time=12110.251..12110.573 rows=512 loops=1)
    -> Sort row IDs: <temporary>.tmp_field_0, <temporary>.execution_date, limit input to 512 row(s) per chunk  (actual time=12110.250..12110.526 rows=512 loops=1)
        -> Table scan on <temporary>  (actual time=0.005..0.800 rows=1176 loops=1)
            -> Temporary table  (actual time=12109.022..12109.940 rows=1176 loops=1)
                -> Nested loop inner join  (cost=10807.83 rows=3) (actual time=1.328..12097.528 rows=1176 loops=1)
                    -> Nested loop inner join  (cost=10785.34 rows=64) (actual time=1.293..12084.371 rows=1193 loops=1)
                        -> Filter: (dag.is_paused = 0)  (cost=1371.40 rows=1285) (actual time=0.087..22.409 rows=13264 loops=1)
                            -> Table scan on dag  (cost=1371.40 rows=12854) (actual time=0.085..15.796 rows=13508 loops=1)
                        -> Filter: ((task_instance.state = 'scheduled') and (task_instance.dag_id = dag.dag_id))  (cost=0.32 rows=0) (actual time=0.907..0.909 rows=0 loops=13264)
                            -> Index lookup on task_instance using PRIMARY (dag_id=dag.dag_id)  (cost=0.32 rows=70) (actual time=0.009..0.845 rows=553 loops=13264)
                    -> Filter: ((dag_run.run_type <> 'backfill') and (dag_run.state = 'running'))  (cost=0.25 rows=0) (actual time=0.010..0.011 rows=1 loops=1193)
                        -> Single-row index lookup on dag_run using dag_run_dag_id_run_id_key (dag_id=task_instance.dag_id, run_id=task_instance.run_id)  (cost=0.25 rows=1) (actual time=0.009..0.010 rows=1 loops=1193)

1 row in set, 1 warning (12.14 sec)
```

### What you think should happen instead

To resolve this, I added a patch on the `scheduler_job.py` file, adding a MySQL index hint to use the `ti_state` index. 
```diff
--- /usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py
+++ /usr/local/lib/python3.9/site-packages/airflow/jobs/scheduler_job.py
@@ -293,6 +293,7 @@ class SchedulerJob(BaseJob):
             # and the dag is not paused
             query = (
                 session.query(TI)
+                .with_hint(TI, 'USE INDEX (ti_state)', dialect_name='mysql')
                 .join(TI.dag_run)
                 .filter(DR.run_type != DagRunType.BACKFILL_JOB, DR.state == DagRunState.RUNNING)
                 .join(TI.dag_model)
```

I think it makes sense to add this index hint upstream.

### How to reproduce

Schedule a large number of dag runs and tasks in a short period of time.

### Operating System

Debian GNU/Linux 10 (buster)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

Airflow 2.2.5 on Kubernetes
MySQL Version: 8.0.18

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/scheduler_dag_execution_timing.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/cncf/kubernetes/example_kubernetes_decorator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/sql_queries.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/oracle/hooks/oracle.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Tasks get failed if capacity of a pool is exceeded
**Apache Airflow version**: 1.10.10 (from PyPI)


**Environment**:

- **OS**: Ubuntu 18.04.4 LTS
- **Database**: PostgreSQL


**What happened**:

Firstly, Airflow summarizes required slots for all queued tasks. As a result, if the total number of slots required across a dag exceeds the limit, all the tasks get failed. There's a log for three tasks of weight 12 (TASK_A in the logs below), 12 (TASK_B) and 16 (TASK_C), and a pool with 24 slots: [deps_issue.log](https://github.com/apache/airflow/files/4781470/deps_issue.log)

The second issue appears after the first one. The tasks get rescheduled due to missing limits. At the same time, corresponding runs report as they are finished. There's a mechanism which fails tasks in queued state if they were reported as finished. This should help with tasks stuck in queued state, but in fact it prevents tasks from being rescheduled. There's a combined log across components: [failure_issue.log](https://github.com/apache/airflow/files/4781482/failure_issue.log)


**What you expected to happen**:

Airflow executes tasks in two stages - 12+12 and 16 (or clockwise)


**How to reproduce it**:

1. Create a pool with capacity 24
2. Create a dag with three tasks which require 12, 12 and 16 slots assigned to the pool
3. Trigger the dag


**Anything else we need to know**:


All the task should be able to run simultaneously. The issue is observed on local and Celery executors.

UPD: This issue doesn't appear on an empty instance. There must be other DAGs, for instance examples enabled.

An example of a dag failing:
```python
# coding: utf-8
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.utils.dates import days_ago

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'schedule_interval': None
}

dag = DAG(
    'THE_DAG',
    default_args=default_args,
    schedule_interval=None,
    concurrency=10,
)


def op(**kwargs):
    print('Hello!')


with dag:
    PythonOperator(
        task_id='TASK_A',
        provide_context=True,
        python_callable=op,
        pool='THE_POOL',
        pool_slots=12,
    )
    PythonOperator(
        task_id='TASK_B',
        provide_context=True,
        python_callable=op,
        pool='THE_POOL',
        pool_slots=12,
    )
    PythonOperator(
        task_id='TASK_C',
        provide_context=True,
        python_callable=op,
        pool='THE_POOL',
        pool_slots=16,
    )
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dags/test_issue_1225.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: [AIRFLOW-1424] make the next execution date of DAGs visible
Dear Airflow maintainers,

Please accept this PR. I understand that it will not be reviewed until I have checked off all the steps below!


### JIRA
- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, "[AIRFLOW-XXX] My Airflow PR"
    - https://issues.apache.org/jira/browse/AIRFLOW-1424


### Description
- [x] Here are some details about my PR, including screenshots of any UI changes:

The scheduler's DAG run creation logic can be tricky and one is
easily confused with the start_date + interval
and period end scheduling way of thinking.

It would ease airflow's usage to add a *next execution* field to DAGs
so that we can very easily see the (un)famous *period end* after which
the scheduler will create a new DAG run for our workflows.

These patches are a simple way to implement this on the DAG model
and make use of this in the interface.

![2017-07-20-221839_1160x357_scrot](https://user-images.githubusercontent.com/474389/28437471-08b35b92-6d9b-11e7-9c30-c5c7e4cb1246.png)

![2017-07-20-221854_1155x302_scrot](https://user-images.githubusercontent.com/474389/28437480-15928d4c-6d9b-11e7-8822-348fd0bc1400.png)


### Tests
- [x] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:

Tests are provided

### Commits
- [x] My commits all reference JIRA issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)":
    1. Subject is separated from body by a blank line
    2. Subject is limited to 50 characters
    3. Subject does not end with a period
    4. Subject uses the imperative mood ("add", not "adding")
    5. Body wraps at 72 characters
    6. Body explains "what" and "why", not "how"



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: When passing the 'False' value to the parameters of a decorated dag function I get this traceback
### Apache Airflow version

2.2.3

### What happened

When passing the `False` value to a decorated dag function I get this traceback below. Also the default value is not shown when clicking 'trigger dag w/ config'.


```[2022-04-07, 20:08:57 UTC] {taskinstance.py:1259} INFO - Executing <Task(_PythonDecoratedOperator): value_consumer> on 2022-04-07 20:08:56.914410+00:00
[2022-04-07, 20:08:57 UTC] {standard_task_runner.py:52} INFO - Started process 2170 to run task
[2022-04-07, 20:08:57 UTC] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'check_ui_config', 'value_consumer', 'manual__2022-04-07T20:08:56.914410+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/check_ui_config.py', '--cfg-path', '/tmp/tmpww9euksv', '--error-file', '/tmp/tmp7kjdfks5']
[2022-04-07, 20:08:57 UTC] {standard_task_runner.py:77} INFO - Job 24: Subtask value_consumer
[2022-04-07, 20:08:57 UTC] {logging_mixin.py:109} INFO - Running <TaskInstance: check_ui_config.value_consumer manual__2022-04-07T20:08:56.914410+00:00 [running]> on host a643f8828615
[2022-04-07, 20:08:57 UTC] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1418, in _execute_task_with_callbacks
    self.render_templates(context=context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1992, in render_templates
    self.task.render_template_fields(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1061, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1074, in _do_render_template_fields
    rendered_content = self.render_template(content, context, jinja_env, seen_oids)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1125, in render_template
    return tuple(self.render_template(element, context, jinja_env) for element in content)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1125, in <genexpr>
    return tuple(self.render_template(element, context, jinja_env) for element in content)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1116, in render_template
    return content.resolve(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/param.py", line 226, in resolve
    raise AirflowException(f'No value could be resolved for parameter {self._name}')
airflow.exceptions.AirflowException: No value could be resolved for parameter test
[2022-04-07, 20:08:57 UTC] {taskinstance.py:1267} INFO - Marking task as FAILED. dag_id=check_ui_config, task_id=value_consumer, execution_date=20220407T200856, start_date=20220407T200857, end_date=20220407T200857
[2022-04-07, 20:08:57 UTC] {standard_task_runner.py:89} ERROR - Failed to execute job 24 for task value_consumer
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/usr/local/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/usr/local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/usr/local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 180, in _run_raw_task
    ti._run_raw_task(
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1418, in _execute_task_with_callbacks
    self.render_templates(context=context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1992, in render_templates
    self.task.render_template_fields(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1061, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1074, in _do_render_template_fields
    rendered_content = self.render_template(content, context, jinja_env, seen_oids)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1125, in render_template
    return tuple(self.render_template(element, context, jinja_env) for element in content)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1125, in <genexpr>
    return tuple(self.render_template(element, context, jinja_env) for element in content)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1116, in render_template
    return content.resolve(context)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/param.py", line 226, in resolve
    raise AirflowException(f'No value could be resolved for parameter {self._name}')
airflow.exceptions.AirflowException: No value could be resolved for parameter test
```

### What you think should happen instead

I think airflow should be able to handle the False value when passing it as a dag param.

### How to reproduce
```
from airflow.decorators import dag, task
from airflow.models.param import Param

from datetime import datetime, timedelta


@task
def value_consumer(val):
    print(val)


@dag(
    start_date=datetime(2021, 1, 1),
    schedule_interval=timedelta(days=365, hours=6)
)
def check_ui_config(test):
    value_consumer(test)
    

the_dag = check_ui_config(False)
```
### Operating System

Docker (debian:buster)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

Astro cli with this image:
quay.io/astronomer/ap-airflow-dev:2.2.3-2

### Anything else

![Screenshot from 2022-04-07 14-13-43](https://user-images.githubusercontent.com/102494105/162288264-bb6c6ca6-977f-4ff7-a0cc-9616c0ce8ac8.png)


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/template/templater.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py']
Ground Truth : ['a/airflow/models/param.py', 'a/airflow/models/dag.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: DAG Run fails when chaining multiple empty mapped tasks
### Apache Airflow version

2.3.3 (latest released)

### What happened

On Kubernetes Executor and Local Executor (others not tested) a significant fraction of the DAG Runs of a DAG that has two consecutive mapped tasks which are are being passed an empty list are marked as failed when all tasks are either succeeding or being skipped.

![image](https://user-images.githubusercontent.com/13177948/180075030-705b3a15-c554-49c1-8470-ecd10ee1d2dc.png)


### What you think should happen instead

The DAG Run should be marked success.

### How to reproduce

Run the following DAG on Kubernetes Executor or Local Executor.

The real world version of this DAG has several mapped tasks that all point to the same list, and that list is frequently empty. I have made a minimal reproducible example.

```py
from datetime import datetime

from airflow import DAG
from airflow.decorators import task


with DAG(dag_id="break_mapping", start_date=datetime(2022, 3, 4)) as dag:

    @task
    def add_one(x: int):
        return x + 1

    @task
    def say_hi():
        print("Hi")


    added_values = add_one.expand(x=[])
    added_more_values = add_one.expand(x=[])
    say_hi() >> added_values
    added_values >> added_more_values
```

### Operating System

Debian Bullseye

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==1!4.0.0
apache-airflow-providers-cncf-kubernetes==1!4.1.0
apache-airflow-providers-elasticsearch==1!4.0.0
apache-airflow-providers-ftp==1!3.0.0
apache-airflow-providers-google==1!8.1.0
apache-airflow-providers-http==1!3.0.0
apache-airflow-providers-imap==1!3.0.0
apache-airflow-providers-microsoft-azure==1!4.0.0
apache-airflow-providers-mysql==1!3.0.0
apache-airflow-providers-postgres==1!5.0.0
apache-airflow-providers-redis==1!3.0.0
apache-airflow-providers-slack==1!5.0.0
apache-airflow-providers-sqlite==1!3.0.0
apache-airflow-providers-ssh==1!3.0.0
```

### Deployment

Astronomer

### Deployment details

Local was tested on docker compose (from astro-cli)

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py']
Ground Truth : ['a/airflow/ti_deps/dep_context.py', 'a/airflow/models/taskinstance.py', 'a/airflow/models/baseoperator.py', 'a/airflow/ti_deps/deps/trigger_rule_dep.py', 'a/airflow/models/dagrun.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Add support for snowflake on the sql_sensor
**Description**

Enable the support for Snowflake on the SqlSensor.

**Use case**

Currently, the `SqlSensor` supports only the following connection types,

- google_cloud_platform
- jdbc
- mssql
- mysql
- oracle
- postgres
- presto
- sqlite
- vertica

At the moment, it doesn't have support for Snowflake. This ticket is to add support for Snowflake on the `SqlSensor`. 



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/hooks/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/sensors/metastore_partition.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_connection_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/slack/transfers/test_base_sql_to_slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/sensors/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/common/sql/sensors/test_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/transfers/copy_into_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/snowflake/operators/test_snowflake_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/presto_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jdbc/hooks/jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0044_1_10_7_add_serialized_dag_table.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py']
Ground Truth : ['a/airflow/models/connection.py', 'a/airflow/sensors/sql_sensor.py', 'a/airflow/operators/sql.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Missing schedule_delay metric
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 2.0.0 but applicable to master

**Environment**: Running on ECS but not relevant to question


**What happened**: I am not seeing the metric dagrun.schedule_delay.<dag_id> being reported.  A search in the codebase seems to reveal that it no longer exists.   It was originally added in https://github.com/apache/airflow/pull/5050.

<!-- (please include exact error messages if you can) -->

<!-- What do you think went wrong? -->
I suspect either:
1.  This metric was intentionally removed, in which case the docs should be updated to remove it.
2. It was unintentionally removed during a refactor, in which case we should add it back.
3. I am bad at searching through code, and someone could hopefully point me to where it is reported from now. 

**How to reproduce it**:
https://github.com/apache/airflow/search?q=schedule_delay


<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: resolve web ui views warning re DISTINCT ON
### Body

Got this warning in webserver output when loading home page

```
/Users/dstandish/code/airflow/airflow/www/views.py:710 SADeprecationWarning: DISTINCT ON is currently supported only by the PostgreSQL dialect.  Use of DISTINCT ON for other backends is currently silently ignored, however this usage is deprecated, and will raise CompileError in a future release for all backends that do not support this syntax.
```
looks like it's this line
```
            dagtags = session.query(DagTag.name).distinct(DagTag.name).all()
```

may be able to change to `func.distinct` 



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Duplicate log lines in CloudWatch after upgrade to 2.4.2
### Apache Airflow version

2.4.2

### What happened

We upgraded airflow from 2.4.1 to 2.4.2 and immediately notice that every task log line is duplicated _into_ CloudWatch. Comparing logs from tasks run before upgrade and after upgrade indicates that the issue is not in how the logs are displayed in Airflow, but rather that it now produces two log lines instead of one. 

When observing both the CloudWatch log streams and the Airflow UI, we can see duplicate log lines for ~_all_~ most log entries post upgrade, whilst seeing single log lines in tasks before upgrade.

This happens _both_ for tasks ran in a remote `EcsRunTaskOperator`'s as well as in regular `PythonOperator`'s.

### What you think should happen instead

A single non-duplicate log line should be produced into CloudWatch.

### How to reproduce

From my understanding now, any setup on 2.4.2 that uses CloudWatch remote logging will produce duplicate log lines. (But I have not been able to confirm other setups)

### Operating System

Docker: `apache/airflow:2.4.2-python3.9` - Running on AWS ECS Fargate

### Versions of Apache Airflow Providers

```
apache-airflow[celery,postgres,apache.hive,jdbc,mysql,ssh,amazon,google,google_auth]==2.4.2
apache-airflow-providers-amazon==6.0.0
```

### Deployment

Other Docker-based deployment

### Deployment details

We are running a docker inside Fargate ECS on AWS.

The following environment variables + config in CloudFormation control remote logging:

```
            - Name: AIRFLOW__LOGGING__REMOTE_LOGGING
              Value: True
            - Name: AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER
              Value: !Sub "cloudwatch://${TasksLogGroup.Arn}"
```

### Anything else

We did not change any other configuration during the upgrade, simply bumped the requirements for provider list + docker image from 2.4.1 to 2.4.2.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py']
Ground Truth : ['a/airflow/utils/log/logging_mixin.py', 'a/airflow/utils/log/file_task_handler.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Programmatic customization of run_id for scheduled DagRuns
### Description

Allow DAG authors to control how `run_id`'s are generated for created DagRuns. Currently the only way to specify a DagRun's `run_id` is through the manual trigger workflow either through the CLI or API and passing in `run_id`. It would be great if DAG authors are able to write a custom logic to generate `run_id`'s from scheduled `DagRunInterval`'s.

### Use case/motivation

In Airflow 1.x, the semantics of `execution_date` were burdensome enough for users that DAG authors would subclass DAG to override `create_dagrun` so that when new DagRuns were created, they were created with `run_id`'s that provided context into semantics about the DagRun. For example, 
```
def create_dagrun(self, **kwargs):
  kwargs['run_id'] = kwargs['execution_date'] + self.following_schedule(kwargs['execution_date']).date()
  return super().create_dagrun(kwargs)
```
would result in the UI DagRun dropdown to display the weekday of when the Dag actually ran.
<img width="528" alt="image001" src="https://user-images.githubusercontent.com/9851473/156280393-e261d7fa-dfe0-41db-9887-941510f4070f.png">


After upgrading to Airflow 2.0 and with Dag serialization in the scheduler overridden methods are no longer there in the SerializedDAG, so we are back to having `scheduled__<execution_date>` values in the UI dropdown. It would be great if some functionality could be exposed either through the DAG or just in the UI to display meaningful values in the DagRun dropdown.  

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py']
Ground Truth : ['a/airflow/api/common/trigger_dag.py', 'a/airflow/utils/types.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dag.py', 'a/airflow/models/dagrun.py', 'a/airflow/timetables/base.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: PythonSensor is not considering mode='reschedule', instead marking task UP_FOR_RETRY
### Apache Airflow version

2.3.0 (latest released)

### What happened

A PythonSensor that works on versions <2.3.0 in mode reschedule is now marking the task as `UP_FOR_RETRY` instead.

Log says:
```
[2022-05-02, 15:48:23 UTC] {python.py:66} INFO - Poking callable: <function test at 0x7fd56286bc10>
[2022-05-02, 15:48:23 UTC] {taskinstance.py:1853} INFO - Rescheduling task, marking task as UP_FOR_RESCHEDULE
[2022-05-02, 15:48:23 UTC] {local_task_job.py:156} INFO - Task exited with return code 0
[2022-05-02, 15:48:23 UTC] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
```

But it directly marks it as `UP_FOR_RETRY` and then follows `retry_delay` and `retries`

### What you think should happen instead

It should mark the task as `UP_FOR_RESCHEDULE` and reschedule it according to the `poke_interval`

### How to reproduce

```
from datetime import datetime, timedelta

from airflow import DAG
from airflow.sensors.python import PythonSensor


def test():
    return False


default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2022, 5, 2),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

dag = DAG("dag_csdepkrr_development_v001",
          default_args=default_args,
          catchup=False,
          max_active_runs=1,
          schedule_interval=None)

t1 = PythonSensor(task_id="PythonSensor",
                  python_callable=test,
                  poke_interval=30,
                  mode='reschedule',
                  dag=dag)
```

### Operating System

Latest Docker image

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==3.3.0
apache-airflow-providers-celery==2.1.4
apache-airflow-providers-cncf-kubernetes==4.0.1
apache-airflow-providers-docker==2.6.0
apache-airflow-providers-elasticsearch==3.0.3
apache-airflow-providers-ftp==2.1.2
apache-airflow-providers-google==6.8.0
apache-airflow-providers-grpc==2.0.4
apache-airflow-providers-hashicorp==2.2.0
apache-airflow-providers-http==2.1.2
apache-airflow-providers-imap==2.2.3
apache-airflow-providers-microsoft-azure==3.8.0
apache-airflow-providers-mysql==2.2.3
apache-airflow-providers-odbc==2.0.4
apache-airflow-providers-oracle==2.2.3
apache-airflow-providers-postgres==4.1.0
apache-airflow-providers-redis==2.0.4
apache-airflow-providers-sendgrid==2.0.4
apache-airflow-providers-sftp==2.5.2
apache-airflow-providers-slack==4.2.3
apache-airflow-providers-sqlite==2.1.3
apache-airflow-providers-ssh==2.4.3
```


### Deployment

Docker-Compose

### Deployment details

Latest Docker compose from the documentation

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py']
Ground Truth : ['a/airflow/sensors/base.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: WEB UI, last page button does not work when all dags are in not active state
**Apache Airflow version**:
1.10.10

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):
none
**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"
- **Kernel** (e.g. `uname -a`):
Linux mid1-e-1 3.10.0-514.2.2.el7.x86_64 #1 SMP Tue Dec 6 23:06:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**:
pip yum
- **Others**:

**What happened**:

It there are no dags the last page button ( >> ) has 

`http://airflow-test.buongiorno.com/home?search=&page=-1` it should be _page=0_ 

set as link and this lead this exception:

```
Node: datalake-test.docomodigital.com
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/flask/app.py", line 2446, in wsgi_app
    response = self.full_dispatch_request()
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/flask/app.py", line 1951, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/flask/app.py", line 1820, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/flask/app.py", line 1949, in full_dispatch_request
    rv = self.dispatch_request()
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/flask/app.py", line 1935, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/flask_appbuilder/security/decorators.py", line 101, in wraps
    return f(self, *args, **kwargs)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib/python2.7/site-packages/airflow/www_rbac/views.py", line 302, in index
    joinedload(DagModel.tags)).offset(start).limit(dags_per_page).all()
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3244, in all
    return list(self)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3403, in __iter__
    return self._execute_and_instances(context)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 3428, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 984, in execute
    return meth(self, multiparams, params)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/sql/elements.py", line 293, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1103, in _execute_clauseelement
    distilled_params,
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1288, in _execute_context
    e, statement, parameters, cursor, context
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1482, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1248, in _execute_context
    cursor, statement, parameters, context
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 588, in do_execute
    cursor.execute(statement, parameters)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/MySQLdb/cursors.py", line 255, in execute
    self.errorhandler(self, exc, value)
  File "/jhub/_prod/server_global_unifieddata_hadoop_airflow_daemon/lib64/python2.7/site-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
ProgrammingError: (_mysql_exceptions.ProgrammingError) (1064, "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '-25, 25) AS anon_1 LEFT OUTER JOIN dag_tag AS dag_tag_1 ON anon_1.dag_dag_id = d' at line 7")
[SQL: SELECT anon_1.dag_dag_id AS anon_1_dag_dag_id, anon_1.dag_root_dag_id AS anon_1_dag_root_dag_id, anon_1.dag_is_paused AS anon_1_dag_is_paused, anon_1.dag_is_subdag AS anon_1_dag_is_subdag, anon_1.dag_is_active AS anon_1_dag_is_active, anon_1.dag_last_scheduler_run AS anon_1_dag_last_scheduler_run, anon_1.dag_last_pickled AS anon_1_dag_last_pickled, anon_1.dag_last_expired AS anon_1_dag_last_expired, anon_1.dag_scheduler_lock AS anon_1_dag_scheduler_lock, anon_1.dag_pickle_id AS anon_1_dag_pickle_id, anon_1.dag_fileloc AS anon_1_dag_fileloc, anon_1.dag_owners AS anon_1_dag_owners, anon_1.dag_description AS anon_1_dag_description, anon_1.dag_default_view AS anon_1_dag_default_view, anon_1.dag_schedule_interval AS anon_1_dag_schedule_interval, dag_tag_1.name AS dag_tag_1_name, dag_tag_1.dag_id AS dag_tag_1_dag_id 
FROM (SELECT dag.dag_id AS dag_dag_id, dag.root_dag_id AS dag_root_dag_id, dag.is_paused AS dag_is_paused, dag.is_subdag AS dag_is_subdag, dag.is_active AS dag_is_active, dag.last_scheduler_run AS dag_last_scheduler_run, dag.last_pickled AS dag_last_pickled, dag.last_expired AS dag_last_expired, dag.scheduler_lock AS dag_scheduler_lock, dag.pickle_id AS dag_pickle_id, dag.fileloc AS dag_fileloc, dag.owners AS dag_owners, dag.description AS dag_description, dag.default_view AS dag_default_view, dag.schedule_interval AS dag_schedule_interval 
FROM dag 
WHERE dag.is_subdag = 0 AND dag.is_active = 1 AND (EXISTS (SELECT 1 
FROM dag_tag 
WHERE dag.dag_id = dag_tag.dag_id AND dag_tag.name IN (%s))) ORDER BY dag.dag_id 
 LIMIT %s, %s) AS anon_1 LEFT OUTER JOIN dag_tag AS dag_tag_1 ON anon_1.dag_dag_id = dag_tag_1.dag_id ORDER BY anon_1.dag_dag_id]
[parameters: (u'example', -25, 25)]
(Background on this error at: http://sqlalche.me/e/f405)

```

**What you expected to happen**:

I expected the last page button acts like the 1st page button: nothing has to happen.

**How to reproduce it**:

Set all the dags as inactive, the click last page.

**Anything else we need to know**:

It's just an annoyance.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/sbom_commands.py']
Ground Truth : ['a/airflow/www/utils.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Fix asset compilation in start-airflow
The recent change #29080 introduced missing black import in the pre-commit that run the compilation. The compilation happened in the background thread and it's ouptut was only visible in the asset_compilation output file

This PR fixes the black import problem by removing unnecessary import, but it will also stop start-airflow and exit with error as well as surface the output of asset compilation to the console.

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of an existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/hatch_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/docker_tests_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py']
Ground Truth : ['a/dev/breeze/src/airflow_breeze/utils/path_utils.py', 'a/scripts/ci/pre_commit/pre_commit_compile_www_assets_dev.py', 'a/dev/breeze/src/airflow_breeze/utils/run_utils.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Remove 'user_id', 'role_id' from User and Role in OpenAPI schema 
Would be good to remove the 'id' of both User and Role schemas from what is dumped in REST API endpoints. ID of User and Role table are sensitive data that would be fine to hide from the endpoints


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/cli_commands/user_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/models/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/api_endpoints/role_and_permission_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/snowflake/hooks/test_snowflake_sql_api.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/hooks/snowflake_sql_api.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/connection_wrapper.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/snowflake/hooks/test_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/cassandra_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/cli_commands/test_user_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/endpoint_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py']
Ground Truth : ['a/airflow/api_connexion/schemas/user_schema.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Exception logging success function instead of failure
https://github.com/apache/airflow/blob/fdd9b6f65b608c516b8a062b058972d9a45ec9e3/airflow/sensors/sql_sensor.py#L97



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_check_deferrable_default.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/microsoft/azure/sensors/test_wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_sagemaker_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/step_function.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/sensors/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_cli_util.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/sensors/test_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/triggers/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/automl/example_automl_translation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/batch.py']
Ground Truth : ['a/airflow/sensors/sql_sensor.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: Refresh Airflow UI
**Description**

As the Airflow 2.0 is coming nearer it would be awesome to **refresh** the Airflow WebUI. By this it mostly means:
- using some "modern colors" (how about Airflow logo palette?)
- fixing some padding/spacing
- and other small HTML / CSS touches 

**Use case / motivation**

The main point is to **refresh** the look, not to rewrite the Web UI.

**Related Issues**

Apache Airflow brand book: https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook

I'm far away from being a frontend expert but removing the "Airflow green" gives a good impression I think:

<img width="2560" alt="Screenshot 2020-09-14 at 10 45 22" src="https://user-images.githubusercontent.com/9528307/93226956-2b1fe900-f774-11ea-9b6a-52579f4109c5.png">



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow/img/diagram_multitenant_airflow_architecture.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/add_back_references.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow/img/diagram_basic_airflow_architecture.py']
Ground Truth : ['a/airflow/utils/platform.py', 'a/airflow/www/extensions/init_views.py', 'a/airflow/www/extensions/init_jinja_globals.py', 'a/airflow/www/views.py', 'a/airflow/www/extensions/init_appbuilder_links.py', 'a/airflow/www/widgets.py', 'a/airflow/www/utils.py']
Current Recall: 0.07216529009890897

=========================================================

ISSUE: GoogleCampaignManagerInsertReportOperator Fails when using Templated Reports
The GoogleCampaignManagerInsertReportOperator fails with the following 400 error when attempting to insert a report rendered from a file template:

> HttpError 400 when requesting https://www.googleapis.com/dfareporting/v3.3/userprofiles/*******/reports?alt=json returned "Report type needs to be set."

Appears the issue is that Jinja templates will always render as a String but the operator is expecting a Dictionary (representing the JSON object).

Here's a sample call:
```
GoogleCampaignManagerInsertReportOperator(
        task_id="create_report",
        gcp_conn_id=self.config.get("gcp_connection_id"),
        profile_id=self.config.get("cm_profile_id"),
        report="./resources/reports/cm_report.json",
        params=report_params,
        dag=self.dag)
```

Relevant Code:
https://github.com/apache/airflow/blob/096f5c5cba963b364ee75f6686d128cd4d34d66e/airflow/providers/google/marketing_platform/operators/campaign_manager.py#L287

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/operators/campaign_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/marketing_platform/example_campaign_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/marketing_platform/operators/test_campaign_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/hooks/campaign_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/operators/display_video.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/operators/search_ads.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/sensors/campaign_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/sensors/display_video.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/hooks/display_video.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/sensors/search_ads.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/facebook_ads_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/mlengine.py']
Ground Truth : ['a/airflow/providers/google/marketing_platform/operators/campaign_manager.py', 'a/airflow/providers/google/marketing_platform/operators/search_ads.py', 'a/airflow/providers/google/marketing_platform/operators/display_video.py']
Current Recall: 0.07287906597328442

=========================================================

ISSUE: Hide sensitive data in UI
**Description**

I'm using Airflow for 2 years now and I have a plugin that get password for a specific account in a Vault and then push it through a XCOM to reuse it on another tasks.

The fact is that if the value is sensitive like a password, I can't hide it in the UI except for XCOM if I add an underscore in the prefix name of the key value.

Eg: **kwargs['ti'].xcom_push('key':'_password', 'value':'my_value')**

But for rendered template UI page, I didn't find anything similar, so if I try to pull a XCOM, it will show the value in the UI and I want to avoid it.

Maybe is it possible to add a condition in **https://github.com/apache/airflow/blob/master/airflow/www/views.py** after line **635**

```python
elif template_field.startswith('_'):
    html_dict[template_field] = ("<pre><code>sensitive data will not be exposed here</pre></code>")
```

**Use case / motivation**

I know that I can use connections but in my case, and due to security politic in my company, we have to store it in a dedicated Vault.

**Related Issues**

N/A


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/container_instances.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/hooks/base.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py', 'a/airflow/config_templates/airflow_local_settings.py', 'a/airflow/www/utils.py', 'a/airflow/configuration.py', 'a/airflow/models/connection.py', 'a/airflow/typing_compat.py', 'a/airflow/www/views.py', '/dev/null', 'a/airflow/settings.py', 'a/airflow/sensors/smart_sensor.py', 'a/airflow/hooks/base.py', 'a/airflow/models/variable.py', 'a/airflow/models/renderedtifields.py', 'a/airflow/models/dag.py']
Current Recall: 0.07287906597328442

=========================================================

ISSUE: Reattach ECS Task when Airflow restarts
**Description**

In similar fashion to https://github.com/apache/airflow/pull/4083, it would be helpful for Airflow to reattach itself to the ECS Task rather than letting another instance to start. However, instead of making this the default behavior, it would be better to use a `reattach` flag.

**Use case / motivation**

Allow Airflow the option to reattach to an existing ECS task when a restart happens, which would avoid having "rogue" tasks.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/ecs.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: A task's returned object should not be checked for mappability if the dag doesn't use it in an expansion.
### Apache Airflow version

main (development)

### What happened

Here's a dag:

```python3
with DAG(...) as dag:

    @dag.task
    def foo():
        return "foo"

    @dag.task
    def identity(thing):
        return thing

    foo() >> identity.expand(thing=[1, 2, 3])
```


`foo` fails with these task logs:
```
[2022-04-14, 14:15:26 UTC] {python.py:173} INFO - Done. Returned value was: foo
[2022-04-14, 14:15:26 UTC] {taskinstance.py:1837} WARNING - We expected to get frame set in local storage but it was not. Please report this as an issue with full logs at https://github.com/apache/airflow/issues/new
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1417, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1564, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1634, in _execute_task
    self._record_task_map_for_downstreams(task_orig, result, session=session)
  File "/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 2314, in _record_task_map_for_downstreams
    raise UnmappableXComTypePushed(value)
airflow.exceptions.UnmappableXComTypePushed: unmappable return type 'str'
```

### What you think should happen instead

Airflow shouldn't bother checking `foo`'s return type for mappability because its return value is never used in an expansion.

### How to reproduce

Run the dag, notice the failure

### Operating System

debian (docker)

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

using image with ref: e5dd6fdcfd2f53ed90e29070711c121de447b404


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/models/taskmixin.py', 'a/airflow/models/xcom_arg.py', 'a/airflow/jobs/backfill_job.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: GlueOperator: iam_role_arn as a parameter
### Description
Hi,
There is mandatory parameter iam_role_name parameter for GlueJobOperator/GlueJobHook.
It adds additional step of translating it to the arn, which needs  connectivity to the global iam AWS endpoint (no privatelink availabale).
For private setups it needs opening connectivity + proxy configuration to make it working.  
It would be great to have also possibility to just pass directly iam_role_arn and avoid this additional step.
### Use case/motivation

Role assignation does not need external connectivity, possibility of adding arn instead of the name.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/glue.py', 'a/airflow/providers/amazon/aws/hooks/glue.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: S3 Remote Logging not working
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**:  v2.0.0b3


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): 1.16.15

**Environment**:

- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**: Custom Helm Chart
- **Others**:

**What happened**:

S3 Remote Logging not working. Below is the stacktrace:
```
 Running <TaskInstance: canary_dag.print_date 2020-12-09T19:46:17.200838+00:00 [queued]> on host canarydagprintdate-9fafada4409d4eafb5e6e9c7187810ae                                                                                                                          
 [2020-12-09 19:54:09,825] {s3_task_handler.py:183} ERROR - Could not verify previous log to append: 'NoneType' object is not callable                                                                                                                                        
 Traceback (most recent call last):                                                                                                                                                                                                                                           
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/log/s3_task_handler.py", line 179, in s3_write                                                                                                                                         
     if append and self.s3_log_exists(remote_log_location):                                                                                                                                                                                                                   
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/log/s3_task_handler.py", line 141, in s3_log_exists                                                                                                                                    
     return self.hook.check_for_key(remote_log_location)                                                                                                                                                                                                                      
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 57, in wrapper                                                                                                                                                      
     connection = self.get_connection(self.aws_conn_id)                                                                                                                                                                                                                       
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/base.py", line 63, in get_connection                                                                                                                                                                  
     conn = Connection.get_connection_from_secrets(conn_id)                                                                                                                                                                                                                   
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 351, in get_connection_from_secrets                                                                                                                                             
     conn = secrets_backend.get_connection(conn_id=conn_id)                                                                                                                                                                                                                   
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 64, in wrapper                                                                                                                                                                      
     with create_session() as session:                                                                                                                                                                                                                                        
   File "/usr/local/lib/python3.7/contextlib.py", line 112, in __enter__                                                                                                                                                                                                      
     return next(self.gen)                                                                                                                                                                                                                                                    
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 29, in create_session                                                                                                                                                               
     session = settings.Session()                                                                                                                                                                                                                                             
 TypeError: 'NoneType' object is not callable                                                                                                                                                                                                                                 
 [2020-12-09 19:54:09,826] {s3_task_handler.py:193} ERROR - Could not write logs to s3://my-favorite-airflow-logs/canary_dag/print_date/2020-12-09T19:46:17.200838+00:00/2.log                                                                                                   
 Traceback (most recent call last):                                                                                                                                                                                                                                           
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/log/s3_task_handler.py", line 190, in s3_write                                                                                                                                         
     encrypt=conf.getboolean('logging', 'ENCRYPT_S3_LOGS'),                                                                                                                                                                                                                   
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/amazon/aws/hooks/s3.py", line 57, in wrapper                                                                                                                                                      
     connection = self.get_connection(self.aws_conn_id)                                                                                                                                                                                                                       
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/hooks/base.py", line 63, in get_connection                                                                                                                                                                  
     conn = Connection.get_connection_from_secrets(conn_id)                                                                                                                                                                                                                   
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/connection.py", line 351, in get_connection_from_secrets                                                                                                                                             
     conn = secrets_backend.get_connection(conn_id=conn_id)                                                                                                                                                                                                                   
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 64, in wrapper                                                                                                                                                                      
     with create_session() as session:                                                                                                                                                                                                                                        
   File "/usr/local/lib/python3.7/contextlib.py", line 112, in __enter__                                                                                                                                                                                                      
     return next(self.gen)                                                                                                                                                                                                                                                    
   File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 29, in create_session                                                                                                                                                               
     session = settings.Session()                                                                                                                                                                                                                                             
 TypeError: 'NoneType' object is not callable                                                                                                                                                                                                                                 
 stream closed
```

**What you expected to happen**
Able to see the task instance logs in the airflow UI being read from S3 remote location.

**How to reproduce it**:

Pulled the latest master and created an airflow image from the dockerfile mentioned in the repo.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/log/s3_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/log/wasb_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py', 'a/airflow/executors/celery_executor.py', 'a/airflow/executors/local_executor.py', 'a/airflow/cli/cli_parser.py', 'a/airflow/task/task_runner/standard_task_runner.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Data Interval wrong when manually triggering with a specific logical date
### Apache Airflow version

2.2.5

### What happened

When I use the date picker in the Trigger DAG w/ config page to choose a specific logical date for some reason on a scheduled daily DAG the Data Interval Start (circled in red) is 2 days before the logical date (circled in blue), instead of the same as the logical date. And the Data Interval End is one day before the logical date. So the interval is the correct length, but on wrong days.

![Screen Shot 2022-05-11 at 5 14 10 PM](https://user-images.githubusercontent.com/45696489/168159891-b080273b-4b22-4ef8-a2ae-98327a503f9f.png)

I encountered this with a DAG with a daily schedule which typically runs at 09:30 UTC. I am testing this in a dev environment (with catchup off) and trying to trigger a run for 2022-05-09 09:30:00. I would expect the data interval to start at that same time and the data interval end to be 1 day after.

It has nothing to do with the previous run since that was way back on 2022-04-26

### What you think should happen instead

The data interval start date should be the same as the logical date (if it is a custom logical date)

### How to reproduce

I made a sample DAG as shown below:
```python
import pendulum
from airflow.models import DAG
from airflow.operators.python import PythonOperator


def sample(data_interval_start, data_interval_end):
    return "data_interval_start: {}, data_interval_end: {}".format(str(data_interval_start), str(data_interval_end))


args = {
    'start_date': pendulum.datetime(2022, 3, 10, 9, 30)
}
with DAG(
    dag_id='sample_data_interval_issue',
    default_args=args,
    schedule_interval='30 9 * * *'  # 09:30 UTC
) as sample_data_interval_issue:
    task = PythonOperator(
        task_id='sample',
        python_callable=sample
    )

```

I then start it to start a scheduled DAG run (`2022-05-11, 09:30:00 UTC`), and the `data_interval_start` is the same as I expect, `2022-05-11T09:30:00+00:00`.

However, when I went to "Trigger DAG w/ config" page and in the date chooser choose `2022-05-09 09:30:00+00:00`, and then triggered that. It shows the run datetime is `2022-05-09, 09:30:00 UTC`, but the `data_interval_start` is incorrectly set to `2022-05-08T09:30:00+00:00`, 2 days before the date I choose.

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

N/A

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_dataset.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/serializers/test_serializers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_dates.py']
Ground Truth : ['a/airflow/timetables/trigger.py', 'a/airflow/timetables/interval.py', 'a/airflow/timetables/_cron.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Kubernetes Invalid executor_config, pod_override filled with Encoding.VAR
### Apache Airflow version

2.3.4

### What happened

Trying to start Kubernetes tasks using a `pod_override` results in pods not starting after upgrading from 2.3.2 to 2.3.4

The pod_override look very odd, filled with many Encoding.VAR objects, see following scheduler log:
```
{kubernetes_executor.py:550} INFO - Add task TaskInstanceKey(dag_id='commit_check', task_id='sync_and_build', run_id='5776-2-1662037155', try_number=1, map_index=-1) with command ['airflow', 'tasks', 'run', 'commit_check', 'sync_and_build', '5776-2-1662037155', '--local', '--subdir', 'DAGS_FOLDER/dag_on_commit.py'] with executor_config {'pod_override': {'Encoding.VAR': {'Encoding.VAR': {'Encoding.VAR': {'metadata': {'Encoding.VAR': {'annotations': {'Encoding.VAR': {}, 'Encoding.TYPE': 'dict'}}, 'Encoding.TYPE': 'dict'}, 'spec': {'Encoding.VAR': {'containers': REDACTED 'Encoding.TYPE': 'k8s.V1Pod'}, 'Encoding.TYPE': 'dict'}}
{kubernetes_executor.py:554} ERROR - Invalid executor_config for TaskInstanceKey(dag_id='commit_check', task_id='sync_and_build', run_id='5776-2-1662037155', try_number=1, map_index=-1)
```

Looking in the UI, the task get stuck in scheduled state forever. By clicking instance details, it shows similar state of the pod_override with many Encoding.VAR. 


This appears like a recent addition, in 2.3.4 via https://github.com/apache/airflow/pull/24356. 
@dstandish  do you understand if this is connected?


### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.0.0
apache-airflow-providers-cncf-kubernetes==4.3.0
apache-airflow-providers-common-sql==1.1.0
apache-airflow-providers-docker==3.1.0
apache-airflow-providers-ftp==3.1.0
apache-airflow-providers-http==4.0.0
apache-airflow-providers-imap==3.0.0
apache-airflow-providers-postgres==5.2.0
apache-airflow-providers-sqlite==3.2.0
kubernetes==23.6.0

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/test_pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/utils/sqlalchemy.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Clearing dag run via UI fails on main branch and 2.5.0rc2
### Apache Airflow version

main (development)

### What happened

Create a simple dag, allow it to completely run through. 

Next, when in grid view, on the left hand side click on the dag run at the top level. 

On the right hand side, then click on "Clear existing tasks". This will error with the following on the web server:

```
[2022-11-29 17:55:05,939] {app.py:1742} ERROR - Exception on /dagrun_clear [POST]
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 2525, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1822, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1820, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.7/site-packages/flask/app.py", line 1796, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/opt/airflow/airflow/www/auth.py", line 47, in decorated
    return func(*args, **kwargs)
  File "/opt/airflow/airflow/www/decorators.py", line 83, in wrapper
    return f(*args, **kwargs)
  File "/opt/airflow/airflow/www/views.py", line 2184, in dagrun_clear
    confirmed=confirmed,
  File "/opt/airflow/airflow/www/views.py", line 2046, in _clear_dag_tis
    session=session,
  File "/opt/airflow/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/opt/airflow/airflow/models/dag.py", line 2030, in clear
    exclude_task_ids=exclude_task_ids,
  File "/opt/airflow/airflow/models/dag.py", line 1619, in _get_task_instances
    tis = session.query(TaskInstance)
AttributeError: 'NoneType' object has no attribute 'query'
```
https://github.com/apache/airflow/blob/527fbce462429fc9836837378f801eed4e9d194f/airflow/models/dag.py#L1619

As per issue title, fails on main branch and `2.5.0rc2`. Works fine on `2.3.3` and `2.4.3`. 


### What you think should happen instead

Tasks within the dag should be cleared as expected. 

### How to reproduce

Run a dag, attempt to clear it within the UI at the top level of the dag. 

### Operating System

Ran via breeze

### Versions of Apache Airflow Providers

N/A

### Deployment

Other 3rd-party Helm chart

### Deployment details

Tested via breeze. 

### Anything else

Happens every time. 

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/setup_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/models/dag.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Migrate MySQL example DAGs to new design
There is a new design of system tests that was introduced by the [AIP-47](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-47+New+design+of+Airflow+System+Tests).

All current example dags need to be migrated and converted into system tests, so they can be run in the CI process automatically before releases.

This is an aggregated issue for all example DAGs related to `MySQL` provider. It is created to track progress of their migration.

List of paths to example DAGs:
- [x] airflow/providers/mysql/example_dags/example_mysql.py

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/pinecone/example_pinecone_openai.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py']
Ground Truth : ['a/airflow/providers/mysql/example_dags/__init__.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Replace flask_oauthlib with Authlib

**Description**

flask_oauthlib has been deprecated in favour of Authlib. It would be good if airflow starts using Authlib

**Use case / motivation**

FlaskAppBuilder is now using Authlib. 
Since FlaskAppBuilder is deeply integrated into Airflow, it will be good to also have this Authlib. Flask-oauthlib documentation recommends Authlib

**Related Issues**


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/skipmixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/connection_wrapper.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/config_templates/default_webserver_config.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Backfill mode with mapped tasks: "Failed to populate all mapping metadata"
### Apache Airflow version

2.3.3

### What happened

I was backfilling some DAGs that use dynamic tasks when I got an exception like the following:

```
Traceback (most recent call last):
  File "/opt/conda/envs/production/bin/airflow", line 11, in <module>
    sys.exit(main())
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/__main__.py", line 38, in main
    args.func(args)
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 51, in command
    return func(*args, **kwargs)
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/utils/cli.py", line 99, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/cli/commands/dag_command.py", line 107, in dag_backfill
    dag.run(
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/models/dag.py", line 2288, in run
    job.run()
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/jobs/base_job.py", line 244, in run
    self._execute()
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/jobs/backfill_job.py", line 847, in _execute
    self._execute_dagruns(
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/jobs/backfill_job.py", line 737, in _execute_dagruns
    processed_dag_run_dates = self._process_backfill_task_instances(
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/jobs/backfill_job.py", line 612, in _process_backfill_task_instances
    for node, run_id, new_mapped_tis, max_map_index in self._manage_executor_state(
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/jobs/backfill_job.py", line 270, in _manage_executor_state
    new_tis, num_mapped_tis = node.expand_mapped_task(ti.run_id, session=session)
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 614, in expand_mapped_task
    operator.mul, self._resolve_map_lengths(run_id, session=session).values()
  File "/opt/conda/envs/production/lib/python3.9/site-packages/airflow/models/mappedoperator.py", line 600, in _resolve_map_lengths
    raise RuntimeError(f"Failed to populate all mapping metadata; missing: {keys}")
RuntimeError: Failed to populate all mapping metadata; missing: 'x'
```

Digging further, it appears this always happens if the task used as input to an `.expand` raises an Exception.  Airflow doesn't handle this exception gracefully like it does with exceptions in "normal" tasks, which can lead to other errors from deeper within Airflow.  This also means that since this is not a "typical" failure case, things like `--rerun-failed-tasks` do not work as expected.

### What you think should happen instead

Airflow should fail gracefully if exceptions are raised in dynamic task generators.

### How to reproduce

```
#!/usr/bin/env python3

import datetime
import logging

from airflow.decorators import dag, task


logger = logging.getLogger(__name__)


@dag(
    schedule_interval='@daily',
    start_date=datetime.datetime(2022, 8, 12),
    default_args={
        'retries': 5,
        'retry_delay': 5.0,
    },
)
def test_backfill():
    @task
    def get_tasks(ti=None):
        logger.info(f'{ti.try_number=}')
        if ti.try_number < 3:
            raise RuntimeError('')
        return ['a', 'b', 'c']

    @task
    def do_stuff(x=None, ti=None):
        logger.info(f'do_stuff: {x=}, {ti.try_number=}')
        if ti.try_number < 3:
            raise RuntimeError('')

    do_stuff.expand(x=do_stuff.expand(x=get_tasks()))
    do_stuff() >> do_stuff()  # this works as expected


dag = test_backfill()


if __name__ == '__main__':
    dag.cli()
```
```
airflow dags backfill test_backfill -s 2022-08-05 -e 2022-08-07 --rerun-failed-tasks
```
You can repeat the `backfill` command multiple times to slowly make progress through the DAG.  Things will eventually succeed (assuming the exception that triggers this bug stops being raised), but obviously this is a pain when trying to backfill a non-trivial number of DAG Runs.

### Operating System

CentOS Stream 8

### Versions of Apache Airflow Providers

None

### Deployment

Other

### Deployment details

Standalone

### Anything else

I was able to reproduce this both with SQLite + `SequentialExecutor` as well as with Postgres + `LocalExecutor`.

I haven't yet been able to reproduce this outside of `backfill` mode.

Possibly related since they mention the same exception text:
* #23533
* #23642

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py']
Ground Truth : ['a/airflow/models/mappedoperator.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: KubernetesPodOperator/KubernetesExecutor: Failed to adopt pod 422
### Apache Airflow version

2.3.0

### What happened

Here i provide steps to reproduce this.

Goal of this: to describe how to reproduce the "Failed to Adopt pod" error condition.

The DAG->step Described Below should be of type KubernetesPodOperator

NOTE: under normal operation,
(where the MAIN_AIRFLOW_POD is never recycled by k8s, we will never see this edge-case)
(it is only when the workerPod is still running, but the MAIN_AIRFLOW_POD is suddenly restarted/stopped)
(that we would see orphan->workerPods)

1] Implement a contrived-DAG, with a single step -> which is long-running (e.g. 6 minutes)
2] Deploy your airflow-2.1.4 / airfow-2.3.0 together with the contrived-DAG
3] Run your contrived-DAG.
4] in the middle of running the single-step, check via "kubectl" that your Kubernetes->workerPod has been created / running
5] while workerPod still running, do "kubectl delete pod <OF_MAIN_AIRFLOW_POD>". This will mean that the workerPod becomes an orphan.
6] the workerPod still continues to run through to completion. after which the K8S->status of the pod will be Completed, however the pod doesn't shut down itself.
7] "kubectl" start up a new <MAIN_AIRFLOW_POD> so the web-ui is running again.
8] MAIN_AIRFLOW_POD->webUi - Run your contrived-DAG again
9] while the contrived-DAG is starting/tryingToStart etc, you will see in the logs printed out "Failed to adopt pod" -> with 422 error code.

The step-9 with the error message, you will find two appearances of this error msg in the airflow-2.1.4, airflow-2.3.0 source-code.
The step-7 may also - general logging from the MAIN_APP - may also output the "Failed to adopt pod" error message also.



### What you think should happen instead

On previous versions of airflow e.g. 1.10.x, the orphan-workerPods would be adopted by the 2nd run-time of the airflowMainApp and either used to continue the same DAG and/or cleared away when complete.

This is not happening with the newer airflow 2.1.4 / 2.3.0 (presumably because the code changed), and upon the 2nd run-time of the airflowMainApp - it would seem to try to adopt-workerPod but fails at that point ("Failed to adopt pod" in the logs and hence it cannot clear away orphan pods).

Given this is an edge-case only, (i.e. we would not expect k8s to be recycling the main airflowApp/pod anyway), it doesn't seem totally urgent bug. However, the only reason for me raising this issue with yourselves is that given any k8s->namespace, in particular in PROD,   over time (e.g. 1 month?) the namespace will slowly be being filled up with orphanPods and somebody would need to manually log-in to delete old pods.

### How to reproduce

Here i provide steps to reproduce this.

Goal of this: to describe how to reproduce the "Failed to Adopt pod" error condition.

The DAG->step Described Below should be of type KubernetesPodOperator

NOTE: under normal operation,
(where the MAIN_AIRFLOW_POD is never recycled by k8s, we will never see this edge-case)
(it is only when the workerPod is still running, but the MAIN_AIRFLOW_POD is suddenly restarted/stopped)
(that we would see orphan->workerPods)

1] Implement a contrived-DAG, with a single step -> which is long-running (e.g. 6 minutes)
2] Deploy your airflow-2.1.4 / airfow-2.3.0 together with the contrived-DAG
3] Run your contrived-DAG.
4] in the middle of running the single-step, check via "kubectl" that your Kubernetes->workerPod has been created / running
5] while workerPod still running, do "kubectl delete pod <OF_MAIN_AIRFLOW_POD>". This will mean that the workerPod becomes an orphan.
6] the workerPod still continues to run through to completion. after which the K8S->status of the pod will be Completed, however the pod doesn't shut down itself.
7] "kubectl" start up a new <MAIN_AIRFLOW_POD> so the web-ui is running again.
8] MAIN_AIRFLOW_POD->webUi - Run your contrived-DAG again
9] while the contrived-DAG is starting/tryingToStart etc, you will see in the logs printed out "Failed to adopt pod" -> with 422 error code.

The step-9 with the error message, you will find two appearances of this error msg in the airflow-2.1.4, airflow-2.3.0 source-code.
The step-7 may also - general logging from the MAIN_APP - may also output the "Failed to adopt pod" error message also.



### Operating System

kubernetes

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

nothing special.

it (CI/CD pipeline) builds the app. using requirements.txt to pull-in all the required python dependencies (including there is a dependency for the airflow-2.1.4 / 2.3.0)

it (CI/CD pipeline) packages the app as an ECR image & then deploy directly to k8s namespace.

### Anything else

this is 100% reproducible each & every time.
i have tested this multiple times.

also - i tested this on the old airflow-1.10.x a couple of times to verify that the bug did not exist previously

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_base.py']
Ground Truth : ['a/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: LocalFileSystemToGCSOperator give false positive while copying file from src to dest, even when src has no file
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-google==6.4.0

### Apache Airflow version

2.1.4

### Operating System

Debian GNU/Linux 10 (buster)

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

When you run LocalFilesSystemToGCSOperator with the params for src and dest, the operator reports a false positive when there are no files present under the specified src directory. I expected it to fail stating the specified directory doesn't have any file.

[2022-03-15 14:26:15,475] {taskinstance.py:1107} INFO - Executing <Task(LocalFilesystemToGCSOperator): upload_files_to_GCS> on 2022-03-15T14:25:59.554459+00:00
[2022-03-15 14:26:15,484] {standard_task_runner.py:52} INFO - Started process 709 to run task
[2022-03-15 14:26:15,492] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'dag', 'upload_files_to_GCS', '2022-03-15T14:25:59.554459+00:00', '--job-id', '1562', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/dag.py', '--cfg-path', '/tmp/tmp_e9t7pl9', '--error-file', '/tmp/tmpyij6m4er']
[2022-03-15 14:26:15,493] {standard_task_runner.py:77} INFO - Job 1562: Subtask upload_files_to_GCS
[2022-03-15 14:26:15,590] {logging_mixin.py:104} INFO - Running <TaskInstance: dag.upload_files_to_GCS 2022-03-15T14:25:59.554459+00:00 [running]> on host 653e566fd372
[2022-03-15 14:26:15,752] {taskinstance.py:1300} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=jet2
AIRFLOW_CTX_DAG_ID=dag
AIRFLOW_CTX_TASK_ID=upload_files_to_GCS
AIRFLOW_CTX_EXECUTION_DATE=2022-03-15T14:25:59.554459+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-03-15T14:25:59.554459+00:00
[2022-03-15 14:26:19,357] {taskinstance.py:1204} INFO - Marking task as SUCCESS. gag, task_id=upload_files_to_GCS, execution_date=20220315T142559, start_date=20220315T142615, end_date=20220315T142619
[2022-03-15 14:26:19,422] {taskinstance.py:1265} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-03-15 14:26:19,458] {local_task_job.py:149} INFO - Task exited with return code 0

### What you think should happen instead

The operator should at least info that no files were copied than just making it successful. 

### How to reproduce

- create a Dag with LocalFilesSystemToGCSOperator 
- specify an empty directory as src and a gcp bucket as bucket_name, dest param(can be blank). 
- run the dag

### Anything else

No

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/presto/hooks/test_presto.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_appflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_branch_datetime_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_mapped_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/local_to_gcs.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Simplify logic to resolve tasks stuck in queued despite stalled_task_timeout
closes: #28120
closes: #21225
closes: #28943

Tasks occasionally get stuck in queued and aren't resolved by `stalled_task_timeout` (#28120). This PR moves the logic for handling stalled tasks to the scheduler and simplifies the logic by marking any task that has been queued for more than `scheduler.task_queued_timeout` as failed, allowing it to be retried if the task has available retries.

This doesn't require an additional scheduler nor allow for the possibility of tasks to get stuck in an infinite loop of scheduled -> queued -> scheduled ... -> queued as exists in #28943.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/dependencies_deps.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/jobs/scheduler_job_runner.py', 'a/airflow/kubernetes/kube_config.py', 'a/airflow/configuration.py', 'a/airflow/executors/celery_executor.py', 'a/docs/conf.py', 'a/airflow/executors/kubernetes_executor.py', 'a/airflow/executors/local_kubernetes_executor.py', 'a/airflow/executors/base_executor.py', 'a/airflow/executors/celery_kubernetes_executor.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Dag Dependency page not showing anything
**Apache Airflow version**: 2.1.

**Environment**: Ubuntu 20.04

- **Cloud provider or hardware configuration**: AWS
- **OS** (e.g. from /etc/os-release): UBUNTU 20.04 LTS
- **Kernel** (e.g. `uname -a`): Linux 20.04.1-Ubuntu SMP Tue Jun 1 09:54:15 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
- **Install tools**: python and pip


**What happened**: After performing the upgrade from 2.0.2 to 2.10 using the guide available in the documentation, Airflow upgraded successfully, Dag dependency page isn't  working as expected.
The DAG dependency page doesn't show the dependency graph.

<!-- (please include exact error messages if you can) -->

**What you expected to happen**: I expected the dag dependency page to show the dags and their dependency in a Graph view

<!-- What do you think went wrong? -->

**How to reproduce it**: Its reproduced by opening these pages every time. 

![Dag Dependency Page](https://user-images.githubusercontent.com/43160555/123028222-6eee0000-d422-11eb-9664-a8c0ee2a6723.png)


How often does this problem occur? Once? Every time etc?
Every time

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>Upgrade Check  Log</summary> 

/home/ubuntu/env_airflow/lib/python3.8/site-packages/airflow/configuration.py:34                                            
6 DeprecationWarning: The hide_sensitive_variable_fields option in [admin] has been moved to the hide_sensitive_var_conn_fields option in [core] - the old setting has been used, but please update your config.
/home/ubuntu/env_airflow/lib/python3.8/site-packages/airflow/configuration.py:34                                            
6 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.
/home/ubuntu/env_airflow/lib/python3.8/site-packages/airflow/plugins_manager.py:                                            
239 DeprecationWarning: This decorator is deprecated.In previous versions, all subclasses of BaseOperator must use apply_default decorator for the`default_args` feature to work properly.
In current version, it is optional. The decorator is applied automatically using the metaclass.
/home/ubuntu/env_airflow/lib/python3.8/site-packages/airflow/configuration.py:34                                           
6 DeprecationWarning: The default_queue option in [celery] has been moved to the default_queue option in [operators] - the old setting has been used, but please update your config.

 </details>



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Connect to Compute Engine instance by name
Hello,

It would be very helpful to have an operator that allows you to execute the SSH command on another virtual machine without the need to manage RSA keys and without the need to configure IP addresses. 

* The server should be specified by instance name and zone.
* The key should be provided using the following methods (user selection):
   - [OSLogin](https://cloud.google.com/compute/docs/instances/managing-instance-access#gcloud_1)
   - [Project/instance metadata](https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys)
   - None 
-  We should support tunnel the ssh connection through [the Cloud Identity-Aware Proxy](https://cloud.google.com/iap/docs/using-tcp-forwarding). Thanks to this, we will be able access to VM instances that do not have public IP addresses or do not permit direct access over the internet.
- We should support Connect to instances using their internal IP addresses rather than their external IP addresses. Thanks to this, we will be able to connect from one instance to another on the same VPC network, over a VPN connection, or between two peered VPC networks.

In brief. This should work similarly to the [gcloud compute ssh](https://cloud.google.com/sdk/gcloud/reference/compute/ssh
) command.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/compute_ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/compute.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/sql_to_sheets/example_sql_to_sheets.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/cloud_memorystore/example_cloud_memorystore_memcached.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/compute.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/gcs/example_mysql_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/transfers/example_postgres_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_memorystore.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/operators/yandexcloud_dataproc.py']
Ground Truth : ['a/setup.py', 'a/airflow/models/connection.py', '/dev/null', 'a/airflow/providers/google/common/hooks/base_google.py', 'a/airflow/providers/google/cloud/hooks/compute.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Timetable registration a la OperatorLinks
Currently (as implemented in #17414), timetables are serialised by their classes full import path. This works most of the time, but not in some cases, including:

* Nested in class or function
* Declared directly in a DAG file without a valid import name (e.g. `12345.py`)

Its fundamentally impossible to fix some of the cases (e.g. function-local class declaration) due to how Python works, but by requiring the user to explicitly register the timetable class, we can at least expose that problem so users dont attempt to do that.

However, since the timetable actually would work a lot of times without any additional mechanism, Im also wondering if we should _require_ registration.

1. Always require registration. A DAG using an unregistered timetable class fails to serialise.
2. Only require registration when the timetable class has wonky import path. Normal classes work out of the box without registering, and user sees a serialisation error asking for registration otherwise.
3. Dont require registration. If a class cannot be correctly serialised, tell the user we cant do it and the timetable must be declared another way.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/interval.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/plugins_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/plugins/workday.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/plugins/workday.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/plugins_manager.py', 'a/airflow/serialization/serialized_objects.py']
Current Recall: 0.07502039359641076

=========================================================

ISSUE: Support 'capacityProviderStrategy' option for ECSOperator
**Description**

[AWS launches Fargate Spot](https://aws.amazon.com/about-aws/whats-new/2019/12/aws-launches-fargate-spot-save-up-to-70-for-fault-tolerant-applications/) on December, 2019. It can be configured in [run_task on boto3 with 'capacityProviderStrategy' option](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html#ECS.Client.run_task). It would be nice if we could set this option for ECSOperator.

**Use case / motivation**

Tasks with Fargate Spot are cheaper than tasks without Fargate Spot.

**Related Issues**

None found.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_eks_with_nodegroup_in_one_step.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/dms.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/datasync.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/athena.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/appflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/dms.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/ecs.py']
Current Recall: 0.0771617212195371

=========================================================

ISSUE: Cast to string in ds macro functions

As already written in this issue https://github.com/apache/airflow/issues/19241 strptime function required string, but got proxy if the variables ds/next_ds (the types of these variables changed on version 2.2.0) sent.
This change will make the function `ds_add` and `ds_format` backward compatible.

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/main/UPDATING.md).

next_ds changed to proxy and it cannot be used in ds_add macro function
### Apache Airflow version

2.2.0 (latest released)

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

Tried to use this this code:
`some_variable='{{macros.ds_format(macros.ds_add(next_ds, '
                 '(ti.start_date - ti.execution_date).days), '
                 '"%Y-%m-%d", "%Y-%m-%d 21:00:00")}}')`
but got this error:
`strptime() argument 1 must be str, not Proxy`
because the `next_ds` variable changed to proxy.

### What you expected to happen

_No response_

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py']
Ground Truth : ['a/airflow/macros/__init__.py']
Current Recall: 0.0771617212195371

=========================================================

ISSUE: Wasb connection to Azure Fileshareservice Hook not working
**Apache Airflow version**: 
Version: v2.1.0
Git Version: .release:2.1.0+304e174674ff6921cb7ed79c0158949b50eff8fe


**What happened**:
INFO - __init__() got an unexpected keyword argument 'extra__wasb__connection_string'

**What you expected to happen**:
We have used the extra field and populated the SAS token according to the documents on how to use wasb connection for fileshare and we should be able to login in using the SAS token. This has been working in 2.0.0

<img width="416" alt="airflow_bug" src="https://user-images.githubusercontent.com/1429314/120696017-a85dcc00-c471-11eb-925d-05794e33d40b.png">




Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/connection_wrapper.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/synapse.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/pagerduty/hooks/pagerduty.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/fileshare.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/transfers/copy_into_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/microsoft/azure/hooks/test_wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_logging_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/sensors/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/transfers/sql_to_slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/azure_fileshare_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/operators/data_factory.py']
Ground Truth : ['a/airflow/providers/microsoft/azure/operators/azure_container_instances.py', 'a/airflow/providers/microsoft/azure/hooks/azure_container_volume.py', 'a/airflow/providers/google/cloud/example_dags/example_azure_fileshare_to_gcs.py', 'a/airflow/providers/google/cloud/transfers/azure_fileshare_to_gcs.py', 'a/airflow/providers/microsoft/azure/hooks/azure_fileshare.py']
Current Recall: 0.0771617212195371

=========================================================

ISSUE: Timeout is ambiguous in SSHHook and SSHOperator
In SSHHook the timeout argument of the constructor is used to set a connection timeout. This is fine.

But in SSHOperator the timeout argument of the constructor is used for *both* the timeout of the SSHHook *and* the timeout of the command itself (see paramiko's ssh client exec_command use of the timeout parameter). This ambiguous use of the same parameter is very dirty.

I see two ways to clean the behaviour: 

1. Let the SSHHook constructor be the only way to handle the connection timeout (thus, if one wants a specific timeout they should explicitely build a hook to be passed to the operator using the operator's constructor).
2. Split the timeout argument in SSHOperator into two arguments conn_timeout and cmd_timeout for example.

The choice between 1 and 2 depends on how frequently people are supposed to want to change the connection timeout. If it is something very frequent. then go for 2. if not go for 1.

BR and thanks for the code!

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ssh/hooks/ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/ssh/operators/test_ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ssh/operators/ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/ssh/hooks/test_ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/hooks/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/compute.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/process_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/docker/hooks/test_docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/providers/ssh/hooks/ssh.py', 'a/airflow/providers/ssh/operators/ssh.py']
Current Recall: 0.07930304884266344

=========================================================

ISSUE: Elasticsearch remote log will not fetch task logs from manual dagruns before 2.2 upgrade
### Apache Airflow Provider(s)

elasticsearch

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==1!2.5.0
apache-airflow-providers-cncf-kubernetes==1!2.1.0
apache-airflow-providers-datadog==1!2.0.1
apache-airflow-providers-elasticsearch==1!2.1.0
apache-airflow-providers-ftp==1!2.0.1
apache-airflow-providers-google==1!6.1.0
apache-airflow-providers-http==1!2.0.1
apache-airflow-providers-imap==1!2.0.1
apache-airflow-providers-microsoft-azure==1!3.3.0
apache-airflow-providers-mysql==1!2.1.1
apache-airflow-providers-postgres==1!2.3.0
apache-airflow-providers-redis==1!2.0.1
apache-airflow-providers-slack==1!4.1.0
apache-airflow-providers-sqlite==1!2.0.1
apache-airflow-providers-ssh==1!2.3.0
```

### Apache Airflow version

2.2.2

### Operating System

Debian Bullseye

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

After upgrading to 2.2, task logs from manual dagruns performed before the upgrade could no longer be retrieved, even though they can still be seen in Kibana. Scheduled dagruns' tasks and tasks for dagruns begun after the upgrade are retrieved without issue.

The issue appears to be because these tasks with missing logs all belong to dagruns that do not have the attribute data_interval_start or data_interval_end set.

### What you expected to happen

Task logs continue to be fetched after upgrade.

### How to reproduce

Below is how I verified the log fetching process.

I ran the code snippet in a python interpreter in the scheduler to test log fetching.

```py
from airflow.models import TaskInstance, DagBag, DagRun
from airflow.settings import Session, DAGS_FOLDER
from airflow.configuration import conf
import logging
from dateutil import parser

logger = logging.getLogger('airflow.task')
task_log_reader = conf.get('logging', 'task_log_reader')
handler = next((handler for handler in logger.handlers if handler.name == task_log_reader), None)

dag_id = 'pipeline_nile_reconciliation'
task_id = 'nile_overcount_spend_resolution_task'
execution_date = parser.parse('2022-01-10T11:49:57.197933+00:00')
try_number=1

session = Session()
ti = session.query(TaskInstance).filter(
    TaskInstance.dag_id == dag_id,
    TaskInstance.task_id == task_id,
    TaskInstance.execution_date == execution_date).first()
dagrun = session.query(DagRun).filter(
    DagRun.dag_id == dag_id,
    DagRun.execution_date == execution_date).first()


dagbag = DagBag(DAGS_FOLDER, read_dags_from_db=True)
dag = dagbag.get_dag(dag_id)
ti.task = dag.get_task(ti.task_id)
ti.dagrun = dagrun

handler.read(ti, try_number, {})
```

The following error log indicates errors in the log reading.

```
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/lib/python3.9/site-packages/airflow/utils/log/file_task_handler.py", line 239, in read
    log, metadata = self._read(task_instance, try_number_element, metadata)
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/elasticsearch/log/es_task_handler.py", line 168, in _read
    log_id = self._render_log_id(ti, try_number)
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/elasticsearch/log/es_task_handler.py", line 107, in _render_log_id
    data_interval_start = self._clean_date(dag_run.data_interval_start)
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/elasticsearch/log/es_task_handler.py", line 134, in _clean_date
    return value.strftime("%Y_%m_%dT%H_%M_%S_%f")
AttributeError: 'NoneType' object has no attribute 'strftime'
```


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py']
Ground Truth : ['a/airflow/providers/elasticsearch/log/es_task_handler.py', 'a/airflow/utils/log/file_task_handler.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: max_tis_per_query=0 leads to nothing being scheduled in 2.0.0
After upgrading to airflow 2.0.0 it seems as if the scheduler isn't working anymore. Tasks hang on scheduled state, but no tasks get executed. I've tested this with sequential and celery executor. When using the celery executor no messages seem to arrive in RabbiyMq

This is on local docker. Everything was working fine before upgrading. There don't seem to be any error messages, so I'm not completely sure if this is a bug or a misconfiguration on my end. 

Using python:3.7-slim-stretch Docker image. Regular setup that we're using is CeleryExecutor. Mysql version is 5.7 

Any help would be greatly appreciated.

**Python packages**
alembic==1.4.3
altair==4.1.0
amazon-kclpy==1.5.0
amqp==2.6.1
apache-airflow==2.0.0
apache-airflow-providers-amazon==1.0.0
apache-airflow-providers-celery==1.0.0
apache-airflow-providers-ftp==1.0.0
apache-airflow-providers-http==1.0.0
apache-airflow-providers-imap==1.0.0
apache-airflow-providers-jdbc==1.0.0
apache-airflow-providers-mysql==1.0.0
apache-airflow-providers-sqlite==1.0.0
apache-airflow-upgrade-check==1.1.0
apispec==3.3.2
appdirs==1.4.4
argcomplete==1.12.2
argon2-cffi==20.1.0
asn1crypto==1.4.0
async-generator==1.10
attrs==20.3.0
azure-common==1.1.26
azure-core==1.9.0
azure-storage-blob==12.6.0
Babel==2.9.0
backcall==0.2.0
bcrypt==3.2.0
billiard==3.6.3.0
black==20.8b1
bleach==3.2.1
boa-str==1.1.0
boto==2.49.0
boto3==1.7.3
botocore==1.10.84
cached-property==1.5.2
cattrs==1.1.2
cbsodata==1.3.3
celery==4.4.2
certifi==2020.12.5
cffi==1.14.4
chardet==3.0.4
click==7.1.2
clickclick==20.10.2
cmdstanpy==0.9.5
colorama==0.4.4
colorlog==4.0.2
commonmark==0.9.1
connexion==2.7.0
convertdate==2.3.0
coverage==4.2
croniter==0.3.36
cryptography==3.3.1
cycler==0.10.0
Cython==0.29.21
decorator==4.4.2
defusedxml==0.6.0
dill==0.3.3
dnspython==2.0.0
docutils==0.14
email-validator==1.1.2
entrypoints==0.3
ephem==3.7.7.1
et-xmlfile==1.0.1
fbprophet==0.7.1
fire==0.3.1
Flask==1.1.2
Flask-AppBuilder==3.1.1
Flask-Babel==1.0.0
Flask-Bcrypt==0.7.1
Flask-Caching==1.9.0
Flask-JWT-Extended==3.25.0
Flask-Login==0.4.1
Flask-OpenID==1.2.5
Flask-SQLAlchemy==2.4.4
flask-swagger==0.2.13
Flask-WTF==0.14.3
flatten-json==0.1.7
flower==0.9.5
funcsigs==1.0.2
future==0.18.2
graphviz==0.15
great-expectations==0.13.2
gunicorn==19.10.0
holidays==0.10.4
humanize==3.2.0
idna==2.10
importlib-metadata==1.7.0
importlib-resources==1.5.0
inflection==0.5.1
ipykernel==5.4.2
ipython==7.19.0
ipython-genutils==0.2.0
ipywidgets==7.5.1
iso8601==0.1.13
isodate==0.6.0
itsdangerous==1.1.0
JayDeBeApi==1.2.3
jdcal==1.4.1
jedi==0.17.2
jellyfish==0.8.2
Jinja2==2.11.2
jmespath==0.10.0
joblib==1.0.0
JPype1==1.2.0
json-merge-patch==0.2
jsonpatch==1.28
jsonpointer==2.0
jsonschema==3.2.0
jupyter-client==6.1.7
jupyter-core==4.7.0
jupyterlab-pygments==0.1.2
kinesis-events==0.1.0
kiwisolver==1.3.1
kombu==4.6.11
korean-lunar-calendar==0.2.1
lazy-object-proxy==1.4.3
lockfile==0.12.2
LunarCalendar==0.0.9
Mako==1.1.3
Markdown==3.3.3
MarkupSafe==1.1.1
marshmallow==3.10.0
marshmallow-enum==1.5.1
marshmallow-oneofschema==2.0.1
marshmallow-sqlalchemy==0.23.1
matplotlib==3.3.3
mistune==0.8.4
mock==1.0.1
mockito==1.2.2
msrest==0.6.19
mypy-extensions==0.4.3
mysql-connector-python==8.0.18
mysqlclient==2.0.2
natsort==7.1.0
nbclient==0.5.1
nbconvert==6.0.7
nbformat==5.0.8
nest-asyncio==1.4.3
nose==1.3.7
notebook==6.1.5
numpy==1.19.4
oauthlib==3.1.0
openapi-spec-validator==0.2.9
openpyxl==3.0.5
oscrypto==1.2.1
packaging==20.8
pandas==1.1.5
pandocfilters==1.4.3
parso==0.7.1
pathspec==0.8.1
pendulum==2.1.2
pexpect==4.8.0
phonenumbers==8.12.15
pickleshare==0.7.5
Pillow==8.0.1
prison==0.1.3
prometheus-client==0.8.0
prompt-toolkit==3.0.8
protobuf==3.14.0
psutil==5.8.0
ptyprocess==0.6.0
pyarrow==2.0.0
pycodestyle==2.6.0
pycparser==2.20
pycryptodomex==3.9.9
pydevd-pycharm==193.5233.109
Pygments==2.7.3
PyJWT==1.7.1
PyMeeus==0.3.7
pyodbc==4.0.30
pyOpenSSL==19.1.0
pyparsing==2.4.7
pyrsistent==0.17.3
pystan==2.19.1.1
python-crontab==2.5.1
python-daemon==2.2.4
python-dateutil==2.8.1
python-editor==1.0.4
python-nvd3==0.15.0
python-slugify==4.0.1
python3-openid==3.2.0
pytz==2019.3
pytzdata==2020.1
PyYAML==5.3.1
pyzmq==20.0.0
recordlinkage==0.14
regex==2020.11.13
requests==2.23.0
requests-oauthlib==1.3.0
rich==9.2.0
ruamel.yaml==0.16.12
ruamel.yaml.clib==0.2.2
s3transfer==0.1.13
scikit-learn==0.23.2
scipy==1.5.4
scriptinep3==0.3.1
Send2Trash==1.5.0
setproctitle==1.2.1
setuptools-git==1.2
shelljob==0.5.6
six==1.15.0
sklearn==0.0
snowflake-connector-python==2.3.7
snowflake-sqlalchemy==1.2.4
SQLAlchemy==1.3.22
SQLAlchemy-JSONField==1.0.0
SQLAlchemy-Utils==0.36.8
swagger-ui-bundle==0.0.8
tabulate==0.8.7
TagValidator==0.0.8
tenacity==6.2.0
termcolor==1.1.0
terminado==0.9.1
testpath==0.4.4
text-unidecode==1.3
threadpoolctl==2.1.0
thrift==0.13.0
toml==0.10.2
toolz==0.11.1
tornado==6.1
tqdm==4.54.1
traitlets==5.0.5
typed-ast==1.4.1
typing-extensions==3.7.4.3
tzlocal==1.5.1
unicodecsv==0.14.1
urllib3==1.24.2
validate-email==1.3
vine==1.3.0
watchtower==0.7.3
wcwidth==0.2.5
webencodings==0.5.1
Werkzeug==1.0.1
widgetsnbextension==3.5.1
wrapt==1.12.1
WTForms==2.3.1
xlrd==2.0.1
XlsxWriter==1.3.7
zipp==3.4.0

**Relevant config**
```
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repositories
# This path must be absolute
dags_folder = /usr/local/airflow/dags

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
sql_alchemy_conn = db+mysql://airflow:airflow@postgres/airflow

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite.
sql_alchemy_pool_recycle = 3600

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# When not using pools, tasks are run in the "default pool",
# whose size is guided by this config element
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# How long before timing out a python file import while filling the DagBag
dagbag_import_timeout = 60

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# Whether to enable pickling for xcom (note that this is insecure and allows for
# RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
enable_xcom_pickling = True

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

#  This flag decides whether to serialise DAGs and persist them in DB. If set to True, Webserver reads from DB instead of parsing DAG files
store_dag_code = True

# You can also update the following default configurations based on your needs
min_serialized_dag_update_interval = 30
min_serialized_dag_fetch_interval = 10

[celery]
# This section only applies if you are using the CeleryExecutor in
# [core] section above

# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# "airflow worker" command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
broker_url = amqp://amqp:5672/1

# Another key Celery setting
result_backend = db+mysql://airflow:airflow@postgres/airflow

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it `airflow flower`. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# This defines the port that Celery Flower runs on
flower_port = 5555

# Default queue that tasks get assigned to and that worker listen on.
default_queue = airflow

# Import path for celery configuration options
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG

# No SSL
ssl_active = False

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# after how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# after how much time a new DAGs should be picked up from the filesystem
min_file_process_interval = 60

use_row_level_locking=False

dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

child_process_log_directory = /usr/local/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is False,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
catchup_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# This depends on query length limits and how long you are willing to hold locks.
# 0 for no limit
max_tis_per_query = 0

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
parsing_processes = 4

authenticate = False
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: DB migration job fails with circular import
### Apache Airflow version

2.6.0

### What happened

I upgraded my Airflow 2.5.3 to 2.6.0 using the official Helm chart 1.9.0 installation on a Kubernetes cluster. The DB migration job fails with a circular import of "TaskInstanceKey". The image I'm using is `apache/airflow:2.6.0-python3.10`. I'm using CeleryKubernetesExecutor in my configuration.

Here is the stacktrace:
```
Traceback (most recent call last):
File "/home/airflow/.local/bin/airflow", line 8, in <module>
sys.exit(main())
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/__main__.py", line 48, in main
args.func(args)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 51, in command
return func(*args, **kwargs)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py", line 112, in wrapper
return f(*args, **kwargs)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/db_command.py", line 84, in upgradedb
db.upgradedb(
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
return func(*args, session=session, **kwargs)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/db.py", line 1544, in upgradedb
import_all_models()
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py", line 60, in import_all_models
__getattr__(name)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py", line 78, in __getattr__
val = import_string(f"{path}.{name}")
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/module_loading.py", line 36, in import_string
module = import_module(module_path)
File "/usr/local/lib/python3.10/importlib/__init__.py", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
File "<frozen importlib._bootstrap_external>", line 883, in exec_module
File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dag.py", line 82, in <module>
from airflow.models.dagrun import DagRun
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagrun.py", line 57, in <module>
from airflow.models.taskinstance import TaskInstance as TI
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 99, in <module>
from airflow.sentry import Sentry
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/sentry.py", line 195, in <module>
Sentry = ConfiguredSentry()
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/sentry.py", line 92, in __init__
executor_class, _ = ExecutorLoader.import_default_executor_cls()
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/executor_loader.py", line 158, in import_default_executor_cls
executor, source = cls.import_executor_cls(executor_name)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/executor_loader.py", line 134, in import_executor_cls
return _import_and_validate(cls.executors[executor_name]), ConnectorSource.CORE
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/executor_loader.py", line 129, in _import_and_validate
executor = import_string(path)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/module_loading.py", line 36, in import_string
module = import_module(module_path)
File "/usr/local/lib/python3.10/importlib/__init__.py", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/celery_kubernetes_executor.py", line 26, in <module>
from airflow.executors.kubernetes_executor import KubernetesExecutor
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/kubernetes_executor.py", line 44, in <module>
from airflow.kubernetes import pod_generator
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/kubernetes/pod_generator.py", line 46, in <module>
from airflow.kubernetes.kubernetes_helper_functions import add_pod_suffix, rand_str
File "/home/airflow/.local/lib/python3.10/site-packages/airflow/kubernetes/kubernetes_helper_functions.py", line 26, in <module>
from airflow.models.taskinstance import TaskInstanceKey
ImportError: cannot import name 'TaskInstanceKey' from partially initialized module 'airflow.models.taskinstance' (most likely due to a circular import) (/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py)
```


### What you think should happen instead

The DB migration job will start without an error on circular import.

### How to reproduce

I have a complex automation pipeline with many configurations, so, for now, I will not put my details configurations here. Please let me know if you need specific details.

I installed Airflow on my Kubernetes cluster using the official Helm chart 1.9.0. My database is Postgres. The DB migration job starts, but it fails with the error above.

### Operating System

Linux, AWS EKS-based Kubernetes

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

My full configuration is large and contains sensitive data. Please let me know if you need specific details.

I installed Airflow on my Kubernetes cluster using the official Helm chart 1.9.0. My database is Postgres. The DB migration job starts, but it fails with the error above.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/providers/google/cloud/links/dataproc.py', 'a/airflow/executors/dask_executor.py', '/dev/null', 'a/docs/conf.py', 'a/airflow/providers/databricks/operators/databricks.py', 'a/airflow/executors/celery_kubernetes_executor.py', 'a/airflow/providers/google/cloud/links/datafusion.py', 'a/airflow/providers/qubole/operators/qubole.py', 'a/airflow/models/xcom.py', 'a/airflow/providers/microsoft/azure/operators/data_factory.py', 'a/airflow/providers/google/cloud/operators/dataproc_metastore.py', 'a/airflow/executors/celery_executor.py', 'a/airflow/executors/local_executor.py', 'a/airflow/kubernetes/kubernetes_helper_functions.py', 'a/airflow/providers/amazon/aws/links/base_aws.py', 'a/airflow/executors/base_executor.py', 'a/airflow/executors/debug_executor.py', 'a/airflow/executors/sequential_executor.py', 'a/airflow/providers/google/cloud/links/base.py', 'a/airflow/models/baseoperator.py', 'a/airflow/operators/trigger_dagrun.py', 'a/airflow/executors/kubernetes_executor.py', 'a/airflow/providers/google/cloud/operators/bigquery.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Replace Airflow Slack Invite old link to short link
Follow up to https://github.com/apache/airflow/pull/10034

https://apache-airflow-slack.herokuapp.com/ to https://s.apache.org/airflow-slack/

<!--
Thank you for contributing! Please make sure that your code changes
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.

Feel free to ping committers for the review!

In case of existing issue, reference it using one of the following:

closes: #ISSUE
related: #ISSUE

How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->

---
**^ Add meaningful description above**

Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py']
Ground Truth : ['a/docs/build_docs.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Error setting dependencies on task_group defined using the decorator
### Apache Airflow version

2.2.2 (latest released)

### Operating System

MacOS 11.6.1

### Versions of Apache Airflow Providers

$ pip freeze | grep airflow
apache-airflow==2.2.2
apache-airflow-providers-celery==2.1.0
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-sqlite==2.0.1

### Deployment

Other

### Deployment details

`airflow standalone`

### What happened

```
AttributeError: 'NoneType' object has no attribute 'update_relative'
```

### What you expected to happen

Task group should be set as downstream of `start` task, and upstream of `end` task

### How to reproduce

* Add the following code to dags folder

```python
from datetime import datetime

from airflow.decorators import dag, task, task_group


@dag(start_date=datetime(2023, 1, 1), schedule_interval="@once")
def test_dag_1():
    @task
    def start():
        pass

    @task
    def do_thing(x):
        print(x)

    @task_group
    def do_all_things():
        do_thing(1)
        do_thing(2)

    @task
    def end():
        pass

    start() >> do_all_things() >> end()


test_dag_1()
```

* Run `airflow standalone`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py']
Ground Truth : ['a/airflow/decorators/task_group.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Allow custom email backends
The current `send_email` helper uses SMTP, but sometimes it's useful to be able to send mail without SMTP--for example when using API-based services like Sendgrid, Mailgun, etc. It would be useful to include a hook for overriding `send_email`--maybe a Django-style override like `email_backend = path.to.plugin:send_email`. If folks would be interested, I'm happy to send in a patch for this.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/email.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/send_email.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sendgrid/utils/emailer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/smtp/hooks/smtp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/ses.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_dag_decorator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/sendgrid/utils/test_emailer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/operators/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/email.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_email.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/smtp/notifications/smtp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/secrets/key_vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/ads/hooks/ads.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/otel_logger.py']
Ground Truth : ['a/airflow/utils.py', 'a/airflow/configuration.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: UI time zone not used in Gantt charts tooltips
**Apache Airflow version**: v2.0.0.dev0

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): -

**Environment**: Docker 

- **Cloud provider or hardware configuration**: -
- **OS** (e.g. from /etc/os-release): Debian GNU/Linux 10 (buster)
- **Kernel** (e.g. `uname -a`): 
Linux 6dac905dd519 4.19.76-linuxkit #1 SMP Thu Oct 17 19:31:58 UTC 2019 x86_64 GNU/Linux
- **Install tools**: Airflow Breeze (image: apache/airflow:master-python3.6-ci)
- **Others**: -
**What happened**:

Gantt charts seem to always use UTC for displaying task start/end dates, but use local time for time axis, regardless of the time zone set in UI (see screens below: my local time zone is CEST).
<img width="815" alt="Zrzut ekranu 2020-04-8 o 09 18 50" src="https://user-images.githubusercontent.com/34898234/78764706-3c73f680-7987-11ea-925b-e394ac329298.png">
<img width="840" alt="Zrzut ekranu 2020-04-8 o 09 17 17" src="https://user-images.githubusercontent.com/34898234/78764752-4eee3000-7987-11ea-957b-338376d1bf41.png">
<img width="925" alt="Zrzut ekranu 2020-04-8 o 10 58 09" src="https://user-images.githubusercontent.com/34898234/78765233-01be8e00-7988-11ea-8215-9179102cb390.png">

**What you expected to happen**:

Time zone of the time axis and task start/end dates should be the same - probably the time zone specified in the UI should be used.

**How to reproduce it**:

Simply go to Gantt chart of any Dag Run and try changing the time zone in the UI to see that neither the time axis nor task start/end dates reflect the change.

You can also change the local time on your workstation and refresh the page with Gantt chart to see that time axis changed to reflect the new time zone.

**Anything else we need to know**: -

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/compute.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/cloud/sql_to_sheets/example_sql_to_sheets.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: AIP-56 - FAB AM - Permissions view
Move permissions view to FAB Auth manager

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_user_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow-providers-fab/img/diagram_fab_auth_manager_airflow_architecture.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_role_and_permission_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/security_manager/aws_security_manager_override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/views/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/example_dags/example_looker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/dataform.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/views/user.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/utils/fab.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/views/user_edit.py']
Ground Truth : ['a/airflow/www/fab_security/views.py', 'a/airflow/www/security.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Dynamic Task Mapping not working with op_kwargs in PythonOperator
### Apache Airflow version

2.3.0 (latest released)

### What happened

The following DAG was written and expected to generate 3 tasks (one for each string in the list)

**dag_code**
```python
import logging
from airflow.decorators import dag, task
from airflow.operators.python import PythonOperator

from airflow.utils.dates import datetime


def log_strings_operator(string, *args, **kwargs):
    logging.info("we've made it into the method")
    logging.info(f"operator log - {string}")


@dag(
    dag_id='dynamic_dag_test',
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=['example', 'dynamic_tasks']
)
def tutorial_taskflow_api_etl():
    op2 = (PythonOperator
           .partial(task_id="logging_with_operator_task",
                    python_callable=log_strings_operator)
           .expand(op_kwargs=[{"string": "a"}, {"string": "b"}, {"string": "c"}]))

    return op2


tutorial_etl_dag = tutorial_taskflow_api_etl()
```

**error message**
```python
Broken DAG: [/usr/local/airflow/dags/dynamic_dag_test.py] Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 343, in _serialize
    return SerializedBaseOperator.serialize_mapped_operator(var)
  File "/usr/local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 608, in serialize_mapped_operator
    assert op_kwargs[Encoding.TYPE] == DAT.DICT
TypeError: list indices must be integers or slices, not Encoding

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 1105, in to_dict
    json_dict = {"__version": cls.SERIALIZER_VERSION, "dag": cls.serialize_dag(var)}
  File "/usr/local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 1013, in serialize_dag
    raise SerializationError(f'Failed to serialize DAG {dag.dag_id!r}: {e}')
airflow.exceptions.SerializationError: Failed to serialize DAG 'dynamic_dag_test': list indices must be integers or slices, not Encoding
```

### What you think should happen instead

Dag should contain 1 task `logging_with_operator_task` that contains 3 indices

### How to reproduce

copy/paste dag code into a dag file and run on airflow 2.3.0. Airflow UI will flag the error

### Operating System

Debian

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py']
Ground Truth : ['a/airflow/models/mappedoperator.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/decorators/base.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Add DagProcessor status to health endpoint.
### Description

Add DagProcessor status including latest heartbeat to health endpoint similar to Triggerer status added recently. Related PRs.

https://github.com/apache/airflow/pull/31529
https://github.com/apache/airflow/pull/27755

### Use case/motivation

It helps in dag processor monitoring 

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/chart/build_changelog_annotations.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py']
Ground Truth : ['a/airflow/api_connexion/schemas/health_schema.py', 'a/airflow/api/common/airflow_health.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: ExternalTaskSensor logs a poke message that lists the current execution_date, it should show the execution_date it is poking for.
The log for ExternalTaskSensor shows a poke for "dag.task on context[executino_date]" which is the execution_date of the running task.  But it would be more helpful to know what execution date it is actually looking for.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/subdag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/glue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/trigger_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/sensors/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/mark_tasks.py']
Ground Truth : ['a/airflow/operators/sensors.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Editing a jdbc connection in 1.5.1 throws internal server error
Editing an existing jdbc connection results in:

```
File "/usr/local/lib/python2.7/site-packages/airflow-1.5.1-py2.7.egg/airflow/www/app.py", line 1765, in on_form_prefill
    field = getattr(form, field)
AttributeError: 'ConnectionForm' object has no attribute 'extra__jdbc__drv_path'
```

At the same time creating a new jdbc connection doesn't show the extra form fields anymore.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jdbc/hooks/jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/forms.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/hooks/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/data_lake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/hooks/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datacatalog.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/connection_wrapper.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_memorystore.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py']
Ground Truth : ['a/airflow/www/app.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Running alembic migration downgrade on database causes error 
### Apache Airflow version

2.2.2 (latest released)

### Operating System

PRETTY_NAME="Debian GNU/Linux 10 (buster)" NAME="Debian GNU/Linux" VERSION_ID="10" VERSION="10 (buster)" VERSION_CODENAME=buster ID=debian HOME_URL="https://www.debian.org/" SUPPORT_URL="https://www.debian.org/support" BUG_REPORT_URL="https://bugs.debian.org/"

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==2.4.0
apache-airflow-providers-celery==2.1.0
apache-airflow-providers-cncf-kubernetes==2.1.0
apache-airflow-providers-datadog==2.0.1
apache-airflow-providers-docker==2.3.0
apache-airflow-providers-elasticsearch==2.1.0
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==6.1.0
apache-airflow-providers-grpc==2.0.1
apache-airflow-providers-hashicorp==2.1.1
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-microsoft-azure==3.3.0
apache-airflow-providers-mysql==2.1.1
apache-airflow-providers-odbc==2.0.1
apache-airflow-providers-postgres==2.3.0
apache-airflow-providers-redis==2.0.1
apache-airflow-providers-sendgrid==2.0.1
apache-airflow-providers-sftp==2.2.0
apache-airflow-providers-slack==4.1.0
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.3.0

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

My current database alembic_version is set at e9304a3141f0 

While running the command `alembic -c alembic.ini downgrade a13f7613ad25` I received the following error:

INFO  [alembic.runtime.migration] Running downgrade e9304a3141f0 -> 83f031fd9f1c, make xcom pkey columns non-nullable
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidTableDefinition: column "key" is in a primary key


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/bin/alembic", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/config.py", line 588, in main
    CommandLine(prog=prog).main(argv=argv)
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/config.py", line 582, in main
    self.run_cmd(cfg, options)
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/config.py", line 562, in run_cmd
    **dict((k, getattr(options, k, None)) for k in kwarg)
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/command.py", line 366, in downgrade
    script.run_env()
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/script/base.py", line 563, in run_env
    util.load_python_file(self.dir, "env.py")
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/util/pyfiles.py", line 92, in load_python_file
    module = load_module_py(module_id, path)
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/util/pyfiles.py", line 108, in load_module_py
    spec.loader.exec_module(module)  # type: ignore
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "migrations/env.py", line 107, in <module>
    run_migrations_online()
  File "migrations/env.py", line 101, in run_migrations_online
    context.run_migrations()
  File "<string>", line 8, in run_migrations
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/runtime/environment.py", line 851, in run_migrations
    self.get_context().run_migrations(**kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/runtime/migration.py", line 620, in run_migrations
    step.migration_fn(**kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/migrations/versions/e9304a3141f0_make_xcom_pkey_columns_non_nullable.py", line 76, in downgrade
    bop.alter_column("execution_date", type_=_get_timestamp(conn), nullable=True)
  File "/usr/local/lib/python3.7/contextlib.py", line 119, in __exit__
    next(self.gen)
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/operations/base.py", line 374, in batch_alter_table
    impl.flush()
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/operations/batch.py", line 107, in flush
    fn(*arg, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/ddl/postgresql.py", line 185, in alter_column
    **kw
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/ddl/impl.py", line 240, in alter_column
    existing_comment=existing_comment,
  File "/home/airflow/.local/lib/python3.7/site-packages/alembic/ddl/impl.py", line 197, in _exec
    return conn.execute(construct, multiparams)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1011, in execute
    return meth(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/ddl.py", line 72, in _execute_on_connection
    return connection._execute_ddl(self, multiparams, params)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1073, in _execute_ddl
    compiled,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.InvalidTableDefinition) column "key" is in a primary key

[SQL: ALTER TABLE xcom ALTER COLUMN key DROP NOT NULL]


It seems the DOWN migration step for revision `e9304a3141f0 - Make XCom primary key columns non-nullable` is not backwards compatible as it is attempting to make a Primary Key nullable, which is not possible.

I suspect the revision of `bbf4a7ad0465 - Remove id column from xcom` has something to do with this - id perhaps being the old primary key for the table.

Revision list: https://airflow.apache.org/docs/apache-airflow/stable/migrations-ref.html

### What you expected to happen

I expected the alembic version to downgrade to the appropriate version

### How to reproduce

On airflow image 2.2.2 attempt to run `alembic -c alembic.ini downgrade {any version lower than revision id e9304a3141f0}`



### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/env.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py']
Ground Truth : ['a/airflow/migrations/versions/7b2661a43ba3_taskinstance_keyed_to_dagrun.py', 'a/airflow/migrations/versions/e9304a3141f0_make_xcom_pkey_columns_non_nullable.py', 'a/airflow/migrations/versions/54bebd308c5f_add_trigger_table_and_task_info.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Provider dependencies do not work well with alphas.
The dependencies in Alphas are currently >= 2.0.0 and should be >=2.0.0a0 in order to work with Alphas in cases which are not PEP 440-compliant.

According to https://www.python.org/dev/peps/pep-0440/, >= 2.0.0 should also work with alpha/beta releases (a1/a2) but in some cases it does not (https://apache-airflow.slack.com/archives/C0146STM600/p1602774750041800) 

Changing to ">=2.0.0a0" should help.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/presto_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/trino_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/field_validator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py']
Ground Truth : ['a/provider_packages/setup_provider_packages.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: Custom Timetable Import Error
### Apache Airflow version

2.2.2 (latest released)

### Operating System

Darwin Kernel Version 21.1.0 RELEASE_ARM64_T8101 arm64

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

python_version  | 3.9.7 (default, Sep 16 2021, 23:53:23)  [Clang 12.0.0 ]

### What happened

The following error is displayed in Web UI:
```
Broken DAG: [<EDITED>/scripts/airflow/dags/master/sample_dag/sample_dag.py] Traceback (most recent call last):
  File "<EDITED>/miniconda3/envs/dev-airflow/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 271, in serialize_to_json
    serialized_object[key] = _encode_timetable(value)
  File "<EDITED>/miniconda3/envs/dev-airflow/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 152, in _encode_timetable
    raise _TimetableNotRegistered(importable_string)
airflow.serialization.serialized_objects._TimetableNotRegistered: Timetable class 'workday.AfterWorkdayTimetable' is not registered

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<EDITED>/miniconda3/envs/dev-airflow/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 937, in to_dict
    json_dict = {"__version": cls.SERIALIZER_VERSION, "dag": cls.serialize_dag(var)}
  File "<EDITED>/miniconda3/envs/dev-airflow/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py", line 849, in serialize_dag
    raise SerializationError(f'Failed to serialize DAG {dag.dag_id!r}: {e}')
airflow.exceptions.SerializationError: Failed to serialize DAG 'learning_example_workday_timetable': Timetable class 'workday.AfterWorkdayTimetable' is not registered
```

### What you expected to happen

For the custom timetable to be implemented and used by DAG.

### How to reproduce

Following instructions from [Custom DAG Scheduling with Timetables](https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html) with following new DAG to implement:

```python
import datetime

from airflow import DAG
from airflow import plugins_manager
from airflow.operators.dummy import DummyOperator

from workday import AfterWorkdayTimetable

with DAG(
    dag_id="learning_example_workday_timetable",
    start_date=datetime.datetime(2021,11,20),
    timetable=AfterWorkdayTimetable(),
    tags=["example","learning","timetable"],
) as dag:
    DummyOperator(task_id="run_this")
```

### Anything else

I have tried digging through the code and believe the issue is in this line:
https://github.com/apache/airflow/blob/fb478c00cdc5e78d5e85fe5ac103707c829be2fb/airflow/serialization/serialized_objects.py#L149

Perhaps the [Custom DAG Scheduling with Timetables](https://airflow.apache.org/docs/apache-airflow/stable/howto/timetable.html) expects an `__eq__` implemented in the `AfterWorkdayTimetable` class but it would appear that the `AfterWorkdayTimetable` class imported through the DAG and the `AfterWorkdayTimetable` class imported through `plugin_manager` have different `id()`'s:
https://github.com/apache/airflow/blob/fb478c00cdc5e78d5e85fe5ac103707c829be2fb/airflow/serialization/serialized_objects.py#L129

The only way I could get it to import successfully was via the following sequence of import statements since [_get_registered_timetable](https://github.com/apache/airflow/blob/fb478c00cdc5e78d5e85fe5ac103707c829be2fb/airflow/serialization/serialized_objects.py#L124) uses a lazy import:
```python
import datetime

from airflow import DAG
from airflow import plugins_manager
from airflow.operators.dummy import DummyOperator

plugins_manager.initialize_timetables_plugins()
from workday import AfterWorkdayTimetable
```

I also had the webserver and scheduler restarted and confirmed the plugin is seen via cli:
```bash
airflow plugins                                                                                                                    dev-airflow   19:01:43 
name                     | source
=========================+===========================
workday_timetable_plugin | $PLUGINS_FOLDER/workday.py
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/plugins_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py']
Ground Truth : ['a/airflow/serialization/serialized_objects.py']
Current Recall: 0.0803737126542266

=========================================================

ISSUE: AIP-56 - FAB AM - Move profile view into auth manager
The profile view (`/users/userinfo/`) needs to be moved to FAB auth manager. The profile URL needs to be returned as part of `get_url_account()` as specified in the AIP

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_user_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/hooks/campaign_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/views/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_role_and_permission_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow-providers-fab/img/diagram_fab_auth_manager_airflow_architecture.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/security/permissions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/default_webserver_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/utils/eks_test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/eks.py']
Ground Truth : ['a/airflow/www/security.py', '/dev/null', 'a/airflow/auth/managers/fab/fab_auth_manager.py', 'a/airflow/auth/managers/base_auth_manager.py', 'a/airflow/www/fab_security/views.py', 'a/airflow/www/extensions/init_appbuilder.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: AIP-56 - Move sync-perm command
It has been brought up by Jarek in this [discussion](https://github.com/apache/airflow/discussions/32187#discussioncomment-6323078). I dont have much context on it but it seems we need to move this command as well as part of the AIP-56 work

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/expandinput.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/suite/transfers/gcs_to_gdrive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_emr_eks.py']
Ground Truth : ['a/airflow/cli/cli_config.py', '/dev/null', 'a/airflow/auth/managers/fab/fab_auth_manager.py', 'a/airflow/auth/managers/base_auth_manager.py', 'a/airflow/cli/cli_parser.py', 'a/airflow/cli/commands/role_command.py', 'a/airflow/cli/commands/user_command.py', 'a/airflow/providers/celery/executors/celery_executor.py', 'a/airflow/www/extensions/init_auth_manager.py', 'a/airflow/cli/commands/sync_perm_command.py', 'a/airflow/cli/commands/standalone_command.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: API Endpoints - Read-only - Pools
**Description**
Hello 

We need to create several endpoints that perform basic read-only operations on **Pools** . We need the following endpoints:

- GET /pools
- GET /pools/{pool_name}

Detailed information is available in the issue:
https://github.com/apache/airflow/issues/8118

**Use case / motivation**
N/A

**Related Issues**
N/A

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/client/json_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/validators.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_pool_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py']
Ground Truth : ['/dev/null', 'a/airflow/api_connexion/endpoints/pool_endpoint.py', 'a/airflow/api_connexion/exceptions.py', 'a/airflow/api_connexion/parameters.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: Remove ability to import hooks from plugins in 2.0.0 
Simliar to #9506, but this time for master where we should remove entirely the ability to import hooks from plugins.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/hatch_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/mypy/plugin/outputs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_plugins_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/listeners/listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/pinecone/example_pinecone_openai.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/cassandra_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/plugins_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/plugins/test_plugins_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/plugins_command.py']
Ground Truth : ['a/airflow/cli/commands/plugins_command.py', 'a/airflow/models/taskinstance.py', 'a/airflow/models/dagbag.py', 'a/airflow/www/views.py', 'a/airflow/plugins_manager.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: Dynamic task mapping does not correctly handle depends_on_past
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

Using Airflow 2.4.2.

I've got a task that retrieves some filenames, which then creates dynamically mapped tasks to move the files, one per task.
I'm using a similar task across multiple DAGs. However, task mapping fails on some DAG runs: it inconsistently happens per DAG run, and some DAGs do not seem to be affected at all. These seem to be the DAGs where no task was ever mapped, so that the mapped task instance ended up in a Skipped state.

What happens is that multiple files will be found, but only a single dynamically mapped task will be created. This task never starts and has map_index of -1. It can be found under the "List instances, all runs" menu, but says "No Data found." under the "Mapped Tasks" tab.

When I press the "Run" button when the mapped task is selected, the following error appears:

```
Could not queue task instance for execution, dependencies not met: Previous Dagrun State: depends_on_past is true for this task's DAG, but the previous task instance has not run yet., Task has been mapped: The task has yet to be mapped!
```

The previous task *has* run however. No errors appeared in my Airflow logs.

### What you think should happen instead

The appropriate amount of task instances should be created, they should correctly resolve the ```depends_on_past``` check and then proceed to run correctly.

### How to reproduce

This DAG reliably reproduces the error for me. The first set of mapped tasks succeeds, the subsequent ones do not.

```python
from airflow import DAG
from airflow.decorators import task
import datetime as dt

from airflow.operators.python import PythonOperator

@task
def get_filenames_kwargs():
    return [
        {"file_name": i}
        for i in range(10)
    ]

def print_filename(file_name):
    print(file_name)

with DAG(
        dag_id="dtm_test",
        start_date=dt.datetime(2022, 12, 10),
        default_args={
            "owner": "airflow",
            "depends_on_past": True,
        },
        schedule="@daily",
) as dag:
    get_filenames_task = get_filenames_kwargs.override(task_id="get_filenames_task")()

    print_filename_task = PythonOperator.partial(
        task_id="print_filename_task",
        python_callable=print_filename,
    ).expand(op_kwargs=get_filenames_task)

    # Perhaps redundant
    get_filenames_task >> print_filename_task
```


### Operating System

Amazon Linux 2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py']
Ground Truth : ['a/airflow/ti_deps/deps/prev_dagrun_dep.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: Unable to add a new user when logged using LDAP auth
### Discussed in https://github.com/apache/airflow/discussions/18290

<div type='discussions-op-text'>

<sup>Originally posted by **pawsok** September 16, 2021</sup>
### Apache Airflow version

2.1.4 (latest released)

### Operating System

Amazon Linux AMI 2018.03

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

- AWS ECS EC2 mode
- RDS PostgreSQL for DB
- LDAP authentication enabled


### What happened

We upgraded Airflow from 2.0.1 to 2.1.3 and now when i log into Airflow (Admin role) using LDAP authentication and go to Security --> List Users i cannot see **add button** ("plus"). 

**Airflow 2.0.1** (our current version):

![image](https://user-images.githubusercontent.com/90831710/133586254-24e22cd6-7e02-4800-b90f-d6f575ba2826.png)

**Airflow 2.1.3:**

![image](https://user-images.githubusercontent.com/90831710/133586024-48952298-a906-4189-abe1-bd88d96518bc.png)


### What you expected to happen

Option to add a new user (using LDAP auth).

### How to reproduce

1. Upgrade to Airflow 2.1.3
2. Log in to Airflow as LDAP user type
3. Go to Security --> List Users

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/config_templates/default_webserver_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: AirflowMacroPluginRemovedRule fails on non-python files
**Apache Airflow version**: 1.10.14


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**: X
- **OS** (e.g. from /etc/os-release): X
- **Kernel** (e.g. `uname -a`): X
- **Install tools**: X
- **Others**: X

**What happened**:

The `AirflowMacroPluginRemovedRule` seems unable to process non-standard python files (e.g. `.xlsx`) and chokes out with an unhelpful error message.:

```python
========================================================================================================================================================== STATUS ==========================================================================================================================================================

Check for latest versions of apache-airflow and checker...........................................................................................................................................................................................................................................................SUCCESS
Traceback (most recent call last):
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/bin/airflow", line 37, in <module>
    args.func(args)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/checker.py", line 88, in run
    all_problems = check_upgrade(formatter, rules)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/checker.py", line 37, in check_upgrade
    rule_status = RuleStatus.from_rule(rule)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/problem.py", line 44, in from_rule
    result = rule.check()
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/airflow_macro_plugin_removed.py", line 52, in check
    problems.extend(self._check_file(file_path))
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/airflow_macro_plugin_removed.py", line 42, in _check_file
    for line_number, line in enumerate(file_pointer, 1):
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x82 in position 16: invalid start byte
```

**What you expected to happen**:

I expected the macro to skip over files it could not process/understand

**How to reproduce it**:

Add an `.xlsx` or other binary document to the DAGs folder and run the upgrade check.


**Suggested resolution**:

I think it's fine to fail out on these files (it led us to add certain items to the `.airflowignore` which should have been there anyway) but I had to modify the upgrade rule directly to tell me _which_ files were the problem. A more helpful error message here, and possibly a message prompting users to add said files to their `.airflowignore` would be ideal.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/setup_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py']
Ground Truth : ['a/airflow/upgrade/rules/airflow_macro_plugin_removed.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: [AIRFLOW-6529] Pickle error occurs when the scheduler tries to run on macOS.
When we try to run the scheduler on macOS, we will get a serialization error like as follows.
```
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2020-01-10 19:54:41,974] {executor_loader.py:59} INFO - Using executor SequentialExecutor
[2020-01-10 19:54:41,983] {scheduler_job.py:1462} INFO - Starting the scheduler
[2020-01-10 19:54:41,984] {scheduler_job.py:1469} INFO - Processing each file at most -1 times
[2020-01-10 19:54:41,984] {scheduler_job.py:1472} INFO - Searching for files in /Users/sarutak/airflow/dags
[2020-01-10 19:54:42,025] {scheduler_job.py:1474} INFO - There are 27 files in /Users/sarutak/airflow/dags
[2020-01-10 19:54:42,025] {scheduler_job.py:1527} INFO - Resetting orphaned tasks for active dag runs
[2020-01-10 19:54:42,059] {scheduler_job.py:1500} ERROR - Exception when executing execute_helper
Traceback (most recent call last):
  File "/Users/sarutak/work/oss/airflow-env/master-python3.8.1/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 1498, in _execute
    self._execute_helper()
  File "/Users/sarutak/work/oss/airflow-env/master-python3.8.1/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 1531, in _execute_helper
    self.processor_agent.start()
  File "/Users/sarutak/work/oss/airflow-env/master-python3.8.1/lib/python3.8/site-packages/airflow/utils/dag_processing.py", line 348, in start
    self._process.start()
  File "/opt/python/3.8.1/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/opt/python/3.8.1/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/opt/python/3.8.1/lib/python3.8/multiprocessing/context.py", line 283, in _Popen
    return Popen(process_obj)
  File "/opt/python/3.8.1/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/opt/python/3.8.1/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/opt/python/3.8.1/lib/python3.8/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/opt/python/3.8.1/lib/python3.8/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
AttributeError: Can't pickle local object 'SchedulerJob._execute.<locals>.processor_factory'
```

The reason is scheduler try to run subprocesses using multiprocessing with spawn mode and the mode tries to pickle objects. In this case, `processor_factory` inner method is tried to be pickled.
Actually, as of Python 3.8, spawn mode is the default mode in macOS.

The solution I propose is that pull the method out of the enclosing method.
---
Issue link: [AIRFLOW-6529](https://issues.apache.org/jira/browse/AIRFLOW-6529)

- [x] Description above provides context of the change
- [x] Commit message/PR title starts with `[AIRFLOW-NNNN]`. AIRFLOW-NNNN = JIRA ID<sup>*</sup>
- [x] Unit tests coverage for changes (not needed for documentation changes)
- [x] Commits follow "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)"
- [x] Relevant documentation is updated including usage instructions.
- [x] I will engage committers as explained in [Contribution Workflow Example](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#contribution-workflow-example).

<sup>*</sup> For document-only changes commit message can start with `[AIRFLOW-XXXX]`.

---
In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.
In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).
In case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py']
Ground Truth : ['/dev/null', 'a/airflow/utils/dag_processing.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/configuration.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: Manual task trigger fails for kubernetes executor with psycopg2 InvalidTextRepresentation error
### Apache Airflow version

main (development)

### What happened

Manual task trigger fails for kubernetes executor with the following error.  Manual trigger of dag works without any issue.

```
[2022-12-15 20:05:38,442] {app.py:1741} ERROR - Exception on /run [POST]
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidTextRepresentation: invalid input syntax for integer: "manual"
LINE 3: ...ate = 'queued' AND task_instance.queued_by_job_id = 'manual'
                                                               ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/flask/app.py", line 2525, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/airflow/.local/lib/python3.10/site-packages/flask/app.py", line 1822, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/airflow/.local/lib/python3.10/site-packages/flask/app.py", line 1820, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/airflow/.local/lib/python3.10/site-packages/flask/app.py", line 1796, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/www/auth.py", line 47, in decorated
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/www/decorators.py", line 125, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/www/views.py", line 1896, in run
    executor.start()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/kubernetes_executor.py", line 586, in start
    self.clear_not_launched_queued_tasks()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/kubernetes_executor.py", line 510, in clear_not_launched_queued_tasks
    queued_tis: list[TaskInstance] = query.all()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2773, in all
    return self._iter().all()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1714, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1943, in _execute_context
    self._handle_dbapi_exception(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.DataError: (psycopg2.errors.InvalidTextRepresentation) invalid input syntax for integer: "manual"
LINE 3: ...ate = 'queued' AND task_instance.queued_by_job_id = 'manual'
```
                                                               ^


### What you think should happen instead

should be able to trigger the task manually from the UI

### How to reproduce

deploy the main branch with kubernetes executor and postgres db. 

### Operating System

ubuntu 20

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Python version: 3.10.9
Airflow version: 2.6.0.dev0
helm.sh/chart=postgresql-10.5.3

### Anything else

the issue is caused due to this check: 

https://github.com/apache/airflow/blob/b263dbcb0f84fd9029591d1447a7c843cb970f15/airflow/executors/kubernetes_executor.py#L505-L507

in `celery_executor` there is a similar check, but i believe it is not called at the ti executor time. and also since it is in a try/catch the exception is not visible. 
https://github.com/apache/airflow/blob/b263dbcb0f84fd9029591d1447a7c843cb970f15/airflow/executors/celery_executor.py#L394-L412


### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/cli/commands/task_command.py', 'a/airflow/executors/kubernetes_executor.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: Dynamically mapped sensors throw TypeError at DAG parse time
### Apache Airflow version

main (development)

### What happened

Here's a DAG:
```python3
from datetime import datetime
from airflow import DAG
from airflow.sensors.date_time import DateTimeSensor

template = "{{{{ ti.start_date + macros.timedelta(seconds={}) }}}}"

with DAG(
    dag_id="datetime_mapped",
    start_date=datetime(1970, 1, 1),
) as dag:

    @dag.task
    def get_sleeps():
        return [30, 60, 90]

    @dag.task
    def dt_templates(sleeps):
        return [template.format(s) for s in sleeps]

    templates_xcomarg = dt_templates(get_sleeps())

    DateTimeSensor.partial(task_id="sleep", mode="reschedule").apply(
        target_time=templates_xcomarg
    )
```

I wanted to see if it would parse, so I ran:

```
$ python dags/the_dag.py
```

And I got this error:
```
Traceback (most recent call last):
  File "/Users/matt/2022/02/22/dags/datetime_mapped.py", line 23, in <module>
    DateTimeSensor.partial(task_id="sleep", mode="reschedule").apply(
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 203, in apply
    deps=MappedOperator.deps_for(self.operator_class),
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 287, in deps_for
    return operator_class.deps | {MappedTaskIsExpanded()}
TypeError: unsupported operand type(s) for |: 'property' and 'set'
Exception ignored in: <function OperatorPartial.__del__ at 0x10ed90160>
Traceback (most recent call last):
  File "/Users/matt/src/airflow/airflow/models/mappedoperator.py", line 182, in __del__
    warnings.warn(f"{self!r} was never mapped!")
  File "/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/warnings.py", line 109, in _showwarnmsg
    sw(msg.message, msg.category, msg.filename, msg.lineno,
  File "/Users/matt/src/airflow/airflow/settings.py", line 115, in custom_show_warning
    from rich.markup import escape
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 925, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1414, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1380, in _get_spec
TypeError: 'NoneType' object is not iterable
```

### What you expected to happen

No errors.  Instead a dag with three parallel sensors.

### How to reproduce

Try to use the DAG shown above.

### Operating System

Mac OS Bug Sur

### Versions of Apache Airflow Providers

N/A

### Deployment

Virtualenv installation

### Deployment details

version: 2.3.0.dev0
cloned at: [8ee8f2b34](https://github.com/apache/airflow/commit/8ee8f2b34b8df168a4d3f2664a9f418469079723)


### Anything else

A comment from @ashb about this
> We're assuming we can call deps on the class. Which we can for everything but a sensor.



### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/mappedoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py']
Ground Truth : ['a/airflow/exceptions.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/sensors/base.py', 'a/airflow/ti_deps/deps/ready_to_reschedule.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: [AIRFLOW-1067] rename airflow.com to example.com in examples
Dear Airflow maintainers,

Please accept this PR. I understand that it will not be reviewed until I have checked off all the steps below!


### JIRA
- [x] My PR addresses the following [Airflow JIRA](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, "[AIRFLOW-XXX] My Airflow PR"
    - https://issues.apache.org/jira/browse/AIRFLOW-1067


### Description
- [x] Here are some details about my PR, including screenshots of any UI changes:

We use `airflow@airflow.com` in examples. However, https://airflow.com is owned by a company named Airflow (selling fans, etc). We should use `airflow@example.com` instead. That domain is created for this purpose.

### Tests
- [x] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:

Only text changes. I did `git grep airflow.com` after this change.

### Commits
- [x] My commits all reference JIRA issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)":
    1. Subject is separated from body by a blank line
    2. Subject is limited to 50 characters
    3. Subject does not end with a period
    4. Subject uses the imperative mood ("add", not "adding")
    5. Body wraps at 72 characters
    6. Body explains "what" and "why", not "how"



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/minor_release_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py']
Ground Truth : ['a/airflow/example_dags/tutorial.py', 'a/airflow/contrib/example_dags/example_twitter_dag.py', 'a/airflow/hooks/presto_hook.py', 'a/airflow/contrib/example_dags/example_emr_job_flow_manual_steps.py', 'a/airflow/api/auth/backend/default.py', 'a/airflow/operators/latest_only_operator.py', 'a/airflow/www/views.py', 'a/airflow/example_dags/example_docker_operator.py', 'a/airflow/operators/python_operator.py', 'a/airflow/ti_deps/deps/trigger_rule_dep.py', 'a/airflow/contrib/task_runner/__init__.py', 'a/airflow/example_dags/docker_copy_data.py', 'a/airflow/dag/__init__.py', 'a/airflow/example_dags/example_http_operator.py', 'a/airflow/contrib/example_dags/example_qubole_operator.py', 'a/airflow/contrib/example_dags/example_emr_job_flow_automatic_steps.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: Spark driver relaunches if "driverState" is not found in curl response due to transient network issue 
### Apache Airflow Provider(s)

apache-spark

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-spark==2.0.1

### Apache Airflow version

2.1.4

### Operating System

Amazon Linux 2

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

In the file `airflow/providers/apache/spark/hooks/spark_submit.py`, the function [_process_spark_status_log](https://github.com/apache/airflow/blob/main/airflow/providers/apache/spark/hooks/spark_submit.py#L516-L535) iterates through a `curl` response to get the driver state of a `SparkSubmitOperator` task.

If there's a transient network issue and there is no valid response from the cluster (e.g. timeout, etc.), there is no "driverState" in the `curl` response, which makes the driver state "UNKNOWN".

That state [exits the loop](https://github.com/apache/airflow/blob/main/airflow/providers/apache/spark/hooks/spark_submit.py#L573) and then makes the task to go on a [retry](https://github.com/apache/airflow/blob/main/airflow/providers/apache/spark/hooks/spark_submit.py#L464-L467), while the original task is actually still in a "RUNNING" state.

### What you expected to happen

I would expect the task not to go on a retry while the original task is running. The function `_process_spark_status_log` should probably ensure the `curl` response is valid before changing the driver state, e.g. check that there is a "submissionId"  in the response as well, otherwise leave the state to `None` and continue with the polling loop. A valid response would be something like this:
```
curl http://spark-host:6066/v1/submissions/status/driver-FOO-BAR

{
  "action" : "SubmissionStatusResponse",
  "driverState" : "RUNNING",
  "serverSparkVersion" : "2.4.6",
  "submissionId" : "driver-FOO-BAR,
  "success" : true,
  "workerHostPort" : "FOO:BAR",
  "workerId" : "worker-FOO-BAR-BAZ"
}
```
### How to reproduce

Use any DAG with a `SparkSubmitOperator` task on a Spark Standalone cluster where you can reset the network connection, or modify the `curl` command to return something other than the response above.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/operators/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/sensors/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/operators/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/sensors/test_spark_kubernetes.py']
Ground Truth : ['a/airflow/providers/apache/spark/hooks/spark_submit.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: 2.1.3/4 queued dag runs changes catchup=False behaviour
### Apache Airflow version

2.1.4 (latest released)

### Operating System

Amazon Linux 2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

Say, for example, you have a DAG that has a sensor. This DAG is set to run every minute, with max_active_runs=1, and catchup=False.

This sensor may pass 1 or more times per day.

Previously, when this sensor is satisfied once per day, there is 1 DAG run for that given day. When the sensor is satisfied twice per day, there are 2 DAG runs for that given day.

With the new queued dag run state, new dag runs will be queued for each minute (up to AIRFLOW__CORE__MAX_QUEUED_RUNS_PER_DAG), this seems to be against the spirit of catchup=False.

This means that if a dag run is waiting on a sensor for longer than the schedule_interval, it will still in effect 'catchup' due to the queued dag runs.

### What you expected to happen

_No response_

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py']
Ground Truth : ['a/airflow/timetables/interval.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: Airflow >= 2.2.0 execution date change is failing TaskInstance get_task_instances method and possibly others
### Apache Airflow version

2.2.3 (latest released)

### What happened

This is my first time reporting or posting on this forum. Please let me know if I need to provide any more information. Thanks for looking at this!

I have a Python Operator that uses the BaseOperator get_task_instances method and during the execution of this method, I encounter the following error:
<img width="1069" alt="Screen Shot 2022-02-17 at 2 28 48 PM" src="https://user-images.githubusercontent.com/18559784/154581673-718bc199-8390-49cf-a3fe-8972b6f39f81.png">

This error is from doing an upgrade from airflow 1.10.15 -> 2.2.3. 

I am using SQLAlchemy version 1.2.24 but I also tried with version 1.2.23 and encountered the same error. However, I do not think this is a sqlAlchemy issue. 

The issue seems to have been introduced with Airflow 2.2.0 (pr: https://github.com/apache/airflow/pull/17719/files), where the TaskInstance.execution_date changed from being a column to this association_proxy. I do not have deep knowledge of SQLAlchemny so I am not sure why this change was made, but it results in it the error I'm getting. 

2.2 .0 +
<img width="536" alt="Screen Shot 2022-02-17 at 2 41 00 PM" src="https://user-images.githubusercontent.com/18559784/154583252-4729b44d-40e2-4a89-9018-95b09ef4da76.png">

1.10.15
<img width="428" alt="Screen Shot 2022-02-17 at 2 56 15 PM" src="https://user-images.githubusercontent.com/18559784/154585325-4546309c-66b6-4e69-aba2-9b6979762a1b.png">



if you follow the stack trace you will get to this chunk of code that leads to the error because the association_proxy has a '__clause_element__' attr, but the attr raises the exception in the error when called.

<img width="465" alt="Screen Shot 2022-02-17 at 2 43 51 PM" src="https://user-images.githubusercontent.com/18559784/154583639-a7957209-b19e-4134-a5c2-88d53176709c.png">

### What you expected to happen

_No response_

### How to reproduce

_No response_

### Operating System

Linux from the official airflow helm chart docker image python version 3.7

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon          2.4.0
apache-airflow-providers-celery          2.1.0
apache-airflow-providers-cncf-kubernetes 2.2.0
apache-airflow-providers-databricks      2.2.0
apache-airflow-providers-docker          2.3.0
apache-airflow-providers-elasticsearch   2.1.0
apache-airflow-providers-ftp             2.0.1
apache-airflow-providers-google          6.2.0
apache-airflow-providers-grpc            2.0.1
apache-airflow-providers-hashicorp       2.1.1
apache-airflow-providers-http            2.0.1
apache-airflow-providers-imap            2.0.1
apache-airflow-providers-microsoft-azure 3.4.0
apache-airflow-providers-mysql           2.1.1
apache-airflow-providers-odbc            2.0.1
apache-airflow-providers-postgres        2.4.0
apache-airflow-providers-redis           2.0.1
apache-airflow-providers-sendgrid        2.0.1
apache-airflow-providers-sftp            2.3.0
apache-airflow-providers-slack           4.1.0
apache-airflow-providers-sqlite          2.0.1
apache-airflow-providers-ssh             2.3.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

The only extra dependency I am using is awscli==1.20.65. I have changed very little with the deployment besides a few environments variables and some pod annotations.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py']
Ground Truth : ['a/airflow/models/baseoperator.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: BUG: AirflowException doesn't raise exception in SequentialExecutor or LocalExecutor
Many bugs I've addressed recently share a symptom -- they all log an error that "The airflow run command failed at reporting an error. This should not occur in normal circumstances." (from https://github.com/airbnb/airflow/blob/master/airflow/jobs.py#L808)

Trying to reproduce. These are my notes...

Sequence of events:
1. Use SequentialExecutor, as with `airflow test ...`
2. The SequentialExecutor runs the task airflow command with (https://github.com/airbnb/airflow/blob/master/airflow/executors/sequential_executor.py):
   
   ``` python
   try:
       sp = subprocess.Popen(command, shell=True)
       sp.wait()
   except Exception as e:
       self.change_state(key, State.FAILED)
       raise e
   self.change_state(key, State.SUCCESS)
   ```
3. A task raises `AirflowException` **prior** to being executing
   - For example, the "dag not found" error raised by #1168 (prior to fix in #1196) due to SubDag errors
4. For some reason, the `AirflowException` is NOT caught by the executor's trap
   - **pretty sure this is the problem, still exploring**
   - don't we need to check `sp.returncode` rather than trapping errors?
5. The state gets changed to `success`
6. The task's state doesn't change
7. The "normal circumstances" error gets raised


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py']
Ground Truth : ['a/airflow/jobs.py', 'a/airflow/executors/sequential_executor.py', 'a/airflow/executors/local_executor.py']
Current Recall: 0.08073060059141432

=========================================================

ISSUE: celery_executor becomes stuck if child process receives signal before reset_signals is called
**Apache Airflow version**: 1.10.13 onwards (Any version that picked up #11278, including Airflow 2.0.* and 2.1.*)


**Environment**:
- **Cloud provider or hardware configuration**: Any
- **OS** (e.g. from /etc/os-release): Only tested on Debian Linux, but others may be affected too
- **Kernel** (e.g. `uname -a`): Any
- **Install tools**: Any
- **Others**: Only celery_executor is affected

**What happened**:
This was first reported [here](https://github.com/apache/airflow/issues/7935#issuecomment-839656436).
airflow-scheduler sometimes stops heartbeating and stops scheduling any tasks with this last line in the log. This happen at random times, a few times a week. Happens more often if the scheduler machine is slow. 

```
{scheduler_job.py:746} INFO - Exiting gracefully upon receiving signal 15
```

The problem is that sometimes the machine is slow, `reset_signals()` of one or more slow child processes is not yet called before other child processes send `SIGTERM` when they exit. As a result, the slow child processes respond to the `SIGTERM` as if they are the main scheduler process. Thus we see the `Exiting gracefully upon receiving signal 15` in the scheduler log. Since the probability of this happening is very low, this issue is really difficult to reproduce reliably in production.

Related to #7935
Most likely caused by #11278

**What you expected to happen**: 
Scheduler should not become stuck

**How to reproduce it**:

Here's a small reproducing example of the problem. There's roughly 1/25 chance it will be stuck. Run it many times to see it happen.

```python
#!/usr/bin/env python3.8
import os
import random
import signal
import time
from multiprocessing import Pool


def send_task_to_executor(arg):
    pass


def _exit_gracefully(signum, frame):
    print(f"{os.getpid()} Exiting gracefully upon receiving signal {signum}")


def register_signals():
    print(f"{os.getpid()} register_signals()")
    signal.signal(signal.SIGINT, _exit_gracefully)
    signal.signal(signal.SIGTERM, _exit_gracefully)
    signal.signal(signal.SIGUSR2, _exit_gracefully)


def reset_signals():
    if random.randint(0, 500) == 0:
        # This sleep statement here simulates the machine being busy
        print(f"{os.getpid()} is slow")
        time.sleep(0.1)
    signal.signal(signal.SIGINT, signal.SIG_DFL)
    signal.signal(signal.SIGTERM, signal.SIG_DFL)
    signal.signal(signal.SIGUSR2, signal.SIG_DFL)


if __name__ == "__main__":
    register_signals()

    task_tuples_to_send = list(range(20))
    sync_parallelism = 15
    chunksize = 5

    with Pool(processes=sync_parallelism, initializer=reset_signals) as pool:
        pool.map(
            send_task_to_executor,
            task_tuples_to_send,
            chunksize=chunksize,
        )


```

The reproducing example above can become stuck with a `py-spy dump` that looks exactly like what airflow scheduler does:

`py-spy dump` for the parent `airflow scheduler` process
```
Python v3.8.7

Thread 0x7FB54794E740 (active): "MainThread"
    poll (multiprocessing/popen_fork.py:27)
    wait (multiprocessing/popen_fork.py:47)
    join (multiprocessing/process.py:149)
    _terminate_pool (multiprocessing/pool.py:729)
    __call__ (multiprocessing/util.py:224)
    terminate (multiprocessing/pool.py:654)
    __exit__ (multiprocessing/pool.py:736)
    _send_tasks_to_celery (airflow/executors/celery_executor.py:331)
    _process_tasks (airflow/executors/celery_executor.py:272)
    trigger_tasks (airflow/executors/celery_executor.py:263)
    heartbeat (airflow/executors/base_executor.py:158)
    _run_scheduler_loop (airflow/jobs/scheduler_job.py:1388)
    _execute (airflow/jobs/scheduler_job.py:1284)
    run (airflow/jobs/base_job.py:237)
    scheduler (airflow/cli/commands/scheduler_command.py:63)
    wrapper (airflow/utils/cli.py:89)
    command (airflow/cli/cli_parser.py:48)
    main (airflow/__main__.py:40)
    <module> (airflow:8)
```

`py-spy dump` for the child `airflow scheduler` process

```
Python v3.8.7

Thread 16232 (idle): "MainThread"
    __enter__ (multiprocessing/synchronize.py:95)
    get (multiprocessing/queues.py:355)
    worker (multiprocessing/pool.py:114)
    run (multiprocessing/process.py:108)
    _bootstrap (multiprocessing/process.py:315)
    _launch (multiprocessing/popen_fork.py:75)
    __init__ (multiprocessing/popen_fork.py:19)
    _Popen (multiprocessing/context.py:277)
    start (multiprocessing/process.py:121)
    _repopulate_pool_static (multiprocessing/pool.py:326)
    _repopulate_pool (multiprocessing/pool.py:303)
    __init__ (multiprocessing/pool.py:212)
    Pool (multiprocessing/context.py:119)
    _send_tasks_to_celery (airflow/executors/celery_executor.py:330)
    _process_tasks (airflow/executors/celery_executor.py:272)
    trigger_tasks (airflow/executors/celery_executor.py:263)
    heartbeat (airflow/executors/base_executor.py:158)
    _run_scheduler_loop (airflow/jobs/scheduler_job.py:1388)
    _execute (airflow/jobs/scheduler_job.py:1284)
    run (airflow/jobs/base_job.py:237)
    scheduler (airflow/cli/commands/scheduler_command.py:63)
    wrapper (airflow/utils/cli.py:89)
    command (airflow/cli/cli_parser.py:48)
    main (airflow/__main__.py:40)
    <module> (airflow:8)
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/celery/executors/test_celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/local_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/standard_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/process_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_process_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py']
Ground Truth : ['a/airflow/executors/celery_executor.py', 'a/airflow/jobs/scheduler_job.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Task modal links are broken in the dag gantt view
### Apache Airflow version

2.2.0 (latest released)

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

N/A

### Deployment

Other Docker-based deployment

### Deployment details

CeleryExecutor / ECS / Postgres

### What happened

![image](https://user-images.githubusercontent.com/160865/139075540-25fca98f-a858-49ce-8f3f-1b9a145a6853.png)

Clicking on logs / instance details on the following dialog causes an exception:

```
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "/usr/local/lib/python3.9/site-packages/flask/_compat.py", line 39, in reraise
    raise value
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.9/site-packages/flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "/usr/local/lib/python3.9/site-packages/airflow/www/auth.py", line 51, in decorated
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/www/decorators.py", line 63, in wrapper
    log.execution_date = pendulum.parse(execution_date_value, strict=False)
  File "/usr/local/lib/python3.9/site-packages/pendulum/parser.py", line 29, in parse
    return _parse(text, **options)
  File "/usr/local/lib/python3.9/site-packages/pendulum/parser.py", line 45, in _parse
    parsed = base_parse(text, **options)
  File "/usr/local/lib/python3.9/site-packages/pendulum/parsing/__init__.py", line 74, in parse
    return _normalize(_parse(text, **_options), **_options)
  File "/usr/local/lib/python3.9/site-packages/pendulum/parsing/__init__.py", line 120, in _parse
    return _parse_common(text, **options)
  File "/usr/local/lib/python3.9/site-packages/pendulum/parsing/__init__.py", line 177, in _parse_common
    return date(year, month, day)
ValueError: year 0 is out of range
```
This is because the execution_date in the query param of the url is empty i.e:

`http://localhost:50008/log?dag_id=test_logging&task_id=check_exception_to_sentry&execution_date=`

### What you expected to happen

The logs to load / task instance detail page to load

### How to reproduce

See above

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_emr_create_job_flow.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: [Google Cloud] DataprocCreateBatchOperator returns incorrect results and does not reattach
### Apache Airflow version

main (development)

### What happened

The provider operator for Google Cloud Dataproc Batches has two bugs: 

1. The running [operator](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/dataproc.py#L2123-L2124) returns successful even if the job transitions to State.CANCELLED or State.CANCELLING 
2. It [attempts](https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/dataproc.py#L2154) to 'reattach' to a potentially running job if it AlreadyExists, but it sends the wrong type since 'result' is a Batch and needs Operation

### What you think should happen instead

A new hook that polls for batch job completion.  There is precedent for it in traditional dataproc with 'wait_for_job'.

### How to reproduce

Use the Breeze environment and a DAG that runs DataprocCreateBatchOperator.  Allow the first instance to start. 

Use the gcloud CLI to cancel the job. 

`gcloud dataproc batches cancel <batch_id> --project <project_id> --region <region>`

Observe that the task completes successfully after a 3-5 minute timeout, even though the job was cancelled. 

Run the task again with the same batch_id.  Observe the ValueError where it expects Operation but receives Batch



### Operating System

Darwin 5806 21.6.0 Darwin Kernel Version 21.6.0: Mon Aug 22 20:17:10 PDT 2022; root:xnu-8020.140.49~2/RELEASE_X86_64 x86_64

### Versions of Apache Airflow Providers

Same as dev (main) version. 

### Deployment

Other Docker-based deployment

### Deployment details

Observable in the Breeze environment, when running against real Google Infrastructure. 

### Anything else

Every time. 

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/triggers/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery_dts.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py']
Ground Truth : ['a/airflow/providers/google/cloud/hooks/dataproc.py', 'a/airflow/providers/google/cloud/operators/dataproc.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Automate docs for `values.yaml` via pre-commit config and break them into logical groups
Automate docs for values.yaml via pre-commit config and break them into logical groups like https://github.com/bitnami/charts/tree/master/bitnami/airflow

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_migration_reference.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/provider_init_hack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jdbc/hooks/jdbc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/hatch_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py']
Ground Truth : ['a/scripts/ci/pre_commit/pre_commit_json_schema.py', 'a/docs/conf.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Version Incompatibility in 1.10.14 resulting from snowflake workaround
**Apache Airflow version**: 1.10.14
**Environment**: Python 3.8
- **Install tools**: pipenv

**What happened**:

So I appear to have stumbled across a version incompatibility for `airflow-1.10.14`
apache-airflow[aws] was modified with this [pr](https://github.com/apache/airflow/commit/3f438461498b2f6c13671fed8f70a6a12a51f418#diff-60f61ab7a8d1910d86d9fda2261620314edcae5894d5aaa236b821c7256badd7R185)  to restrict boto to boto3~=1.10,<1.11 with the comment for snowflake
When attempting to install the latest amazon-provider backport apache-airflow-backport-providers-amazon==2020.11.23->-r Im faced with an incompatible version in resolved dependancies as the backport requires boto3<2.0.0,>=1.12.0

attempting to install `aws` extra and amazon-provide backport results in a version incompatibility error.

```
There are incompatible versions in the resolved dependencies:
  boto3<1.11,~=1.10 (from apache-airflow[aws]==1.10.14->-r /var/folders/dw/8s4dltks7bg5ws8kht1wfbqw0000gp/T/pipenvtfsk4ltbrequirements/pipenv-ukucfwxw-constraints.txt (line 2))
  boto3<2.0.0,>=1.12.0 (from apache-airflow-backport-providers-amazon==2020.11.23->-r /var/folders/dw/8s4dltks7bg5ws8kht1wfbqw0000gp/T/pipenvtfsk4ltbrequirements/pipenv-ukucfwxw-constraints.txt (line 3))
```

**What you expected to happen**:

Airflow has compatible versions.

**How to reproduce it**:

<details><summary> 1. Create Pipfile with the following contents</summary> 

```
[[source]]
name = "pypi"
url = "https://pypi.org/simple"
verify_ssl = true

[dev-packages]

[packages]
apache-airflow = {extras = ["aws"],version = "*"}
# https://github.com/apache/airflow/blob/master/README.md#backport-packages
apache-airflow-backport-providers-amazon = "*"

[requires]
python_version = "3.8"
```
</details>
2. run `pipenv lock`




**Anything else we need to know**:

Suggested solution: move all snowflake version modifications out of related airflow extras and directly into the snowflake extra.
Slack Thread: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1608057981159400


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: [AIP-31] Create PythonFunctionalOperator
**Description**

Operator that enables wrapping a python function and call it as a function in the DAG.

- Should have` __call__` method that captures `args` and `kwargs` and sets them as `op_args` and `op_kwargs`.

**Use case / motivation**

Base class needed to implement functions wrapped in operators. See `Python function decorator` in [AIP-31](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-31:+Airflow+functional+DAG+definition) 

Example implementation: https://github.com/casassg/corrent/blob/master/corrent/operators.py

**Stretch goal**
- Add `copy(new_task_id:str)` method that allows you to create a new PythonFunctionOperator from the previous operator and changing the task_id. This should make it easier to reuse functional operators in a DAG without affecting the constraint of having a unique ID. 
- Another option is to append a number in the function if no `new_task_id` is set by looking in the DAG object for other operators with the same task_id.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/decorators/sensors/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/policies.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/decorators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/decorators/task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py']
Ground Truth : ['/dev/null', 'a/airflow/operators/python.py', 'a/airflow/models/dag.py', 'a/airflow/ti_deps/deps/trigger_rule_dep.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Upgrading to airflow 2.4.0 from 2.3.4 causes NotNullViolation error
### Apache Airflow version

2.4.0

### What happened

Stopped existing processes, upgraded from airflow 2.3.4 to 2.4.0, and ran airflow db upgrade successfully. Upon restarting the services, I'm not seeing any dag runs from the past 10 days. I kick off a new job, and I don't see it show up in the grid view. Upon checking the systemd logs, I see that there are a lot of postgress errors with webserver. Below is a sample of such errors.

```
[SQL: INSERT INTO ab_view_menu (name) VALUES (%(name)s) RETURNING ab_view_menu.id]
[parameters: {'name': 'Datasets'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2022-09-19 14:03:16,183] {manager.py:511} ERROR - Creation of Permission View Error: (psycopg2.errors.NotNullViolation) null value in column "id" violates not-null constraint
DETAIL:  Failing row contains (null, 13, null).

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 13, 'view_menu_id': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2022-09-19 14:03:16,209] {manager.py:420} ERROR - Add View Menu Error: (psycopg2.errors.NotNullViolation) null value in column "id" violates not-null constraint
DETAIL:  Failing row contains (null, Datasets).

[SQL: INSERT INTO ab_view_menu (name) VALUES (%(name)s) RETURNING ab_view_menu.id]
[parameters: {'name': 'Datasets'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2022-09-19 14:03:16,212] {manager.py:511} ERROR - Creation of Permission View Error: (psycopg2.errors.NotNullViolation) null value in column "id" violates not-null constraint
DETAIL:  Failing row contains (null, 17, null).

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 17, 'view_menu_id': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2022-09-19 14:03:16,229] {manager.py:420} ERROR - Add View Menu Error: (psycopg2.errors.NotNullViolation) null value in column "id" violates not-null constraint
DETAIL:  Failing row contains (null, DAG Warnings).

[SQL: INSERT INTO ab_view_menu (name) VALUES (%(name)s) RETURNING ab_view_menu.id]
[parameters: {'name': 'DAG Warnings'}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2022-09-19 14:03:16,232] {manager.py:511} ERROR - Creation of Permission View Error: (psycopg2.errors.NotNullViolation) null value in column "id" violates not-null constraint
DETAIL:  Failing row contains (null, 17, null).

[SQL: INSERT INTO ab_permission_view (permission_id, view_menu_id) VALUES (%(permission_id)s, %(view_menu_id)s) RETURNING ab_permission_view.id]
[parameters: {'permission_id': 17, 'view_menu_id': None}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2022-09-19 14:03:16,250] {manager.py:511} ERROR - Creation of Permission View Error: (psycopg2.errors.NotNullViolation) null value in column "id" violates not-null constraint
DETAIL:  Failing row contains (null, 13, 23).
```

I tried running airflow db check, init, check-migration, upgrade without any errors, but the errors still remain. 

Please let me know if I missed any steps during the upgrade, or if this is a known issue with a workaround.

### What you think should happen instead

All dag runs should be visible 

### How to reproduce

upgrade airflow, upgrade db, restart the services

### Operating System

Ubuntu 18.04.6 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/oracle/hooks/oracle.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0057_1_10_13_add_fab_tables.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/flink/sensors/test_flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/datasets/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/common/sql/operators/test_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0093_2_2_0_taskinstance_keyed_to_dagrun.py']
Ground Truth : ['a/airflow/migrations/env.py', 'a/airflow/utils/db.py', '/dev/null', 'a/airflow/settings.py', 'a/airflow/migrations/versions/0118_2_5_0_add_updated_at_to_dagrun_and_ti.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Metrics documentation fixes and deprecations
**Apache Airflow version**: 2.0.2

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): N/A

**Environment**: N/A

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:
* `dag_processing.last_runtime.*` - In version 1.10.6 [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md#airflow-1106) it was indicated that this metrics will be removed in 2.0. It was not removed from the metrics documentation. Also the metrics documentation doesn't mention it supposed to be removed/deprecated, it's documented as a gauge but it is actually a timer (reported https://github.com/apache/airflow/issues/10091). 
* `dag_processing.processor_timeouts`: documented as a guage but it is actually a counter. Again from https://github.com/apache/airflow/issues/10091. 
* `dag_file_processor_timeouts` - indicated as supposed to be removed in 2.0, was not removed from [code](https://github.com/apache/airflow/blob/37d549/airflow/utils/dag_processing.py#L1169) but removed from docs. 
* Would be nice if documentation of 1.10.15 indicated the deprecated metrics more clearly, not only in `UPDATING.md`.

**What you expected to happen**:

* The Metrics page should document all metrics being emitted by Airflow.
* The Metrics page should correctly document the type of the metric.

**How to reproduce it**:
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->

Check official [Metrics Docs](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/metrics.html?highlight=metrics#)

**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/otel_logger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py']
Ground Truth : ['a/airflow/dag_processing/manager.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: from airflow.operators.python import PythonOperator does not work
This is not necessarily a bug in core Airflow, but the upgrade-check scripts recommend this as a solution when the old 1.10.x version of importing the python operator is used. 

So, there is a mismatch between the core Airflow code and the recommendations given in the upgrade check. 

<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**:


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release):
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:

<!-- (please include exact error messages if you can) -->

**What you expected to happen**:

<!-- What do you think went wrong? -->

**How to reproduce it**:
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py']
Ground Truth : ['a/airflow/upgrade/rules/import_changes.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: DAG's parameter access_control is not refreshing in the UI
**Apache Airflow version**: 1.10.9

**Environment**:

- **Cloud provider or hardware configuration**: N/A
- **OS** (e.g. from /etc/os-release): Debian GNU/Linux 9 (stretch)
- **Kernel** (e.g. `uname -a`): Linux 4cc8ac3c2cfb 5.4.0-26-generic #30-Ubuntu SMP Mon Apr 20 16:58:30 UTC 2020 x86_64 GNU/Linux
- **Install tools**: N/A

**What happened**:

When I update DAG's parameter `access_control` the change is not updated in the UI at `roles/list/`. I have to trigger refresh DAG manually in the UI to get the change. (I tried it many times and waited 10+ minutes.)

**What you expected to happen**:

I assume the change change should be updated automatically, as for any other DAG's parameter.

**How to reproduce it**:

Create new DAG for example with parameter: `access_control={'Public': ['can_dag_read']}`.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py']
Ground Truth : ['a/airflow/www/security.py', 'a/airflow/models/dagbag.py', 'a/airflow/cli/cli_parser.py', 'a/airflow/models/serialized_dag.py', 'a/airflow/cli/commands/sync_perm_command.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Add Variable, Connection "description" fields available in the API
### Description

I'd like to get the "description" field from the variable, and connection table available through the REST API for the calls:

1. /variables/{key}
2. /connections/{conn_id}

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_variable_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py']
Ground Truth : ['a/airflow/api_connexion/schemas/variable_schema.py', 'a/airflow/api_connexion/endpoints/variable_endpoint.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Import connections/variables from a file
Hello,

We have a new local file system secret backend that allows us to load connections and variables from files. We support 3 file formats: YAML, JSON, ENV.
https://airflow.readthedocs.io/en/latest/howto/use-alternative-secrets-backend.html#local-filesystem-secrets-backend
https://github.com/apache/airflow/blob/master/airflow/secrets/local_filesystem.py
I think we can use the same code to create new commands for the CLI that allow us o import connections or variables. 

Best regards,
Kamil

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/secrets/local_filesystem.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/secrets/lockbox.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/secrets/key_vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/systems_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_variable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker_swarm.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/data_lake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_secrets.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/secrets/secret_manager.py']
Ground Truth : ['a/airflow/cli/commands/connection_command.py', 'a/airflow/models/connection.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Airflow v2.0.0b1 package doesnt include "api_connexion/exceptions"
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 2.0.0b1


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): NA

**Environment**:

- **Cloud provider or hardware configuration**:
- **OS** (e.g. from /etc/os-release): macOS
- **Kernel** (e.g. `uname -a`):
- **Install tools**:
- **Others**:

**What happened**:
Installed apache-airlfow==2.0.0b1 using pip. 
Running `airflow webserver` gave the following error:
```
Traceback (most recent call last):
  File "/Users/abagri/Workspace/service-workflows/venv/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/Users/abagri/Workspace/service-workflows/venv/lib/python3.8/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/Users/abagri/Workspace/service-workflows/venv/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 49, in command
    func = import_string(import_path)
  File "/Users/abagri/Workspace/service-workflows/venv/lib/python3.8/site-packages/airflow/utils/module_loading.py", line 32, in import_string
    module = import_module(module_path)
  File "/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1014, in _gcd_import
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 783, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/Users/abagri/Workspace/service-workflows/venv/lib/python3.8/site-packages/airflow/cli/commands/webserver_command.py", line 43, in <module>
    from airflow.www.app import cached_app, create_app
  File "/Users/abagri/Workspace/service-workflows/venv/lib/python3.8/site-packages/airflow/www/app.py", line 39, in <module>
    from airflow.www.extensions.init_views import (
  File "/Users/abagri/Workspace/service-workflows/venv/lib/python3.8/site-packages/airflow/www/extensions/init_views.py", line 25, in <module>
    from airflow.api_connexion.exceptions import common_error_handler
ModuleNotFoundError: No module named 'airflow.api_connexion.exceptions'
```
<!-- (please include exact error messages if you can) -->

**What you expected to happen**:
Expect this command to start the webserver
<!-- What do you think went wrong? -->

**How to reproduce it**:
Install a fresh version of airflow, run `airflow db init` followed by `airflow webserver`
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py']
Ground Truth : ['a/docs/conf.py', '/dev/null']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: PostgresToGoogleCloudStorageOperator - Custom schema mapping
Version : 1.10.12

I used PostgresToGoogleCloudStorageOperator to export the data and the schema file as well. But I saw a column on Postgres was `TIMESTAMP without time zone` but in BigQuery the auto-create table (via `GoogleCloudStorageToBigQueryOperator`) used the JSON schema file and created the table. When I checked the BQ table the data type was `TIMESTAMP`.

For without timezone data, **`DATETIME`** would be the right choice. So can we manually MAP the data types during the schema file export? 

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/cassandra_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/sql_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/oracle/hooks/oracle.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/mssql_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/exasol/hooks/exasol.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/postgres_to_gcs.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: KubernetesExecutor leaves failed pods due to deepcopy issue with Google providers
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

With Airflow 2.3 and 2.4 there appears to be a bug in the KubernetesExecutor when used in conjunction with the Google airflow providers. This bug does not affect Airflow 2.2 due to the pip version requirements.

The bug specifically presents itself when using nearly any Google provider operator. During the pod lifecycle, all is well until the executor in the pod starts to clean up following a successful run. Airflow itself still see's the task marked as a success, but in Kubernetes, while the task is finishing up after reporting status, it actually crashes and puts the pod into a Failed state silently:
```
Traceback (most recent call last):
  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 39, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 52, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 103, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 382, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 189, in _run_task_by_selected_method
    _run_task_by_local_task_job(args, ti)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 247, in _run_task_by_local_task_job
    run_job.run()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/base_job.py", line 247, in run
    self._execute()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/local_task_job.py", line 137, in _execute
    self.handle_task_exit(return_code)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/local_task_job.py", line 168, in handle_task_exit
    self._run_mini_scheduler_on_child_tasks()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/local_task_job.py", line 253, in _run_mini_scheduler_on_child_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2188, in partial_subset
    dag.task_dict = {
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2189, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py", line 2186, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/local/lib/python3.9/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 1163, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/local/lib/python3.9/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/local/lib/python3.9/copy.py", line 264, in _reconstruct
    y = func(*args)
  File "/usr/local/lib/python3.9/enum.py", line 384, in __call__
    return cls.__new__(cls, value)
  File "/usr/local/lib/python3.9/enum.py", line 702, in __new__
    raise ve_exc
ValueError: <object object at 0x7f570181a3c0> is not a valid _MethodDefault
```

Based on a quick look, it appears to be related to the default argument that Google is using in its operators which happens to be an Enum, and fails during a deepcopy at the end of the task.

Example operator that is affected: https://github.com/apache/airflow/blob/403ed7163f3431deb7fc21108e1743385e139907/airflow/providers/google/cloud/hooks/dataproc.py#L753
Reference to the Google Python API core which has the Enum causing the problem: https://github.com/googleapis/python-api-core/blob/main/google/api_core/gapic_v1/method.py#L31

### What you think should happen instead

Kubernetes pods should succeed, be marked as `Completed`, and then be gracefully terminated.

### How to reproduce

Use any `apache-airflow-providers-google` >= 7.0.0 which includes `google-api-core` >= 2.2.2. Run a DAG with a task which uses any of the Google operators which have `_MethodDefault` as a default argument.

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==6.0.0
apache-airflow-providers-apache-hive==5.0.0
apache-airflow-providers-celery==3.0.0
apache-airflow-providers-cncf-kubernetes==4.4.0
apache-airflow-providers-common-sql==1.3.1
apache-airflow-providers-docker==3.2.0
apache-airflow-providers-elasticsearch==4.2.1
apache-airflow-providers-ftp==3.1.0
apache-airflow-providers-google==8.4.0
apache-airflow-providers-grpc==3.0.0
apache-airflow-providers-hashicorp==3.1.0
apache-airflow-providers-http==4.0.0
apache-airflow-providers-imap==3.0.0
apache-airflow-providers-microsoft-azure==4.3.0
apache-airflow-providers-mysql==3.2.1
apache-airflow-providers-odbc==3.1.2
apache-airflow-providers-postgres==5.2.2
apache-airflow-providers-presto==4.2.0
apache-airflow-providers-redis==3.0.0
apache-airflow-providers-sendgrid==3.0.0
apache-airflow-providers-sftp==4.1.0
apache-airflow-providers-slack==6.0.0
apache-airflow-providers-sqlite==3.2.1
apache-airflow-providers-ssh==3.2.0

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/check_files.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py']
Ground Truth : ['a/airflow/providers/google/cloud/operators/cloud_base.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: UI is broken for `breeze kind-cluster deploy`
Using `breeze kind-cluster deploy` to deploy airflow in Kubernetes cluster for development results in unusable UI


**Apache Airflow version**: main

**How to reproduce it**:
Start kind cluster with `./breeze kind-cluster start`
Deploy airflow with `./breeze kind-cluster deploy`

Check the UI and see that it's broken:
![airflowui](https://user-images.githubusercontent.com/4122866/125270717-da304100-e301-11eb-862d-0526ffe7fad2.PNG)



**Anything else we need to know**:

This is likely as a result of https://github.com/apache/airflow/pull/16577

[Smart sensor] Runtime error: dictionary changed size during iteration
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->


**What happened**:

<!-- (please include exact error messages if you can) -->
Smart Sensor TI crashes with a Runtime error. Here's the logs:
```
RuntimeError: dictionary changed size during iteration
  File "airflow/sentry.py", line 159, in wrapper
    return func(task_instance, *args, session=session, **kwargs)
  File "airflow/models/taskinstance.py", line 1112, in _run_raw_task
    self._prepare_and_execute_task_with_callbacks(context, task)
  File "airflow/models/taskinstance.py", line 1285, in _prepare_and_execute_task_with_callbacks
    result = self._execute_task(context, task_copy)
  File "airflow/models/taskinstance.py", line 1315, in _execute_task
    result = task_copy.execute(context=context)
  File "airflow/sensors/smart_sensor.py", line 736, in execute
    self.flush_cached_sensor_poke_results()
  File "airflow/sensors/smart_sensor.py", line 681, in flush_cached_sensor_poke_results
    for ti_key, sensor_exception in self.cached_sensor_exceptions.items():
```


**What you expected to happen**:

<!-- What do you think went wrong? -->
Smart sensor should always execute without any runtime error.

**How to reproduce it**:
I haven't been able to reproduce it consistently since it sometimes works and sometimes errors.

**Anything else we need to know**:
It's a really noisy error in Sentry. In just 4 days, 3.8k events were reported in Sentry.
<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->

Deleted DAG raises SerializedDagNotFound exception when accessing webserver
**Apache Airflow version**: 2.1.2


**Environment**:

- **Docker image**: official apache/airflow:2.1.2 extended with pip installed packages
- **Executor**: CeleryExecutor
- **Database**: PostgreSQL (engine version: 12.5)

**What happened**:

I've encountered `SerializedDagNotFound: DAG 'my_dag_id' not found in serialized_dag table` after deleting DAG via web UI. Exception is raised each time when UI is accessed. The exception itself does not impact UI, but still an event is sent to Sentry each time I interact with it.

**What you expected to happen**:

After DAG deletion I've expected that all records of it apart from logs would be deleted, but it's DAG run was still showing up in Webserver UI (even though, I couldn't find any records of DAG in the metadb, apart from dag_tag table still containing records related to deleted DAG (Which may be a reason for a new issue), but manual deletion of those records had no impact. 

After I've deleted DAG Run via Web UI, the exception is no longer raised.

**How to reproduce it**:

To this moment, I've only encountered this with one DAG, which was used for debugging purposes. It consists of multiple PythonOperators and it uses the same boilerplate code I use for dynamic DAG generation (return DAG object from `create_dag` function, add it to globals), but in it I've replaced dynamic generation logic with just `dag = create_dag(*my_args, **my_kwargs)`. I think this may have been caused by DAG run still running, when DAG was deleted, but cannot support this theory with actual information. 

<details>
  <summary>Traceback</summary> 

```python
SerializedDagNotFound: DAG 'my_dag_id' not found in serialized_dag table
  File "flask/app.py", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File "flask/app.py", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "flask/app.py", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "flask/_compat.py", line 39, in reraise
    raise value
  File "flask/app.py", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File "flask/app.py", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "airflow/www/auth.py", line 34, in decorated
    return func(*args, **kwargs)
  File "airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "airflow/www/views.py", line 1679, in blocked
    dag = current_app.dag_bag.get_dag(dag_id)
  File "airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "airflow/models/dagbag.py", line 186, in get_dag
    self._add_dag_from_db(dag_id=dag_id, session=session)
  File "airflow/models/dagbag.py", line 258, in _add_dag_from_db
    raise SerializedDagNotFound(f"DAG '{dag_id}' not found in serialized_dag table")
```
</details>


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py']
Ground Truth : ['a/airflow/utils/cli.py', 'a/airflow/models/taskinstance.py', 'a/airflow/configuration.py', 'a/airflow/models/dagbag.py', 'a/airflow/utils/db.py', '/dev/null', 'a/docs/conf.py', 'a/airflow/jobs/local_task_job.py', 'a/airflow/__init__.py', 'a/airflow/providers/google/cloud/hooks/cloud_memorystore.py', 'a/setup.py', 'a/airflow/providers/elasticsearch/log/es_task_handler.py', 'a/scripts/upstart/airflow-webserver.conf', 'a/airflow/www/views.py', 'a/scripts/tools/list-integrations.py', 'a/dev/provider_packages/prepare_provider_packages.py', 'a/dev/retag_docker_images.py', 'a/airflow/utils/log/log_reader.py', 'a/airflow/sensors/smart_sensor.py', 'a/airflow/utils/helpers.py', 'a/airflow/models/dag.py', 'a/airflow/utils/state.py', 'a/scripts/in_container/run_clear_tmp.sh', 'a/airflow/cli/commands/task_command.py', 'a/scripts/upstart/airflow-worker.conf', 'a/scripts/upstart/airflow-flower.conf', 'a/airflow/executors/local_executor.py', 'a/airflow/utils/log/secrets_masker.py', 'a/airflow/providers_manager.py', 'a/airflow/utils/log/file_task_handler.py', 'a/airflow/cli/commands/scheduler_command.py', 'a/airflow/utils/log/logging_mixin.py', 'a/airflow/models/dagrun.py', 'a/scripts/ci/pre_commit/pre_commit_check_provider_yaml_files.py', 'a/airflow/utils/task_group.py', 'a/airflow/typing_compat.py', 'a/airflow/api/client/json_client.py', 'a/airflow/models/baseoperator.py', 'a/airflow/cli/commands/kubernetes_command.py', 'a/airflow/hooks/base.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/jobs/scheduler_job.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Guide for Apache Druid operators 
**Description**

Hello,

A guide that describes how to use Apache Druid software operators would be useful.

Other guides are available:
https://airflow.readthedocs.io/en/latest/howto/operator/index.html

Are you wondering how to start contributing to this project? Start by reading our [contributor guide](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)

Best regards,
Kamil

**Use case / motivation**

N/A

**Related Issues**

Missing guides for Apache operators:
Apache Cassandra: https://github.com/apache/airflow/issues/8189
Apache Druid: https://github.com/apache/airflow/issues/8199
Apache Hive: https://github.com/apache/airflow/issues/8191
Apache Livy: https://github.com/apache/airflow/issues/8192
Apache Pig: https://github.com/apache/airflow/issues/8193
Apache Pinot: https://github.com/apache/airflow/issues/8194
Apache Spark: https://github.com/apache/airflow/issues/8195
Apache Sqoop: https://github.com/apache/airflow/issues/8196
Hadoop Distributed File System (HDFS): https://github.com/apache/airflow/issues/8197
WebHDFS: https://github.com/apache/airflow/issues/8198



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/non_caching_file_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/typing_compat.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_bulk_issues.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/cloud_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/calculate_statistics_provider_testing_issues.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/operators_and_hooks_ref.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/bigquery_dts.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/task_instance_session.py']
Ground Truth : ['/dev/null']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Fix POST /taskInstances/list with wildcards returns unhelpful error
This PR closes #26424
Problem Statement: The POST Request to the endpoint  `/api/v1/dags/~/dagRuns/~/taskInstances/list` returns an unhelpful error -> "detail": "None is not of type 'object'", when the JSON Body is empty. Upon further inspection, it seems that this error is raised by vaildation.py and is not handled properly leading to a generic error message and a Python Exception inside the logs.
Fix: Allowing the request payload to be nullable and then checking if the data in the request is None before parsing it into JSON -> allows us to handle this error gracefully and even check for other unhandled cases.

New Response: 

```json
{
    "detail": "['POST Body must not be None']",
    "status": 400,
    "title": "Bad Request",
    "type": "http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/stable-rest-api-ref.html#section/Errors/BadRequest"
}
```

Old Response:

```json
{
    "detail": "None is not of type 'object'",
    "status": 400,
    "title": "Bad Request",
    "type": "http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/stable-rest-api-ref.html#section/Errors/BadRequest"
}
```

Old Logs:

```
2023-03-28T17:16:17.637+0000 validation.py[200] None is not type object
```
`POST /taskInstances/list` with wildcards returns unhelpful error
### Apache Airflow version

2.3.4

### What happened

https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_task_instances_batch
fails with an error with wildcards while
https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_task_instances 
succeeds with wildcards

Error:
```
400
"None is not of type 'object'"
```


### What you think should happen instead

_No response_

### How to reproduce

1) `astro dev init`
2) `astro dev start`
3) `test1.py` and `python test1.py`
```
import requests
host = "http://localhost:8080/api/v1"
kwargs = {
    'auth': ('admin', 'admin'),
    'headers': {'content-type': 'application/json'}
}
r = requests.post(f'{host}/dags/~/dagRuns/~/taskInstances/list', **kwargs, timeout=10)
print(r.url, r.text)
```

output
```
http://localhost:8080/api/v1/dags/~/dagRuns/~/taskInstances/list 
{
  "detail": "None is not of type 'object'",
  "status": 400,
  "title": "Bad Request",
  "type": "http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/stable-rest-api-ref.html#section/Errors/BadRequest"
}
```

3) `test2.py` and `python test2.py`
```
import requests
host = "http://localhost:8080/api/v1"
kwargs = {
    'auth': ('admin', 'admin'),
    'headers': {'content-type': 'application/json'}
}
r = requests.get(f'{host}/dags/~/dagRuns/~/taskInstances', **kwargs, timeout=10)  # change here
print(r.url, r.text)
```
```
<correct output>
```

### Operating System

Debian

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_docker_compose_quick_start.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/http/hooks/test_http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py']
Ground Truth : ['a/airflow/www/extensions/init_views.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Add how-to Guide for Snowflake operators
**Description**

A guide that describes how to use all the operators for Snowflake (https://github.com/apache/airflow/tree/master/airflow/providers/snowflake/operators) would be useful.

Other guides are available:
https://airflow.readthedocs.io/en/latest/howto/operator/index.html

Source code for those guides are at:
https://github.com/apache/airflow/tree/master/docs/howto/operator

Are you wondering how to start contributing to this project? Start by reading our [contributor guide](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)

Best regards,
Kaxil


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/docs_build/lint_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/dms.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/rds.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/neptune.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/operators_and_hooks_ref.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vision.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/cloud_build.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/appflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eventbridge.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/ecs.py']
Ground Truth : ['a/airflow/providers/snowflake/example_dags/example_snowflake.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Forward slash in `dag_run_id` gives rise to trouble accessing things through the REST API
### Apache Airflow version

2.1.4

### Operating System

linux

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==2.2.0
apache-airflow-providers-celery==2.0.0
apache-airflow-providers-cncf-kubernetes==2.0.2
apache-airflow-providers-docker==2.1.1
apache-airflow-providers-elasticsearch==2.0.3
apache-airflow-providers-ftp==2.0.1
apache-airflow-providers-google==5.1.0
apache-airflow-providers-grpc==2.0.1
apache-airflow-providers-hashicorp==2.1.0
apache-airflow-providers-http==2.0.1
apache-airflow-providers-imap==2.0.1
apache-airflow-providers-microsoft-azure==3.1.1
apache-airflow-providers-mysql==2.1.1
apache-airflow-providers-postgres==2.2.0
apache-airflow-providers-redis==2.0.1
apache-airflow-providers-sendgrid==2.0.1
apache-airflow-providers-sftp==2.1.1
apache-airflow-providers-slack==4.0.1
apache-airflow-providers-sqlite==2.0.1
apache-airflow-providers-ssh==2.1.1


### Deployment

Docker-Compose

### Deployment details

We tend to trigger dag runs by some external event, e.g., a media-file upload, see #19745.  It is useful to use the media-file path as a dag run id.  The media-id can come with some partial path, e.g., `path/to/mediafile`.  All this seems to work fine in airflow, but we can't figure out a way to use the such a dag run id in the REST API, as the forward slashes `/` interfere with the API routing.  

### What happened

When using the API route `api/v1/dags/{dag_id}/dagRuns/{dag_run_id}` in, e.g., a HTTP GET, we expect a dag run to be found when `dag_run_id` has the value `path/to/mediafile`, but instead a `.status: 404` is returned.  When we change the `dag_run_id` to the format `path|to|mediafile`, the dag run is returned. 

### What you expected to happen

We would expect a dag run to be returned, even if it contains the character `/`

### How to reproduce

Trigger a dag using a dag_run_id that contains a `/`, then try to retrieve it though the REST API. 

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py']
Ground Truth : ['a/airflow/www/views.py', 'a/airflow/models/dag.py']
Current Recall: 0.08180126440297748

=========================================================

ISSUE: Pagination doesn't work with tags filter

**Apache Airflow version**:
2.0.1

**Environment**:

- **OS**: Linux Mint 19.2 
- **Kernel**: 5.5.0-050500-generic #202001262030 SMP Mon Jan 27 01:33:36 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

**What happened**:

Seems that pagination doesn't work. I filter DAGs by tags and get too many results to get them at one page. When I click second page I'm redirected to the first one (actually, it doesn't matter if I click second, last or any other - I'm always getting redirected to the first one).

**What you expected to happen**:

I expect to be redirected to the correct page when I click number on the bottom of the page.

**How to reproduce it**:
1. Create a lot of DAGs with the same tag
2. Filter by tag
3. Go to the next page in the pagination bar

**Implementation example**:



```
from airflow import DAG
from airflow.utils.dates import days_ago
for i in range(200):
    name = 'test_dag_' + str(i)
    dag = DAG(
        dag_id=name,
        schedule_interval=None,
        start_date=days_ago(2),
        tags=['example1'],
    )
    globals()[name] = dag
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_home.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/example_ec2.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/hooks/test_emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/www/utils.py', 'a/airflow/www/views.py']
Current Recall: 0.08287192821454065

=========================================================

ISSUE: Use enum for task_states/dag_states
Hello,

We use Python 3+, so we should consider using an enumerated type for objects that are buckets for contents  This will give us better type hints.
https://github.com/apache/airflow/blob/master/airflow/utils/state.py

Best regards,
Kamil Bregua

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/file.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_new_session_in_provide_session.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py']
Ground Truth : ['a/airflow/typing_compat.py', 'a/airflow/models/dagrun.py', 'a/airflow/utils/state.py']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: UnicodeDecodeError in bash_operator.py
Hi,

I see a lot of these errors when running `airflow backfill` : 

```
Traceback (most recent call last):
  File "/usr/lib/python2.7/logging/__init__.py", line 851, in emit
    msg = self.format(record)
  File "/usr/lib/python2.7/logging/__init__.py", line 724, in format
    return fmt.format(record)
  File "/usr/lib/python2.7/logging/__init__.py", line 467, in format
    s = self._fmt % record.__dict__
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 13: ordinal not in range(128)
Logged from file bash_operator.py, line 72
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/redis/log/redis_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/colored_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/sensors/test_emr_step.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/gcs_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db_cleanup.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_logging_mixin.py']
Ground Truth : ['a/airflow/operators/bash_operator.py']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: `airflow dags status` fails if parse time is near `dagbag_import_timeout`
### Apache Airflow version

2.2.3 (latest released)

### What happened

I had just kicked off a DAG and I was periodically running `airflow dags status ...` to see if it was done yet.  At first it seemed to work, but later it failed with this error:

```
$ airflow dags state load_13 '2022-02-09T05:25:28+00:00'

    [2022-02-09 05:26:56,493] {dagbag.py:500} INFO - Filling up the DagBag from /usr/local/airflow/dags
    queued

$ airflow dags state load_13 '2022-02-09T05:25:28+00:00'

    [2022-02-09 05:27:29,096] {dagbag.py:500} INFO - Filling up the DagBag from /usr/local/airflow/dags
    [2022-02-09 05:27:59,084] {timeout.py:36} ERROR - Process timed out, PID: 759
    [2022-02-09 05:27:59,088] {dagbag.py:334} ERROR - Failed to import: /usr/local/airflow/dags/many_tasks.py
    Traceback (most recent call last):
      File "/usr/local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 331, in _load_modules_from_file
        loader.exec_module(new_module)
      File "<frozen importlib._bootstrap_external>", line 850, in exec_module
      File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
      File "/usr/local/airflow/dags/many_tasks.py", line 61, in <module>
        globals()["dag_{:02d}".format(i)] = parameterized_load(i, step)
      File "/usr/local/airflow/dags/many_tasks.py", line 50, in parameterized_load
        return load()
      File "/usr/local/lib/python3.9/site-packages/airflow/models/dag.py", line 2984, in factory
        f(**f_kwargs)
      File "/usr/local/airflow/dags/many_tasks.py", line 48, in load
        [worker_factory(i) for i in range(1, size**2 + 1)]
      File "/usr/local/airflow/dags/many_tasks.py", line 48, in <listcomp>
        [worker_factory(i) for i in range(1, size**2 + 1)]
      File "/usr/local/airflow/dags/many_tasks.py", line 37, in worker_factory
        return worker(num)
      File "/usr/local/lib/python3.9/site-packages/airflow/decorators/base.py", line 219, in factory
        op = decorated_operator_class(
      File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 188, in apply_defaults
        result = func(self, *args, **kwargs)
      File "/usr/local/lib/python3.9/site-packages/airflow/decorators/python.py", line 59, in __init__
        super().__init__(kwargs_to_upstream=kwargs_to_upstream, **kwargs)
      File "/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py", line 152, in apply_defaults
        dag_params = copy.deepcopy(dag.params) or {}
      File "/usr/local/lib/python3.9/copy.py", line 172, in deepcopy
        y = _reconstruct(x, memo, *rv)
      File "/usr/local/lib/python3.9/copy.py", line 264, in _reconstruct
        y = func(*args)
      File "/usr/local/lib/python3.9/copy.py", line 263, in <genexpr>
        args = (deepcopy(arg, memo) for arg in args)
      File "/usr/local/lib/python3.9/site-packages/airflow/utils/timeout.py", line 37, in handle_timeout
        raise AirflowTaskTimeout(self.error_message)
    airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /usr/local/airflow/dags/many_tasks.py after 30.0s.
    Please take a look at these docs to improve your DAG import time:
    * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/best-practices.html#top-level-python-code
    * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/latest/best-practices.html#reducing-dag-complexity, PID: 759
    Traceback (most recent call last):
      File "/usr/local/bin/airflow", line 8, in <module>
        sys.exit(main())
      File "/usr/local/lib/python3.9/site-packages/airflow/__main__.py", line 48, in main
        args.func(args)
      File "/usr/local/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 48, in command
        return func(*args, **kwargs)
      File "/usr/local/lib/python3.9/site-packages/airflow/utils/cli.py", line 92, in wrapper
        return f(*args, **kwargs)
      File "/usr/local/lib/python3.9/site-packages/airflow/cli/commands/dag_command.py", line 241, in dag_state
        dag = get_dag(args.subdir, args.dag_id)
      File "/usr/local/lib/python3.9/site-packages/airflow/utils/cli.py", line 192, in get_dag
        raise AirflowException(
    airflow.exceptions.AirflowException: Dag 'load_13' could not be found; either it does not exist or it failed to parse.
```

### What you expected to happen

If we were able to parse the DAG in the first place, I expect that downstream actions (like querying for status) would not fail due to a dag parsing timeout.

Also, is parsing the dag necessary for this action?

### How to reproduce

1. start with the dag shown here: https://gist.github.com/MatrixManAtYrService/842266aac42390aadee75fe014cd372e
2. increase "scale" until `airflow dags list` stop showing the load dags
3. decrease by one and check that they start showing back up
4. trigger a dag run
5. check its status (periodically), eventually the status check will fail

I initially discovered this using the `CeleryExecutor` and a much messier dag, but once I understood what I was looking for I was able to recreate it using the dag linked above and `astro dev start`

### Operating System

docker/debian

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

```
FROM quay.io/astronomer/ap-airflow:2.2.3-onbuild
```

### Anything else

When I was running this via the CeleryExecutor (deployed via helm on a single-node k8s cluster), I noticed similar dag-parsing timeouts showing up in the worker logs.  I failed to capture them because I didn't yet know what I was looking for, but if they would be helpful I can recreate that scenario and post them here.

----

I tried to work around this error by doubling the following configs:
- [dagbag_import_timeout](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#dagbag-import-timeout)
- [dag_file_processor_timeout](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#dag-file-processor-timeout)

This "worked", as in the status started showing up without error, but it seemed like making the dag __longer__ had also made it  __slower__.  As if whatever re-parsing steps were occurring along the way were also slowing it down.  It used to take 1h to complete, but when I checked on it after 1h it was only 30% complete (the new tasks hadn't even started yet).

Should I expect that splitting my large dag into smaller dags will fix this?  Or is the overall parsing workload going to eat into my runtime regardless of how it is sliced?

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py']
Ground Truth : ['a/airflow/cli/commands/dag_command.py']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: Dynamic task context fails to be pickled
### Apache Airflow version

2.5.0

### What happened



When I upgrade to 2.5.0, run dynamic task test failed. 

```py
from airflow.decorators import task, dag
import pendulum as pl

@dag(
    dag_id='test-dynamic-tasks', 
    schedule=None,
    start_date=pl.today().add(days=-3),
    tags=['example'])
def test_dynamic_tasks():

    @task.virtualenv(requirements=[])
    def sum_it(values):
        print(values)

    @task.virtualenv(requirements=[])
    def add_one(value):
        return value + 1

    added_values  = add_one.expand(value = [1,2])
    sum_it(added_values)

dag = test_dynamic_tasks()

```



```log
*** Reading local file: /home/andi/airflow/logs/dag_id=test-dynamic-tasks/run_id=manual__2022-12-06T10:07:41.355423+00:00/task_id=sum_it/attempt=1.log
[2022-12-06, 18:07:53 CST] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: test-dynamic-tasks.sum_it manual__2022-12-06T10:07:41.355423+00:00 [queued]>
[2022-12-06, 18:07:53 CST] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: test-dynamic-tasks.sum_it manual__2022-12-06T10:07:41.355423+00:00 [queued]>
[2022-12-06, 18:07:53 CST] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2022-12-06, 18:07:53 CST] {taskinstance.py:1284} INFO - Starting attempt 1 of 1
[2022-12-06, 18:07:53 CST] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2022-12-06, 18:07:53 CST] {taskinstance.py:1304} INFO - Executing <Task(_PythonVirtualenvDecoratedOperator): sum_it> on 2022-12-06 10:07:41.355423+00:00
[2022-12-06, 18:07:53 CST] {standard_task_runner.py:55} INFO - Started process 25873 to run task
[2022-12-06, 18:07:53 CST] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'test-dynamic-tasks', 'sum_it', 'manual__2022-12-06T10:07:41.355423+00:00', '--job-id', '41164', '--raw', '--subdir', 'DAGS_FOLDER/andi/test-dynamic-task.py', '--cfg-path', '/tmp/tmphudvake2']
[2022-12-06, 18:07:53 CST] {standard_task_runner.py:83} INFO - Job 41164: Subtask sum_it
[2022-12-06, 18:07:53 CST] {task_command.py:389} INFO - Running <TaskInstance: test-dynamic-tasks.sum_it manual__2022-12-06T10:07:41.355423+00:00 [running]> on host sh-dataops-airflow.jinde.local
[2022-12-06, 18:07:53 CST] {taskinstance.py:1511} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=andi@google.com
AIRFLOW_CTX_DAG_OWNER=andi
AIRFLOW_CTX_DAG_ID=test-dynamic-tasks
AIRFLOW_CTX_TASK_ID=sum_it
AIRFLOW_CTX_EXECUTION_DATE=2022-12-06T10:07:41.355423+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-12-06T10:07:41.355423+00:00
[2022-12-06, 18:07:53 CST] {process_utils.py:179} INFO - Executing cmd: /home/andi/airflow/venv38/bin/python -m virtualenv /tmp/venv7lc4m6na --system-site-packages
[2022-12-06, 18:07:53 CST] {process_utils.py:183} INFO - Output:
[2022-12-06, 18:07:54 CST] {process_utils.py:187} INFO - created virtual environment CPython3.8.0.final.0-64 in 220ms
[2022-12-06, 18:07:54 CST] {process_utils.py:187} INFO -   creator CPython3Posix(dest=/tmp/venv7lc4m6na, clear=False, no_vcs_ignore=False, global=True)
[2022-12-06, 18:07:54 CST] {process_utils.py:187} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/andi/.local/share/virtualenv)
[2022-12-06, 18:07:54 CST] {process_utils.py:187} INFO -     added seed packages: pip==22.2.1, setuptools==63.2.0, wheel==0.37.1
[2022-12-06, 18:07:54 CST] {process_utils.py:187} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
[2022-12-06, 18:07:54 CST] {process_utils.py:179} INFO - Executing cmd: /tmp/venv7lc4m6na/bin/pip install -r /tmp/venv7lc4m6na/requirements.txt
[2022-12-06, 18:07:54 CST] {process_utils.py:183} INFO - Output:
[2022-12-06, 18:07:55 CST] {process_utils.py:187} INFO - Looking in indexes: http://pypi:8081
[2022-12-06, 18:08:00 CST] {process_utils.py:187} INFO - 
[2022-12-06, 18:08:00 CST] {process_utils.py:187} INFO - [notice] A new release of pip available: 22.2.1 -> 22.3.1
[2022-12-06, 18:08:00 CST] {process_utils.py:187} INFO - [notice] To update, run: python -m pip install --upgrade pip
[2022-12-06, 18:08:00 CST] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/andi/airflow/venv38/lib/python3.8/site-packages/airflow/decorators/base.py", line 217, in execute
    return_value = super().execute(context)
  File "/home/andi/airflow/venv38/lib/python3.8/site-packages/airflow/operators/python.py", line 356, in execute
    return super().execute(context=serializable_context)
  File "/home/andi/airflow/venv38/lib/python3.8/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/andi/airflow/venv38/lib/python3.8/site-packages/airflow/operators/python.py", line 553, in execute_callable
    return self._execute_python_callable_in_subprocess(python_path, tmp_path)
  File "/home/andi/airflow/venv38/lib/python3.8/site-packages/airflow/operators/python.py", line 397, in _execute_python_callable_in_subprocess
    self._write_args(input_path)
  File "/home/andi/airflow/venv38/lib/python3.8/site-packages/airflow/operators/python.py", line 367, in _write_args
    file.write_bytes(self.pickling_library.dumps({"args": self.op_args, "kwargs": self.op_kwargs}))
_pickle.PicklingError: Can't pickle <class 'sqlalchemy.orm.session.Session'>: it's not the same object as sqlalchemy.orm.session.Session
[2022-12-06, 18:08:00 CST] {taskinstance.py:1322} INFO - Marking task as FAILED. dag_id=test-dynamic-tasks, task_id=sum_it, execution_date=20221206T100741, start_date=20221206T100753, end_date=20221206T100800
[2022-12-06, 18:08:00 CST] {warnings.py:109} WARNING - /home/andi/airflow/venv38/lib/python3.8/site-packages/airflow/utils/email.py:120: RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
  send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)

[2022-12-06, 18:08:00 CST] {configuration.py:635} WARNING - section/key [smtp/smtp_user] not found in config
[2022-12-06, 18:08:00 CST] {email.py:229} INFO - Email alerting: attempt 1
[2022-12-06, 18:08:01 CST] {email.py:241} INFO - Sent an alert email to ['andi@google.com']
[2022-12-06, 18:08:01 CST] {standard_task_runner.py:100} ERROR - Failed to execute job 41164 for task sum_it (Can't pickle <class 'sqlalchemy.orm.session.Session'>: it's not the same object as sqlalchemy.orm.session.Session; 25873)
[2022-12-06, 18:08:01 CST] {local_task_job.py:159} INFO - Task exited with return code 1
[2022-12-06, 18:08:01 CST] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
```

### What you think should happen instead

I expect this sample run passed.

### How to reproduce

_No response_

### Operating System

centos 7.9 3.10.0-1160.el7.x86_64

### Versions of Apache Airflow Providers

```
airflow-code-editor==5.2.2
apache-airflow-providers-celery==3.0.0
apache-airflow-providers-microsoft-mssql==3.1.0
apache-airflow-providers-microsoft-psrp==2.0.0
apache-airflow-providers-microsoft-winrm==3.0.0
apache-airflow-providers-mysql==3.0.0
apache-airflow-providers-redis==3.0.0
apache-airflow-providers-samba==4.0.0
apache-airflow-providers-sftp==3.0.0
autopep8==1.6.0
brotlipy==0.7.0
chardet==3.0.4
pip-chill==1.0.1
pyopenssl==19.1.0
pysocks==1.7.1
python-ldap==3.4.2
requests-credssp==2.0.0
swagger-ui-bundle==0.0.9
tqdm==4.51.0
virtualenv==20.16.2
yapf==0.32.0
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/postgres/example_postgres.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_renderedtifields.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/serializers/test_serializers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_dag_run_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/transfers/test_calendar_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_cluster_activity.py']
Ground Truth : ['a/airflow/models/xcom.py']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: Allow mapped Task as input to another mapped task
This dag

```python
with DAG(dag_id="simple_mapping", start_date=pendulum.DateTime(2022, 4, 6), catchup=True) as d3:
    @task(email='a@b.com')
    def add_one(x: int):
      return x + 1

    two_three_four = add_one.expand(x=[1, 2, 3])
    three_four_five = add_one.expand(x=two_three_four)
```

Fails with this error:

```
  File "/home/ash/code/airflow/airflow/airflow/models/taskinstance.py", line 2239, in _record_task_map_for_downstreams
    raise UnmappableXComTypePushed(value)
airflow.exceptions.UnmappableXComTypePushed: unmappable return type 'int'
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_dynamic_task_mapping.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py']
Ground Truth : ['a/airflow/models/mappedoperator.py', 'a/airflow/models/taskinstance.py']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: Apache Pinot provider.yaml references missing PinotHook class
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

When starting Airflow (the problem seems to be in both 2.6.3 and in 2.7.0, see "How to reproduce" below) I am getting the following warning:
```
{providers_manager.py:253} WARNING - Exception when importing 'airflow.providers.apache.pinot.hooks.pinot.PinotHook' from 'apache-airflow-providers-apache-pinot' package
Traceback (most recent call last):
  File "/opt/bitnami/airflow/venv/lib/python3.9/site-packages/airflow/utils/module_loading.py", line 39, in import_string
    return getattr(module, class_name)
AttributeError: module 'airflow.providers.apache.pinot.hooks.pinot' has no attribute 'PinotHook'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/airflow/venv/lib/python3.9/site-packages/airflow/providers_manager.py", line 285, in _sanity_check
    imported_class = import_string(class_name)
  File "/opt/bitnami/airflow/venv/lib/python3.9/site-packages/airflow/utils/module_loading.py", line 41, in import_string
    raise ImportError(f'Module "{module_path}" does not define a "{class_name}" attribute/class')
ImportError: Module "airflow.providers.apache.pinot.hooks.pinot" does not define a "PinotHook" attribute/class
```

I looked into the issue and it appears the problem in the Apache Pinot provider. The `airflow/providers/apache/pinot/provider.yaml` (which is loaded by the `_sanity_check` in `providers_manager`) is referencing a `PinotHook` class that does not exist: https://github.com/apache/airflow/blob/487b174073c01e03ae64760405a8d88f6a488ca6/airflow/providers/apache/pinot/provider.yaml#L57-L59

The module `airflow.providers.apache.pinot.hooks.pinot` contains `PinotAdminHook` and `PinotDbApiHook`, but not `PinotHook` (and the classes have been separate since before the Apache classes were split into the Apache provider).

I am willing to fix this, but I am not sure which is a better fix:

1. I could list both classes in `connection-types` of `provider.yaml`, but keep both as `connection-type: pinot`, but there will be two connection types with the same name (which may not be possible?):
    ```yaml
    connection-types:
      - hook-class-name: airflow.providers.apache.pinot.hooks.pinot.PinotAdminHook
        connection-type: pinot
      - hook-class-name: airflow.providers.apache.pinot.hooks.pinot.PinotDbApiHook
        connection-type: pinot
    ```
    - Note: `create_default_connections` in `airflow/utils/db.py` is currently including both connection with the same `conn_type="pinot"`: https://github.com/apache/airflow/blob/487b174073c01e03ae64760405a8d88f6a488ca6/airflow/utils/db.py#L474-L492
2. or we change one (or both) of the connection types to a different name. `PinotAdminHook` already uses a default connection name of `pinot_admin_default`, and `PinotDbApiHook` already uses a default connection name of `pinot_broker_default`, so it might make sense to name these connection types `pinot_admin` and `pinot_broker`:
    ```yaml
    connection-types:
      - hook-class-name: airflow.providers.apache.pinot.hooks.pinot.PinotAdminHook
        connection-type: pinot_admin
      - hook-class-name: airflow.providers.apache.pinot.hooks.pinot.PinotDbApiHook
        connection-type: pinot_broker
    ```
    - I think we will need to change `create_default_connections` in `airflow/utils/db.py` (as shown above) if we end up changing the connection types. Possibly other places, but I have not seen any other references of `conn_type="pinot"` beside the default connections.

Thoughts on which approach is better / less disruptive to users?

### What you think should happen instead

When starting Airflow the `_sanity_check` in `providers_manager` should not trigger a warning. Both connection types should be useable.

### How to reproduce

I am seeing this every time I start Airflow with the Docker image `bitnami/airflow:2.6.3`, but I will also test using Breeze and other ways to see if I see the same warning in 2.7.0. But since the problem line of code is unchanged in the `main`branch I am sure this is still an issue.

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-pinot==4.1.1

### Deployment

Docker-Compose

### Deployment details

Docker image `bitnami/airflow:2.6.3` in Docker Compose (this is the latest Airflow version for Bitnami's image, they have not yet pushed up a 2.7.0 version)

### Anything else

The issue https://github.com/apache/airflow/issues/28790 is related as it mentions that both hooks of the Apache Pinot provider is missing `conn_type`.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/providers/apache/pinot/hooks/pinot.py']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: The command 'airflow dags test' is deadlocking tasks using AIP-42 features with the PythonVirtualenvOperator
### Apache Airflow version

main (development)

### What happened

When using the command airflow dags test tasks are put in the deadlock state.

### What you think should happen instead

I think the airflow dags test shouldn't deadlock tasks.

### How to reproduce
```
from airflow.models import DAG
from airflow.operators.python import PythonOperator, PythonVirtualenvOperator
from airflow.utils.log.log_reader import TaskLogReader
from airflow.utils.dates import days_ago


from pkgutil import iter_modules
from datetime import datetime, timedelta



the_imports = ["flask", "wheel", "click", "cryptography", "packaging"]

def dynamic_imports(imports: list):
    from random import choices
    return [choices(imports, k=3) for _ in range(7)]

def dynamic_importer(*args):
    import importlib
    for mod in args[0]:
        print(mod)
        importlib.import_module(mod)

with DAG(
    dag_id="sys_site_packages_true",
    schedule_interval=timedelta(days=365),
    start_date=datetime(2001, 1, 1),
    doc_md=docs,
    tags=["core", "extended_tags", "venv_op"],
) as  dag:

    pv0 = PythonVirtualenvOperator.partial(
        task_id="dont_install_packages_already_in_outer_environment",
        python_callable=dynamic_importer,
        python_version=3.8,
        system_site_packages=True,
        requirements=the_imports,
    ).expand(op_args=dynamic_imports(the_imports))

[2022-03-30 17:13:36,447] {taskinstance.py:850} DEBUG - Setting task state for <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [None]> to scheduled
[2022-03-30 17:13:36,455] {backfill_job.py:405} DEBUG - *** Clearing out not_ready list ***
[2022-03-30 17:13:36,458] {taskinstance.py:759} DEBUG - Refreshing TaskInstance <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> from DB
[2022-03-30 17:13:36,461] {backfill_job.py:418} DEBUG - Task instance to run <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> state scheduled
[2022-03-30 17:13:36,461] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Previous Dagrun State' PASSED: True, The context specified that the state of past DAGs could be ignored.
[2022-03-30 17:13:36,461] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2022-03-30 17:13:36,462] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Task has been mapped' PASSED: False, The task has yet to be mapped!
[2022-03-30 17:13:36,462] {taskinstance.py:1041} DEBUG - Dependencies not met for <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]>, dependency 'Task has been mapped' FAILED: The task has yet to be mapped!
[2022-03-30 17:13:36,462] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2022-03-30 17:13:36,462] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2022-03-30 17:13:36,462] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Task Instance State' PASSED: True, Task state scheduled was valid.
[2022-03-30 17:13:36,463] {backfill_job.py:536} DEBUG - Adding <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> to not_ready
[2022-03-30 17:13:41,428] {base_job.py:226} DEBUG - [heartbeat]
[2022-03-30 17:13:41,428] {base_executor.py:156} DEBUG - 0 running task instances
[2022-03-30 17:13:41,429] {base_executor.py:157} DEBUG - 0 in queue
[2022-03-30 17:13:41,429] {base_executor.py:158} DEBUG - 32 open slots
[2022-03-30 17:13:41,429] {base_executor.py:167} DEBUG - Calling the <class 'airflow.executors.debug_executor.DebugExecutor'> sync method
[2022-03-30 17:13:41,429] {backfill_job.py:596} WARNING - Deadlock discovered for ti_status.to_run=dict_values([<TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]>])
[2022-03-30 17:13:41,433] {dagrun.py:628} DEBUG - number of tis tasks for <DagRun sys_site_packages_true @ 2022-03-30T17:13:35+00:00: backfill__2022-03-30T17:13:35+00:00, externally triggered: False>: 1 task(s)
[2022-03-30 17:13:41,433] {dagrun.py:644} DEBUG - number of scheduleable tasks for <DagRun sys_site_packages_true @ 2022-03-30T17:13:35+00:00: backfill__2022-03-30T17:13:35+00:00, externally triggered: False>: 0 task(s)
[2022-03-30 17:13:41,434] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Not In Retry Period' PASSED: True, The context specified that being in a retry period was permitted.
[2022-03-30 17:13:41,434] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2022-03-30 17:13:41,434] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2022-03-30 17:13:41,434] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Task has been mapped' PASSED: False, The task has yet to be mapped!
[2022-03-30 17:13:41,434] {taskinstance.py:1041} DEBUG - Dependencies not met for <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]>, dependency 'Task has been mapped' FAILED: The task has yet to be mapped!
[2022-03-30 17:13:41,434] {dagrun.py:570} ERROR - Deadlock; marking run <DagRun sys_site_packages_true @ 2022-03-30T17:13:35+00:00: backfill__2022-03-30T17:13:35+00:00, externally triggered: False> failed
[2022-03-30 17:13:41,434] {dagrun.py:594} INFO - DagRun Finished: dag_id=sys_site_packages_true, execution_date=2022-03-30T17:13:35+00:00, run_id=backfill__2022-03-30T17:13:35+00:00, run_start_date=2022-03-30 23:13:36.434073+00:00, run_end_date=2022-03-30 23:13:41.434824+00:00, run_duration=5.000751, state=failed, external_trigger=False, run_type=backfill, data_interval_start=2022-03-30T17:13:35+00:00, data_interval_end=2023-03-30T17:13:35+00:00, dag_hash=None
[2022-03-30 17:13:41,435] {backfill_job.py:362} INFO - [backfill progress] | finished run 1 of 1 | tasks waiting: 0 | succeeded: 0 | running: 0 | failed: 0 | skipped: 0 | deadlocked: 1 | not ready: 1
[2022-03-30 17:13:41,435] {backfill_job.py:376} DEBUG - Finished dag run loop iteration. Remaining tasks dict_values([])
[2022-03-30 17:13:41,438] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2022-03-30 17:13:41,438] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2022-03-30 17:13:41,438] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2022-03-30 17:13:41,438] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Task has been mapped' PASSED: False, The task has yet to be mapped!
[2022-03-30 17:13:41,438] {taskinstance.py:1041} DEBUG - Dependencies not met for <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]>, dependency 'Task has been mapped' FAILED: The task has yet to be mapped!
[2022-03-30 17:13:41,440] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2022-03-30 17:13:41,440] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Previous Dagrun State' PASSED: True, The context specified that the state of past DAGs could be ignored.
[2022-03-30 17:13:41,440] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2022-03-30 17:13:41,440] {taskinstance.py:1061} DEBUG - <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]> dependency 'Task has been mapped' PASSED: False, The task has yet to be mapped!
[2022-03-30 17:13:41,440] {taskinstance.py:1041} DEBUG - Dependencies not met for <TaskInstance: sys_site_packages_true.dont_install_packages_already_in_outer_environment backfill__2022-03-30T17:13:35+00:00 [scheduled]>, dependency 'Task has been mapped' FAILED: The task has yet to be mapped!
BackfillJob is deadlocked.
These tasks have succeeded:
DAG ID    Task ID    Run ID    Try number
--------  ---------  --------  ------------

These tasks are running:
DAG ID    Task ID    Run ID    Try number
--------  ---------  --------  ------------

These tasks have failed:
DAG ID    Task ID    Run ID    Try number
--------  ---------  --------  ------------

These tasks are skipped:
DAG ID    Task ID    Run ID    Try number
--------  ---------  --------  ------------

These tasks are deadlocked:
DAG ID                  Task ID                                             Run ID                                 Try number
----------------------  --------------------------------------------------  -----------------------------------  ------------
sys_site_packages_true  dont_install_packages_already_in_outer_environment  backfill__2022-03-30T17:13:35+00:00             1
[2022-03-30 17:13:41,446] {cli_action_loggers.py:84} DEBUG - Calling callbacks: []
[2022-03-30 17:13:41,447] {settings.py:383} DEBUG - Disposing DB connection pool (PID 816091)


```
### Operating System

Ubuntu Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

tested with airflow breeze and a local virtualenv with airflow 2.3.0 installed.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/triggers/test_temporal.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_param.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/operators/test_appflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py']
Ground Truth : ['a/airflow/jobs/backfill_job.py', 'a/airflow/models/taskmixin.py']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: Webserver shows wrong datetime (timezone) in log
### Apache Airflow version

2.3.0 (latest released)

### What happened

same as #19401 , when I open task`s log in web interface, it shifts this time forward by 8 hours (for Asia/Shanghai), but it's already in Asia/Shanghai.
here is the log in web:
```
*** Reading local file: /opt/airflow/logs/forecast/cal/2022-05-18T09:50:00+00:00/1.log
[2022-05-19, 13:54:52] {taskinstance.py:1037} INFO ...
```

As you seee, UTC time is 2022-05-18T09:50:00and My timezone is Asia/Shanghai(should shift forward 8 hours)but it shift forward 16hours!

### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Operating System

Debian GNU/Linux 11 (bullseye)(docker)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

1. build my docker image from apache/airflow:2.3.0 to change timezone
```Dockerfile
FROM apache/airflow:2.3.0
# bugfix of log UI in web, here I change ti_log.js file by following on #19401 
COPY ./ti_log.js /home/airflow/.local/lib/python3.7/site-packages/airflow/www/static/js/ti_log.js
USER root
# container timezone changed to CST time
RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
  && rm -rf /etc/timezone \
  && echo Asia/Shanghai >> /etc/timezone \
  && chown airflow /home/airflow/.local/lib/python3.7/site-packages/airflow/www/static/js/ti_log.js
USER airflow
```
2. use my image to run airflow by docker-compose

3. check task logs in web
Although I have changed the file `airflow/www/static/js/ti_log.js`, but it did not work! Then check source from Web, I found another file : `airflow/www/static/dist/tiLog.e915520196109d459cf8.js`, then I replace "+00:00" by  "+08:00" in this file. Finally it works!

```js
# origin tiLog.e915520196109d459cf8.js
replaceAll(c,(e=>`<time datetime="${e}+00:00" data-with-tz="true">${Object(a.f)(`${e}+00:00`)}</time>`))
```

```js
# what I changed
replaceAll(c,(e=>`<time datetime="${e}+08:00" data-with-tz="true">${Object(a.f)(`${e}+08:00`)}</time>`))
```





### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/serializers/test_serializers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_json.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py']
Ground Truth : ['a/airflow/config_templates/airflow_local_settings.py', 'a/airflow/utils/log/colored_log.py', '/dev/null']
Current Recall: 0.0835857040889161

=========================================================

ISSUE: Description Field for Variables
add text column to explain what the variable is used for

same as https://github.com/apache/airflow/issues/10840 just for Variables.



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/variable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/params/shell_params.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/substitution_extensions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0083_2_1_0_add_description_field_to_variable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_bulk_issues.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/hive/hooks/hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/system_tests/update_issue_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py']
Ground Truth : ['a/airflow/models/variable.py', 'a/airflow/www/views.py', '/dev/null']
Current Recall: 0.08429947996329154

=========================================================

ISSUE: "triggered runs" dataset counter doesn't update until *next* run and never goes above 1
### Apache Airflow version

2.4.0b1

### What happened

I have [this test dag](https://gist.github.com/MatrixManAtYrService/2cf0ebbd85faa2aac682d9c441796c58) which I created to report [this issue](https://github.com/apache/airflow/issues/25210).  The idea is that if you unpause "sink" and all of the "sources" then the sources will wait until the clock is like \*:\*:00 and they'll terminate at the same time.  

Since each source triggers the sink with a dataset called "counter", the "sink" dag will run just once, and it will have output like:  `INFO - [(16, 1)]`, that's 16 sources and 1 sink that ran.

At this point, you can look at the dataset history for "counter" and you'll see this:

<img width="524" alt="Screen Shot 2022-09-08 at 6 07 44 PM" src="https://user-images.githubusercontent.com/5834582/189248999-d31141a4-2d0b-4ec2-9ea5-c4c3536b3a28.png">

So we've got a timestamp, but the "triggered runs" count is empty.  That's weird.  One run was triggered (and it finished by the time the screenshot was taken), so why doesn't it say `1`?

So I redeploy and try it again, except this time I wait several seconds between each "unpause" click, the idea being that maybe some of them fire at 07:16:00 and the others fire at 07:17:00.  I end up with this:

<img width="699" alt="Screen Shot 2022-09-08 at 6 19 12 PM" src="https://user-images.githubusercontent.com/5834582/189252116-69067189-751d-40e7-89c5-8d1da1720237.png">

So fifteen of them finished at once and caused the dataset to update, and then just one straggler (number 9)  is waiting for an additional minute.  I wait for the straggler to complete and go back to the dataset view:

<img width="496" alt="Screen Shot 2022-09-08 at 6 20 41 PM" src="https://user-images.githubusercontent.com/5834582/189253874-87bb3eb3-2237-42a1-bc3f-9fc210419f1a.png">

Now it's the straggler that is blank, but the rest of them are populated.  Continuing to manually run these, I find that whichever one I have run most recently is blank, and all of the others are 1, even if this is the second or third time I've run them



### What you think should happen instead

- The triggered runs counter should increment beyond 1
- It should increment immediately after the dag was triggered, not wait until after the *next* dag gets triggered.

### How to reproduce

See dags in in this gist: https://gist.github.com/MatrixManAtYrService/2cf0ebbd85faa2aac682d9c441796c58

1. unpause "sink"
2. unpause half of sources
3. wait one minute
4. unpause the other half of the sources
5. wait for "sink" to run a second time
6. view the dataset history for "counter"
7. ask why only half of the counts are populated
8. manually trigger some sources, wait for them to trigger sink
9. view the dataset history again
10. ask why none of them show more than 1 dagrun triggered

### Operating System

Kubernetes in Docker, deployed via helm

### Versions of Apache Airflow Providers

n/a

### Deployment

Other 3rd-party Helm chart

### Deployment details

see "deploy.sh" in the gist: 

https://gist.github.com/MatrixManAtYrService/2cf0ebbd85faa2aac682d9c441796c58

It's just a fresh install into a k8s cluster

### Anything else

n/a

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py', 'a/airflow/datasets/manager.py']
Current Recall: 0.08429947996329154

=========================================================

ISSUE: Provider packages don't include datafiles
The amazon and google providers don't include necessary datafiles in them.

They were previously included in the sdist via MANIFEST.in (see https://github.com/apache/airflow/pull/12196) and in the bdist via include_package_data from the top level setup.py

Both of these are currently missing.

I've put this against 2.0.0-beta1, it _could_ be changed separately as providers are separate releases.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/testing_commands.py']
Ground Truth : ['a/dev/provider_packages/prepare_provider_packages.py']
Current Recall: 0.08429947996329154

=========================================================

ISSUE: Bump supported mysqlclient to <1.5

**Description**

version 1.4.X introduced in Jan 2019 

we should support it if we can.

**Use case / motivation**

pin of <1.4 was done in https://github.com/apache/airflow/pull/4558 due to lack of Python 2 compatibility. Since Master doesn't support Python 2 anymore there is no need for that restriction

**Related Issues**

Moved from https://issues.apache.org/jira/browse/AIRFLOW-4810

I tried to fix it in https://github.com/apache/airflow/pull/5430 but didn't get help with the tests so if any one wants to pick it up be my guest.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.08429947996329154

=========================================================

ISSUE: Potential Bug in DataFlowCreateJavaJobOperator
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

2.2.3

### Operating System

mac

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

Passing anything other than a GCS bucket path to the `jar` argument results in the job never being started on DataFlow.

### What you expected to happen

Passing in a local path to a jar should result in a job starting on Data Flow.

### How to reproduce

Create a task using the DataFlowCreateJavaJobOperator and pass in a non GCS path to the `jar` argument.

### Anything else

It's probably an indentation error in this [file](https://github.com/apache/airflow/blob/17d3e78e1b4011267e81846b5d496769934a5bcc/airflow/providers/google/cloud/operators/dataflow.py#L413) starting on line 413. The code for starting the job is over indented and causes any non GCS path for the `jar` to be effectively ignored.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/operators/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/livy/hooks/livy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/hooks/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py']
Ground Truth : ['a/airflow/providers/google/cloud/operators/dataflow.py']
Current Recall: 0.08429947996329154

=========================================================

ISSUE: Add missing parameter documentation for `KubernetesHook` and `KubernetesPodOperator`
### Body

Currently the following modules are missing certain parameters in their docstrings. Because of this, these parameters are not captured in the [Python API docs for the Kubernetes provider](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/_api/airflow/providers/cncf/kubernetes/index.html).


- [ ] KubernetesHook: `in_cluster`, `config_file`, `cluster_context`, `client_configuration`
- [ ] KubernetesPodOperator: `env_from`, `node_selectors`, `pod_runtime_info_envs`, `configmaps`

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/hooks/kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/hooks/test_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/flink/operators/flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/operators/test_spark_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/resource.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/eks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py']
Current Recall: 0.08429947996329154

=========================================================

ISSUE: Running airflow dags backfill --reset-dagruns <dag_id> -s  <execution_start_dt> -e <execution_end_dt> results in error when run twice. 
### Apache Airflow version

2.2.3 (latest released)

### What happened

It's the same situation as https://github.com/apache/airflow/issues/21023.
Only change to `airflow dags backfill` from `airflow dags test`.

``` bash
(airflow) [www@np-data-eng-airflow-sync001-lde-jp2v-prod ~]$ airflow dags backfill tutorial --reset-dagruns -s 2022-01-20 -e 2022-01-23
You are about to delete these 9 tasks:
<TaskInstance: tutorial.print_date scheduled__2022-01-20T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.print_date scheduled__2022-01-21T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.print_date scheduled__2022-01-22T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.sleep scheduled__2022-01-20T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.sleep scheduled__2022-01-21T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.sleep scheduled__2022-01-22T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.templated scheduled__2022-01-20T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.templated scheduled__2022-01-21T07:48:54.720148+00:00 [success]>
<TaskInstance: tutorial.templated scheduled__2022-01-22T07:48:54.720148+00:00 [success]>

Are you sure? (yes/no):
y
Traceback (most recent call last):
  File "/home1/www/venv3/airflow/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/__main__.py", line 48, in main
    args.func(args)
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/cli/commands/dag_command.py", line 108, in dag_backfill
    dag_run_state=State.NONE,
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/models/dag.py", line 1948, in clear_dags
    dry_run=False,
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/models/dag.py", line 1887, in clear
    dag_run_state=dag_run_state,
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 270, in clear_task_instances
    dr.state = dag_run_state
  File "<string>", line 1, in __set__
  File "/home1/www/venv3/airflow/lib/python3.7/site-packages/airflow/models/dagrun.py", line 194, in set_state
    raise ValueError(f"invalid DagRun state: {state}")
ValueError: invalid DagRun state: None
```

### What you expected to happen

_No response_

### How to reproduce

1. Setup Airflow 2.2.3
2. Run any dag (in my case, using [tutorial dag file)](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html).
3. Run again same dag by `airflow dags backfill` command.

### Operating System

CentOS Linux 7.9

### Versions of Apache Airflow Providers

Providers info
apache-airflow-providers-celery          | 2.1.0
apache-airflow-providers-cncf-kubernetes | 3.0.1
apache-airflow-providers-ftp             | 2.0.1
apache-airflow-providers-http            | 2.0.2
apache-airflow-providers-imap            | 2.1.0
apache-airflow-providers-mysql           | 2.1.1
apache-airflow-providers-redis           | 2.0.1
apache-airflow-providers-slack           | 4.1.0
apache-airflow-providers-sqlite          | 2.0.1
apache-airflow-providers-ssh             | 2.3.0

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_db_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/utils/state.py']
Current Recall: 0.08429947996329154

=========================================================

ISSUE: TableauRefreshWorkbookOperator fails when using personal access token (Tableau authentication method)
**Apache Airflow version**: 2.0.1

**What happened**:

The operator fails at the last step, after successfully refreshing the workbook with this error:
```
tableauserverclient.server.endpoint.exceptions.ServerResponseError: 

        401002: Unauthorized Access
                Invalid authentication credentials were provided.
```

**What you expected to happen**:
It should not fail, like when we use the username/password authentication method (instead of personal_access_token)

<!-- What do you think went wrong? -->

Tableau server does not allow concurrent connections when using personal_access_token https://github.com/tableau/server-client-python/issues/717
The solution would be redesigning completely the operator to only call the hook once.
My quick fix was to edit this in TableauHook:

```
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        pass
```

**How to reproduce it**:

Run this operator TableauRefreshWorkbookOperator using Tableau personal_access_token authentication (token_name, personal_access_token).


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/hooks/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/operators/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/tableau/hooks/test_tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/sensors/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jenkins/operators/jenkins_job_trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/base_aws.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py']
Ground Truth : ['a/airflow/providers/tableau/hooks/tableau.py', 'a/airflow/providers/tableau/operators/tableau_refresh_workbook.py', 'a/airflow/providers/tableau/sensors/tableau_job_status.py']
Current Recall: 0.08501325583766699

=========================================================

ISSUE: AIP-56 - FAB AM - Login
Create the whole login process:

- Move the login form view to auth manager
- Move the login logic to auth manager
- Return the login URL as part of get_url_login() API

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/views/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/databricks/hooks/test_databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cloudant/hooks/cloudant.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/forms.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/redshift_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/hooks/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/data_lake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/marketing_platform/example_analytics_admin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/presto/hooks/test_presto.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jenkins/hooks/jenkins.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/base_azure.py']
Ground Truth : ['a/airflow/www/security.py', 'a/airflow/www/extensions/init_security.py', 'a/airflow/www/fab_security/manager.py', 'a/airflow/www/fab_security/sqla/manager.py', '/dev/null', 'a/airflow/auth/managers/fab/fab_auth_manager.py', 'a/airflow/auth/managers/base_auth_manager.py', 'a/airflow/www/auth.py', 'a/airflow/www/extensions/init_auth_manager.py', 'a/airflow/www/extensions/init_appbuilder.py', 'a/airflow/auth/managers/fab/security_manager_override.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: cleanup-pod CLI command fails due to incorrect host
### Apache Airflow version

2.6.1

### What happened

When running `airflow kubernetes cleanup-pods`, the API call to delete a pod fails. A snippet of the log is below:

```
urllib3.exceptions.MaxRetryError:
HTTPConnectionPool(host='localhost', port=80): Max retries exceeded with url: /api/v1/namespaces/airflow/pods/my-task-avd79fq1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f52f9aebfd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
```

[The Kubernetes client provisioned in _delete_pod](https://github.com/apache/airflow/blob/main/airflow/cli/commands/kubernetes_command.py#L151) incorrectly has the host as `http:localhost`. On the scheduler pod if I start a Python environment I can see that the configuration differs from the `get_kube_client()` configuration:

```
>>> get_kube_client().api_client.configuration.host
'https://172.20.0.1:443'
>>> client.CoreV1Api().api_client.configuration.host
'http://localhost/'
```

On Airflow 2.5.3 these two clients have the same configuration.

It's possible I have some mistake in my configuration but I'm not sure what it could be. The above fails on 2.6.0 also.

### What you think should happen instead

Pods should clean up without error

### How to reproduce

Run the following from a Kubernetes deployment of Airflow:

```python
from airflow.kubernetes.kube_client import get_kube_client
from kubernetes import client

print(get_kube_client().api_client.configuration.host)
print(client.CoreV1Api().api_client.configuration.host)
```

Alternatively run `airflow kubernetes cleanup-pods` with pods available for cleanup

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Using `in_cluster` configuration for KubernetesExecutor

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/pod.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/kubernetes_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/hooks/kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/kubernetes_tests/test_kubernetes_pod_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py']
Ground Truth : ['a/airflow/cli/commands/kubernetes_command.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: All task logging goes to the log for try_number 1
**Apache Airflow version**: 2.0.0a1
**What happened**:

When a task fails on the first try, the log output for additional tries go to the log for the first attempt.

**What you expected to happen**:

The logs should go to the correct log file. For the default configuration, the log filename template is `log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log`, so additional numbered `.log` files should be created.

**How to reproduce it**:

Create a test dag:

```
from datetime import timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago


with DAG(
    dag_id="trynumber_demo",
    default_args={"start_date": days_ago(2), "retries": 1, "retry_delay": timedelta(0)},
    schedule_interval=None,
) as dag:

    def demo_task(ti=None):
        print("Running demo_task, try_number =", ti.try_number)
        if ti.try_number <= 1:
            raise ValueError("Shan't")

    task = PythonOperator(task_id="demo_task", python_callable=demo_task)
```

and trigger this dag:

```
$ airflow dags trigger trynumber_demo
```

then observe that `triggernumber_demo/demo_task/<execution_date>/` only contains 1.log, which contains the full output for 2 runs:

```
[...]
--------------------------------------------------------------------------------
[2020-10-21 13:29:07,958] {taskinstance.py:1020} INFO - Starting attempt 1 of 2
[2020-10-21 13:29:07,959] {taskinstance.py:1021} INFO -
--------------------------------------------------------------------------------
[...]
[2020-10-21 13:29:08,163] {logging_mixin.py:110} INFO - Running demo_task, try_number = 1
[2020-10-21 13:29:08,164] {taskinstance.py:1348} ERROR - Shan't
Traceback (most recent call last):
[...]
ValueError: Shan't
[2020-10-21 13:29:08,168] {taskinstance.py:1392} INFO - Marking task as UP_FOR_RETRY. dag_id=trynumber_demo, task_id=demo_task, execution_date=20201021T122907, start_date=20201021T122907, end_date=20201021T122908
[...]
[2020-10-21 13:29:09,121] {taskinstance.py:1019} INFO -
--------------------------------------------------------------------------------
[2020-10-21 13:29:09,121] {taskinstance.py:1020} INFO - Starting attempt 2 of 2
[2020-10-21 13:29:09,121] {taskinstance.py:1021} INFO -
--------------------------------------------------------------------------------
[...]
[2020-10-21 13:29:09,333] {logging_mixin.py:110} INFO - Running demo_task, try_number = 2
[2020-10-21 13:29:09,334] {python.py:141} INFO - Done. Returned value was: None
[2020-10-21 13:29:09,355] {taskinstance.py:1143} INFO - Marking task as SUCCESS.dag_id=trynumber_demo, task_id=demo_task, execution_date=20201021T122907, start_date=20201021T122909, end_date=20201021T122909
[2020-10-21 13:29:09,404] {local_task_job.py:117} INFO - Task exited with return code 0
```

The `TaskInstance()` created for the run needs to first be refreshed from the database, before setting the logging context.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_log_reader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/perf/dags/sql_perf_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_response.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Warning e-mails are sent without the Date header
The warning e-mails don't have the "Date" header.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/smtp/hooks/smtp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/imap/hooks/imap.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sendgrid/utils/emailer.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/ses.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/email.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/jenkins/operators/jenkins_job_trigger.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/http/hooks/http.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/send_email.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/utilities/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/serve_logs.py']
Ground Truth : ['a/airflow/utils.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Bring in more resolution to hivestats


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/sftp/operators/sftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_xcomargs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ftp/operators/ftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/common_options.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/webserver_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serde.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/kubernetes/pre_7_4_0_compatibility/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_generator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py']
Ground Truth : ['a/airflow/providers/amazon/aws/hooks/glue_crawler.py', 'a/airflow/providers/amazon/aws/hooks/glue.py', 'a/airflow/providers/amazon/aws/operators/glue.py', '/dev/null', 'a/airflow/providers/amazon/aws/hooks/base_aws.py', 'a/airflow/providers/amazon/aws/operators/glue_crawler.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Migrate elasticsearch client to 8.* (or latest 7.*)
### Body

The eleasticsearch version we use is old (from 2021). It is eleasticsearch 7.* and since we are using elasticsearch-dsl, we cannot used elasticsearch 8 currently (first version of that was also released in 2021). 

We are currently limited to < 7.15.0 additionally to being limited to 7, because 7.15.0 breaks our tests as we found out when we released some of the dependencies as part of https://github.com/apache/airflow/pull/30067

We should migrate to a newer version of elasticsearch. There are two options:

1) Fix the tests and migrate to lates 7.* version - and bump to latest released 7* by fixing our tests (possible for short term).
2) Migreate to 8 version (better long term)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/elasticsearch/log/es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_events_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py']
Ground Truth : ['a/airflow/providers/elasticsearch/log/es_task_handler.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: dag_processing code needs to handle OSError("handle is closed") in poll() and recv() calls
### Apache Airflow version

2.1.4

### What happened

The problem also exists in the latest version of the Airflow code, but I experienced it in 2.1.4.

This is the root cause of problems experienced in [issue#13542](https://github.com/apache/airflow/issues/13542).

I'll provide a stack trace below.  The problem is in the code of airflow/dag_processing/processor.py (and manager.py), all poll() and recv() calls to the multiprocessing communication channels need to be wrapped in exception handlers, handling OSError("handle is closed") exceptions.  If one looks at the Python multiprocessing source code, it throws this exception when the channel's handle has been closed.

This occurs in Airflow when a DAG File Processor has been killed or terminated; the Airflow code closes the communication channel when it is killing or terminating a DAG File Processor process (for example, when a dag_file_processor_timeout occurs).This killing or terminating happens asynchronously (in another process) from the process calling the poll() or recv() on the communication channel.  This is why an exception needs to be handled.  A pre-check of the handle being open is not good enough, because the other process doing the kill or terminate may close the handle in between your pre-check and actually calling poll() or recv() (a race condition).



### What you expected to happen

Here is the stack trace of the occurence I saw:

```
[2022-03-08 17:41:06,101] {taskinstance.py:914} DEBUG - <TaskInstance: staq_report_daily.gs.wait_staq_csv_file 2022-03-06 17:15:00+00:00 [running]> dependency 'Not In Retry Period' PASSED: True, The context specified that being in a retry p
eriod was permitted.
[2022-03-08 17:41:06,101] {taskinstance.py:904} DEBUG - Dependencies all met for <TaskInstance: staq_report_daily.gs.wait_staq_csv_file 2022-03-06 17:15:00+00:00 [running]>
[2022-03-08 17:41:06,119] {scheduler_job.py:1196} DEBUG - Skipping SLA check for <DAG: gdai_gcs_sync> because no tasks in DAG have SLAs
[2022-03-08 17:41:06,119] {scheduler_job.py:1196} DEBUG - Skipping SLA check for <DAG: unity_creative_import_process> because no tasks in DAG have SLAs
[2022-03-08 17:41:06,119] {scheduler_job.py:1196} DEBUG - Skipping SLA check for <DAG: sales_dm_to_bq> because no tasks in DAG have SLAs
[2022-03-08 17:44:50,454] {settings.py:302} DEBUG - Disposing DB connection pool (PID 1902)
Process ForkProcess-1:
Traceback (most recent call last):
  File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/python3.8/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 370, in _run_processor_manager
    processor_manager.start()
  File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 610, in start
    return self._run_parsing_loop()
  File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 671, in _run_parsing_loop
    self._collect_results_from_processor(processor)
  File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/manager.py", line 981, in _collect_results_from_processor
    if processor.result is not None:
  File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 321, in result
    if not self.done:
  File "/opt/python3.8/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 286, in done
    if self._parent_channel.poll():
  File "/opt/python3.8/lib/python3.8/multiprocessing/connection.py", line 255, in poll
    self._check_closed()
  File "/opt/python3.8/lib/python3.8/multiprocessing/connection.py", line 136, in _check_closed
    raise OSError("handle is closed")
OSError: handle is closed
```

This corresponded in time to the following log entries:

```
% kubectl logs airflow-scheduler-58c997dd98-n8xr8 -c airflow-scheduler --previous | egrep 'Ran scheduling loop in|[[]heartbeat[]]'
[2022-03-08 17:40:47,586] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.56 seconds
[2022-03-08 17:40:49,146] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.56 seconds
[2022-03-08 17:40:50,675] {base_job.py:227} DEBUG - [heartbeat]
[2022-03-08 17:40:50,687] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.54 seconds
[2022-03-08 17:40:52,144] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.46 seconds
[2022-03-08 17:40:53,620] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.47 seconds
[2022-03-08 17:40:55,085] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.46 seconds
[2022-03-08 17:40:56,169] {base_job.py:227} DEBUG - [heartbeat]
[2022-03-08 17:40:56,180] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.49 seconds
[2022-03-08 17:40:57,667] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.49 seconds
[2022-03-08 17:40:59,148] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.48 seconds
[2022-03-08 17:41:00,618] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.47 seconds
[2022-03-08 17:41:01,742] {base_job.py:227} DEBUG - [heartbeat]
[2022-03-08 17:41:01,757] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.58 seconds
[2022-03-08 17:41:03,133] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.55 seconds
[2022-03-08 17:41:04,664] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 0.53 seconds
[2022-03-08 17:44:50,649] {base_job.py:227} DEBUG - [heartbeat]
[2022-03-08 17:44:50,814] {scheduler_job.py:813} DEBUG - Ran scheduling loop in 225.15 seconds
```

You can see that when this exception occurred, there was a hang in the scheduler for almost 4 minutes, no scheduling loops, and no scheduler_job heartbeats.

This hang probably also caused stuck queued jobs as issue#13542 describes.

### How to reproduce

This is hard to reproduce because it is a race condition.  But you might be able to reproduce by having in a dagfile top-level code that calls sleep, so that it takes longer to parse than core dag_file_processor_timeout setting.  That would cause the parsing processes to be terminated, creating the conditions for this bug to occur.

### Operating System

NAME="Ubuntu" VERSION="18.04.6 LTS (Bionic Beaver)" ID=ubuntu ID_LIKE=debian PRETTY_NAME="Ubuntu 18.04.6 LTS" VERSION_ID="18.04" HOME_URL="https://www.ubuntu.com/" SUPPORT_URL="https://help.ubuntu.com/" BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/" PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic

### Versions of Apache Airflow Providers

Not relevant, this is a core dag_processing issue.

### Deployment

Composer

### Deployment details

"composer-1.17.6-airflow-2.1.4"

In order to isolate the scheduler to a separate machine, so as to not have interference from other processes such as airflow-workers running on the same machine, we created an additional node-pool for the scheduler, and ran these k8s patches to move the scheduler to a separate machine.

New node pool definition:
```HCL
    {
      name              = "scheduler-pool"
      machine_type      = "n1-highcpu-8"
      autoscaling       = false
      node_count        = 1
      disk_type         = "pd-balanced"
      disk_size         = 64
      image_type        = "COS"
      auto_repair       = true
      auto_upgrade      = true
      max_pods_per_node = 32
    },
```

patch.sh
```sh
#!/bin/bash
if [ $# -lt 1 ]; then
  echo "Usage: $0 namespace"
  echo "Description: Isolate airflow-scheduler onto it's own node-pool (scheduler-pool)."
  echo "Options:"
  echo "  namespace: kubernetes namespace used by Composer"
  exit 1
fi

namespace=$1

set -eu
set -o pipefail

scheduler_patch="$(cat airflow-scheduler-patch.yaml)"
fluentd_patch="$(cat composer-fluentd-daemon-patch.yaml)"

set -x

kubectl -n default patch daemonset composer-fluentd-daemon -p "${fluentd_patch}"
kubectl -n ${namespace} patch deployment airflow-scheduler -p "${scheduler_patch}"
```

composer-fluentd-daemon-patch.yaml
```yaml
spec:
  template:
    spec:
      nodeSelector: null
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: cloud.google.com/gke-nodepool
                operator: In
                values:
                - default-pool
                - scheduler-pool
```

 airflow-scheduler-patch.yaml
```yaml
spec:
  template:
    spec:
      nodeSelector:
        cloud.google.com/gke-nodepool: scheduler-pool
      containers:
      - name: gcs-syncd
        resources:
          limits:
            memory: 2Gi
```


### Anything else

On the below checkbox of submitting a PR, I could submit one, but it'd be untested code, I don't really have the environment setup to test the patch.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/flink/sensors/test_flink_kubernetes.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py']
Ground Truth : ['a/airflow/dag_processing/manager.py', 'a/airflow/dag_processing/processor.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Random "duplicate key value violates unique constraint" errors when initializing the postgres database
### Apache Airflow version

2.3.0 (latest released)

### What happened

while testing airflow 2.3.0 locally (using postgresql 12.4), the webserver container shows random errors:
```
webserver_1  | + airflow db init
...
webserver_1  | + exec airflow webserver
...
webserver_1  | [2022-05-04 18:58:46,011] {{manager.py:568}} INFO - Added Permission menu access on Permissions to role Admin
postgres_1   | 2022-05-04 18:58:46.013 UTC [41] ERROR:  duplicate key value violates unique constraint "ab_permission_view_role_permission_view_id_role_id_key"
postgres_1   | 2022-05-04 18:58:46.013 UTC [41] DETAIL:  Key (permission_view_id, role_id)=(204, 1) already exists.
postgres_1   | 2022-05-04 18:58:46.013 UTC [41] STATEMENT:  INSERT INTO ab_permission_view_role (id, permission_view_id, role_id) VALUES (nextval('ab_permission_view_role_id_seq'), 204, 1) RETURNING ab_permission_view_role.id
webserver_1  | [2022-05-04 18:58:46,015] {{manager.py:570}} ERROR - Add Permission to Role Error: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_role_permission_view_id_role_id_key"
webserver_1  | DETAIL:  Key (permission_view_id, role_id)=(204, 1) already exists.
webserver_1  |
webserver_1  | [SQL: INSERT INTO ab_permission_view_role (id, permission_view_id, role_id) VALUES (nextval('ab_permission_view_role_id_seq'), %(permission_view_id)s, %(role_id)s) RETURNING ab_permission_view_role.id]
webserver_1  | [parameters: {'permission_view_id': 204, 'role_id': 1}]
```

notes:
1. when the db is first initialized, i have ~40 errors like this (with ~40 different `permission_view_id` but always the same `'role_id': 1`)
2. when it's not the first time initializing db, i always have 1 error like this but it shows different `permission_view_id` each time
3. all these errors don't seem to have any real negative effects, the webserver is still running and airflow is still running and scheduling tasks
4. "occasionally" i do get real exceptions which render the webserver workers all dead:

```
postgres_1   | 2022-05-05 20:03:30.580 UTC [44] ERROR:  duplicate key value violates unique constraint "ab_permission_view_role_permission_view_id_role_id_key"
postgres_1   | 2022-05-05 20:03:30.580 UTC [44] DETAIL:  Key (permission_view_id, role_id)=(214, 1) already exists.
postgres_1   | 2022-05-05 20:03:30.580 UTC [44] STATEMENT:  INSERT INTO ab_permission_view_role (id, permission_view_id, role_id) VALUES (nextval('ab_permission_view_role_id_seq'), 214, 1) RETURNING ab_permission_view_role.id
webserver_1  | [2022-05-05 20:03:30 +0000] [121] [ERROR] Exception in worker process
webserver_1  | Traceback (most recent call last):
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_context
webserver_1  |     self.dialect.do_execute(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 716, in do_execute
webserver_1  |     cursor.execute(statement, parameters)
webserver_1  | psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "ab_permission_view_role_permission_view_id_role_id_key"
webserver_1  | DETAIL:  Key (permission_view_id, role_id)=(214, 1) already exists.
webserver_1  |
webserver_1  |
webserver_1  | The above exception was the direct cause of the following exception:
webserver_1  |
webserver_1  | Traceback (most recent call last):
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/gunicorn/arbiter.py", line 589, in spawn_worker
webserver_1  |     worker.init_process()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/gunicorn/workers/base.py", line 134, in init_process
webserver_1  |     self.load_wsgi()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/gunicorn/workers/base.py", line 146, in load_wsgi
webserver_1  |     self.wsgi = self.app.wsgi()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/gunicorn/app/base.py", line 67, in wsgi
webserver_1  |     self.callable = self.load()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py", line 58, in load
webserver_1  |     return self.load_wsgiapp()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py", line 48, in load_wsgiapp
webserver_1  |     return util.import_app(self.app_uri)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/gunicorn/util.py", line 412, in import_app
webserver_1  |     app = app(*args, **kwargs)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/airflow/www/app.py", line 158, in cached_app
webserver_1  |     app = create_app(config=config, testing=testing)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/airflow/www/app.py", line 146, in create_app
webserver_1  |     sync_appbuilder_roles(flask_app)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/airflow/www/app.py", line 68, in sync_appbuilder_roles
webserver_1  |     flask_app.appbuilder.sm.sync_roles()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/airflow/www/security.py", line 580, in sync_roles
webserver_1  |     self.update_admin_permission()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/airflow/www/security.py", line 562, in update_admin_permission
webserver_1  |     self.get_session.commit()
webserver_1  |   File "<string>", line 2, in commit
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1423, in commit
webserver_1  |     self._transaction.commit(_to_root=self.future)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 829, in commit
webserver_1  |     self._prepare_impl()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 808, in _prepare_impl
webserver_1  |     self.session.flush()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 3255, in flush
webserver_1  |     self._flush(objects)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 3395, in _flush
webserver_1  |     transaction.rollback(_capture_exception=True)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
webserver_1  |     compat.raise_(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
webserver_1  |     raise exception
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 3355, in _flush
webserver_1  |     flush_context.execute()
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py", line 453, in execute
webserver_1  |     rec.execute(self)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py", line 576, in execute
webserver_1  |     self.dependency_processor.process_saves(uow, states)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/dependency.py", line 1182, in process_saves
webserver_1  |     self._run_crud(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/orm/dependency.py", line 1245, in _run_crud
webserver_1  |     connection.execute(statement, secondary_insert)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1200, in execute
webserver_1  |     return meth(self, multiparams, params, _EMPTY_EXECUTION_OPTS)
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py", line 313, in _execute_on_connection
webserver_1  |     return connection._execute_clauseelement(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1389, in _execute_clauseelement
webserver_1  |     ret = self._execute_context(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1748, in _execute_context
webserver_1  |     self._handle_dbapi_exception(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1929, in _handle_dbapi_exception
webserver_1  |     util.raise_(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
webserver_1  |     raise exception
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_context
webserver_1  |     self.dialect.do_execute(
webserver_1  |   File "/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 716, in do_execute
webserver_1  |     cursor.execute(statement, parameters)
webserver_1  | sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint "ab_permission_view_role_permission_view_id_role_id_key"
webserver_1  | DETAIL:  Key (permission_view_id, role_id)=(214, 1) already exists.
webserver_1  |
webserver_1  | [SQL: INSERT INTO ab_permission_view_role (id, permission_view_id, role_id) VALUES (nextval('ab_permission_view_role_id_seq'), %(permission_view_id)s, %(role_id)s) RETURNING ab_permission_view_role.id]
webserver_1  | [parameters: {'permission_view_id': 214, 'role_id': 1}]
webserver_1  | (Background on this error at: http://sqlalche.me/e/14/gkpj)
webserver_1  | [2022-05-05 20:03:30 +0000] [121] [INFO] Worker exiting (pid: 121)
flower_1     | + exec airflow celery flower
scheduler_1  | + exec airflow scheduler
webserver_1  | [2022-05-05 20:03:31 +0000] [118] [INFO] Worker exiting (pid: 118)
webserver_1  | [2022-05-05 20:03:31 +0000] [119] [INFO] Worker exiting (pid: 119)
webserver_1  | [2022-05-05 20:03:31 +0000] [120] [INFO] Worker exiting (pid: 120)
worker_1     | + exec airflow celery worker
```
However such exceptions are rare and pure random, i can't find a way to reproduce them consistently.

### What you think should happen instead

prior to 2.3.0 there were no such errors 

### How to reproduce

_No response_

### Operating System

Linux Mint 20.3

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_provider_yaml_files_check.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py']
Ground Truth : ['a/airflow/www/fab_security/manager.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Several statsd timers record seconds rather than milliseconds
**Apache Airflow version**: 1.10.11

**What happened**:

Statsd timers for 3 categories are reporting values 1000x off as the code passes seconds, rather rather than milliseconds (or a `timedelta()` instance) to [`StatsClient.timing()`](https://statsd.readthedocs.io/en/v3.3/reference.html#StatsClient.timing).

**What you expected to happen**:

The timings should be reported in milliseconds.

**How to reproduce it**:

Enable the `statsd` client and track reporting timings for:

* `dag.loading-duration.*` (emitted from `models/dagbag.py`)
* `dag.<dag_id>.<task_id>.duration` (emitted from `models/taskinstance.py`)
* `dag_processing.last_duration.<filename>` (emitted from `utils/dag_processing.py`) and it's deprecated alias `dag_processing.last_runtime.*`

**Anything else we need to know**:

This was reported before, for the `dag_processing.last_*` stats, at https://issues.apache.org/jira/browse/AIRFLOW-6088.
The accompanying pull request #6682, fixed 2 out of 3 of these, but only for Airflow 2.0.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/metrics/protocols.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/utils/waiter.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py']
Ground Truth : ['a/airflow/utils/dag_processing.py', 'a/airflow/models/dagrun.py', 'a/airflow/models/taskinstance.py', 'a/airflow/models/dagbag.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Json Parse Error in Tree View
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 1.10.10

**What happened**:
When DAG is triggered with custom configuration that contains escape characters in a json variable value, tree view will fail to load due to json parse error at https://github.com/apache/airflow/blob/master/airflow/www/templates/airflow/tree.html#L139

**What you expected to happen**:
Tree view should load

<!-- What do you think went wrong? -->

**How to reproduce it**:
Create any simple dag and trigger with api call or on UI and provide following as configuration.
```
{
     "abc": "this is simple message\n"
}
```

The new logic in tree view will turn above json into string, then trying to parse it out.
<img width="392" alt="Screen Shot 2020-04-22 at 6 44 11 PM" src="https://user-images.githubusercontent.com/1458754/80041001-4b02f780-84c9-11ea-97ff-44e78a4b774e.png">


<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md sytle of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**:
Introduce in https://github.com/apache/airflow/pull/7492

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: show_dag (save) feature is not supporting any other orientation except 'LR'
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
This questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: master

**What happened**: I have DAG in TB orientation and when I tried to save that DAG graph to png file, it is not honoring the orientation, it is taking in 'LR' orientation only.

<!-- (please include exact error messages if you can) -->

**What you expected to happen**: The graph in the png file should be similar to the orientation of the DAG.

<!-- What do you think went wrong? -->

**How to reproduce it**: Change the orientation of the DAG other than 'LR' and try to save the graph.
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md sytle of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py']
Ground Truth : ['a/airflow/utils/dot_renderer.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Abort a DAG Run
**Description**

It would be great having a option to abort a DAG Runs through the REST API.

**Use case / motivation**

The proposed input params would be:
 - DAG_ID
 - DAG_RUN_ID

The DAG Run should abort all its tasks running and mark them as "failed".

**Are you willing to submit a PR?**

**Related Issues**



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/mark_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/serve_logs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_storage_transfer_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py']
Ground Truth : ['a/airflow/api_connexion/endpoints/dag_run_endpoint.py', 'a/airflow/api_connexion/schemas/dag_run_schema.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: dag.cli should detect DAG cycles
**Description**

I wish `dag.cli()` reported cycles in a task graph.


**Use case / motivation**

We use Airflow (now 2.1.1), with about 40 DAGs authored by many people, with daily changes, and put our DAGs into custom docker image that we deploy with flux.

However, I noticed that a lot of commits from our developers, are a lot of small fixes, because it is tricky to test DAGs locally (especially if one uses plugins, which we don't anymore).

So I wrote a script that does import every dag file, runs it, and calls `dag.cli()`, and I then list all tasks, and run a test --dry_run on each task. That proved to be a super useful script, that can detect a lot of issues (malformed imports, syntax errors, typos in jinja2 templates, uses of unitialized variables, task id name collisions, and so on), before the change is even commited to our git repo, docker image is build, and deployed. Thus making iteration speed faster.

However, I noticed that `dag.cli()` does not detect cycles in a task graph.

Example:

```python3
from pprint import pprint

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

def print_context(ds, **kwargs):
    """Print the Airflow context and ds variable from the context."""
    pprint(kwargs)
    print(ds)
    return 'Whatever you return gets printed in the logs'

with DAG(
    dag_id="test_dag1",
    description="Testing",
    schedule_interval="@daily",
    catchup=False,
    start_date=days_ago(2),
) as dag:
    a = PythonOperator(
        task_id='print_the_context1',
        python_callable=print_context,
    )
    b = PythonOperator(
        task_id='print_the_context2',
        python_callable=print_context,
    )

    a >> b
    b >> a

if __name__ == '__main__':
    dag.cli()
```


Now running:

```
$ python3 dags/primary/examples/tutorial_cycles.py tasks list
print_the_context1
print_the_context2
$
```

```
$ python3 dags/primary/examples/tutorial_cycles.py tasks test --dry-run print_the_context2 '2021-07-19T00:00:00+0000'
[2021-07-19 10:37:27,513] {baseoperator.py:1263} INFO - Dry run
$ 
```

No warnings.

When running a dag using a scheduler, it eventually detects a cycle (not sure if on load, or only when executing it, or reaching a specific task), but that is a bit too late.

I wonder if it is possible to make `dag.cli()` detect cycles? It might also be possible to detect cycles even earlier, when adding DAG edges, but that might be too slow to do on every call. However, I am pretty sure dag.cli() could do it efficiently, as it does have a full graph available. (There are well known linear algorithms based on DFS that detect cycles).

Just now, I noticed that there is method `dag.topological_sort()`, that is quite handy, and will detect cycles, so if I add:

```python3
if __name__ == '__main__':
    dag.topological_sort()
    dag.cli()
```

It does detect a cycle:

```
Traceback (most recent call last):
  File "/home/witek/code/airflow/dags/primary/examples/tutorial_cycles.py", line 33, in <module>
    print(dag.topological_sort())
  File "/home/witek/airflow-testing/venv/lib/python3.9/site-packages/airflow/models/dag.py", line 1119, in topological_sort
    raise AirflowException(f"A cyclic dependency occurred in dag: {self.dag_id}")
airflow.exceptions.AirflowException: A cyclic dependency occurred in dag: test_dag1
```


I think it might be useful to add `topological_sort`  (and `tree_view`) to be accessible via `dag.cli()`, so the external script can easily detect cycles this way.

I also noticed that calling `dag.treeview()` does not detect cycle. In fact it does not print anything when there is a cycle.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_python_operator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagbag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/serialization/serialized_objects.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py']
Ground Truth : ['a/airflow/utils/dag_cycle_tester.py', 'a/airflow/models/dag.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: [AIRFLOW-3014] Fix multiple alembic heads
Make sure you have checked _all_ steps below.

### Jira

- [ ] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, "\[AIRFLOW-XXX\] My Airflow PR"
  - https://issues.apache.org/jira/browse/AIRFLOW-6224
  - In case you are fixing a typo in the documentation you can prepend your commit with \[AIRFLOW-XXX\], code changes always need a Jira issue.
  - In case you are proposing a fundamental code change, you need to create an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)).
  - In case you are adding a dependency, check if the license complies with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).

### Description

- [ ] Here are some details about my PR, including screenshots of any UI changes:

### Tests

- [ ] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:

### Commits

- [ ] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from "[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)":
  1. Subject is separated from body by a blank line
  1. Subject is limited to 50 characters (not including Jira issue reference)
  1. Subject does not end with a period
  1. Subject uses the imperative mood ("add", not "adding")
  1. Body wraps at 72 characters
  1. Body explains "what" and "why", not "how"

### Documentation

- [ ] In case of new functionality, my PR adds documentation that describes how to use it.
  - All the public functions and the classes in the PR contain docstrings that explain what it does
  - If you implement backwards incompatible changes, please leave a note in the [Updating.md](https://github.com/apache/airflow/blob/master/UPDATING.md) so we can assign it to a appropriate release


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/docker_tests_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_candidate_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/utils/mlengine_operator_utils.py']
Ground Truth : ['a/airflow/migrations/versions/c1840b4bcf1a_increase_length_of_password_column_in_.py']
Current Recall: 0.08520792198522392

=========================================================

ISSUE: Separate out tableau provider from salesforce
**Description**

Tableau provider is currently embedded into Salesforce, but it should be removed to a separate 'tableau' provider.. This should lead to:

* 1.0.1 version of the Salesforce provider 
* 1.0.0 version of the Tableau provider

Seems that Salesforce provider might be backwards-compatible with deprecation warnings:

* The Tableau classes from the Salesforce provider should be moved to Tableau
* The old Tableau classes in the Salesforce provider should get a deprecation warning and import the classes from the Tableau one
* The Salesforce provider should have a  "hard" dependency on "tableau" provider  (no extra needed)

**Use case / motivation**

Dicussion [here](https://lists.apache.org/thread.html/rb1828485df78df7649000ff586a749ab685635f606b394386e1be808%40%3Cdev.airflow.apache.org%3E) has shown that the tableau provider is really independent from Salesforce and we have already similar examples (slack for example). Also it makes sense not to change provider structure when ownership of a service changes.

**Related Issues**

#13595

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/hooks/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/salesforce/hooks/test_salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/operators/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/tableau/hooks/test_tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/tableau/sensors/tableau.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/hooks/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/tableau/example_tableau.py']
Ground Truth : ['a/setup.py', '/dev/null', 'a/airflow/providers/salesforce/sensors/tableau_job_status.py', 'a/airflow/providers/salesforce/example_dags/example_tableau_refresh_workbook.py', 'a/airflow/providers/salesforce/operators/tableau_refresh_workbook.py', 'a/airflow/providers/salesforce/hooks/tableau.py']
Current Recall: 0.08556480992241164

=========================================================

ISSUE: Don't include upstream_failed in the 'active task instances' test in the scheduler
**Apache Airflow version**: 2.0.0a1

**What happened**:

On my dev environment I have a lot of DAG runs with failed tasks, where the *downstream* task instances then have been set to `upstream_failed`. The scheduler is now refusing to schedule more instances for such DAGs.

The scheduler looks for such task instances that are not in the `State.finished` set:

https://github.com/apache/airflow/blob/db3fe0926bb75008311eed804052c90bfa912424/airflow/jobs/scheduler_job.py#L1482-L1488

However, `State.finished` is currently not a proper inverse of *running* states, it only includes success, failed, and skipped:

https://github.com/apache/airflow/blob/db3fe0926bb75008311eed804052c90bfa912424/airflow/utils/state.py#L110-L114

What is missing is the task instance `upstream_failed` state, so currently my log is being filled (very rapidly) with:

```
[2020-10-18 13:22:18,503] {{scheduler_job.py:1658}} INFO - DAG test_dag already has 26 active runs, not queuing any more tasks
```

Those 26 'active runs' are simply dag runs that failed and have 'upstream_failed' task instances.

Either `upstream_failed` should be added to `State.finished`, or the Scheduler task instance test should explicitly take that state into account.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/state.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/deps/trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/ti_deps/dep_context.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api/common/mark_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/ti_deps/deps/test_trigger_rule_dep.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py']
Ground Truth : ['a/airflow/ti_deps/dep_context.py', 'a/airflow/models/dagrun.py', 'a/airflow/utils/state.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: Scheduler pod hang when K8s API call fail
### Apache Airflow version

Other Airflow 2 version (please specify below)

### What happened

Airflow version: `2.3.4`

I have deployed airflow with the official Helm in K8s with `KubernetesExecutor`. Sometimes the scheduler hang when calling  K8s API. The log:
``` bash
ERROR - Exception when executing Executor.end
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 752, in _execute
    self._run_scheduler_loop()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 842, in _run_scheduler_loop
    self.executor.heartbeat()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/executors/base_executor.py", line 171, in heartbeat
    self.sync()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/executors/kubernetes_executor.py", line 649, in sync
    next_event = self.event_scheduler.run(blocking=False)
  File "/usr/local/lib/python3.8/sched.py", line 151, in run
    action(*argument, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/event_scheduler.py", line 36, in repeat
    action(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/executors/kubernetes_executor.py", line 673, in _check_worker_pods_pending_timeout
    for pod in pending_pods().items:
  File "/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py", line 15697, in list_namespaced_pod
    return self.list_namespaced_pod_with_http_info(namespace, **kwargs)  # noqa: E501
  File "/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py", line 15812, in list_namespaced_pod_with_http_info
    return self.api_client.call_api(
  File "/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py", line 348, in call_api
    return self.__call_api(resource_path, method,
  File "/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py", line 180, in __call_api
    response_data = self.request(
  File "/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py", line 373, in request
    return self.rest_client.GET(url,
  File "/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/rest.py", line 240, in GET
    return self.request("GET", url,
  File "/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/rest.py", line 213, in request
    r = self.pool_manager.request(method, url,
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/request.py", line 74, in request
    return self.request_encode_url(
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/request.py", line 96, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/poolmanager.py", line 376, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py", line 815, in urlopen
    return self.urlopen(
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/connectionpool.py", line 1042, in _validate_conn
    conn.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/connection.py", line 358, in connect
    self.sock = conn = self._new_conn()
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "/home/airflow/.local/lib/python3.8/site-packages/urllib3/util/connection.py", line 85, in create_connection
    sock.connect(sa)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 182, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/scheduler_job.py", line 773, in _execute
    self.executor.end()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/executors/kubernetes_executor.py", line 823, in end
    self._flush_task_queue()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/executors/kubernetes_executor.py", line 776, in _flush_task_queue
    self.log.debug('Executor shutting down, task_queue approximate size=%d', self.task_queue.qsize())
  File "<string>", line 2, in qsize
  File "/usr/local/lib/python3.8/multiprocessing/managers.py", line 835, in _callmethod
    kind, result = conn.recv()
  File "/usr/local/lib/python3.8/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/usr/local/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/usr/local/lib/python3.8/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
```
Then the executor process was killed and the pod was still running. But the scheduler does not work.

After restarting, the scheduler worked usually.

### What you think should happen instead

When the error occurs, the executor needs to auto restart or the scheduler should be killed. 

### How to reproduce

_No response_

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/utils/pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/build_docs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py']
Ground Truth : ['a/airflow/executors/kubernetes_executor.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: Airflow doesn't re-use a secrets backend instance when loading configuration values
### Apache Airflow version

main (development)

### What happened

When airflow is loading its configuration, it creates a new secrets backend instance for each configuration backend it loads from secrets and then additionally creates a global secrets backend instance that is used in `ensure_secrets_loaded` which code outside of the configuration file uses. This can cause issues with the vault backend (and possibly others, not sure) since logging in to vault can be an expensive operation server-side and each instance of the vault secrets backend needs to re-login to use its internal client.

### What you think should happen instead

Ideally, airflow would attempt to create a single secrets backend instance and re-use this. This can possibly be patched in the vault secrets backend, but instead I think updating the `configuration` module to cache the secrets backend would be preferable since it would then apply to any secrets backend.

### How to reproduce

Use the hashicorp vault secrets backend and store some configuration in `X_secret` values. See that it logs in more than you'd expect.

### Operating System

Ubuntu 18.04

### Versions of Apache Airflow Providers

```
apache-airflow==2.3.0
apache-airflow-providers-hashicorp==2.2.0
hvac==0.11.2
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/secrets/key_vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/systems_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/secrets/secret_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/yandex/secrets/lockbox.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/variable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/helm_tests/airflow_aux/test_basic_helm_chart.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_configuration.py']
Ground Truth : ['a/airflow/configuration.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: Add 'mongo_collection' to template_fields in MongoToS3Operator
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

**Description**

<!-- A short description of your feature -->

Make `MongoToS3Operator` `mongo_collection` parameter templated.

**Use case / motivation**

<!-- What do you want to happen?

Rather than telling us how you might implement this solution, try to take a
step back and describe what you are trying to achieve.

-->

This would allow for passing a templated mongo collection from other tasks, such as a mongo collection used as data destination by using `S3Hook`. For instance, we could use templated mongo collection to write data for different dates in different collections by using: `mycollection.{{ ds_nodash }}`.

**Are you willing to submit a PR?**

<!--- We accept contributions! -->

Yes.

**Related Issues**

<!-- Is there currently another issue associated with this? -->

N/A


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/mongo_to_s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/salesforce/hooks/salesforce.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py']
Ground Truth : ['a/airflow/providers/amazon/aws/transfers/mongo_to_s3.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: Refactor OpsgenieAlertHook to official python sdk
### Body

Currently [OpsgenieAlertHook](https://github.com/apache/airflow/blob/d79f506213297dc0dc034d6df3226361b6f95d7a/airflow/providers/opsgenie/hooks/opsgenie_alert.py#L29) works with HTTP.
We should change the hook to work with the [official python sdk](https://github.com/opsgenie/opsgenie-python-sdk)

This will allow also to easily integrate to airflow the various methods and options the SDK has to offer
https://github.com/opsgenie/opsgenie-python-sdk#documentation-for-api-endpoints

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/opsgenie/hooks/opsgenie.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/opsgenie/operators/opsgenie.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/opsgenie/notifications/opsgenie.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/opsgenie/hooks/test_opsgenie.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/facebook_ads_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/github/operators/github.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack_webhook.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/atlassian/jira/operators/jira.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/beam/operators/beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/google/cloud/hooks/test_looker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/perf/perf_kit/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/operators/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py']
Ground Truth : ['a/setup.py', '/dev/null', 'a/airflow/providers/opsgenie/operators/opsgenie_alert.py', 'a/airflow/providers/opsgenie/hooks/opsgenie_alert.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: BigQueryGetDataOperator does not support passing project_id
### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-google==8.3.0

### Apache Airflow version

2.3.2

### Operating System

MacOS

### Deployment

Other

### Deployment details

_No response_

### What happened

Can not actively pass project_id as an argument when using `BigQueryGetDataOperator`. This operator internally fallbacks into `default` project id.

### What you think should happen instead

Should let developers pass project_id when needed

### How to reproduce

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/hyperparameter_tuning_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/auto_ml.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/cloud_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/datafusion.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/bigquery_to_gcs.py']
Ground Truth : ['a/airflow/providers/google/cloud/operators/bigquery.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: EmptyOperator in dynamically mapped TaskGroups does not respect upstream dependencies
### Apache Airflow version

2.6.2

### What happened

When using an EmptyOperator in dynamically mapped TaskGroups (https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#mapping-over-a-task-group), the EmptyOperator of all branches starts as soon as the first upstream task dependency of the EmptyOperator **in any branch** completes. This causes downstream tasks of the EmptyOperator to start prematurely in all branches, breaking depth-first execution of the mapped TaskGroup.

I have provided a test for this behavior below, by introducing an artificial wait time in a `variable_task`, followed by an `EmptyOperator` in `checkpoint` and a `final` dependent task . 
![image](https://github.com/apache/airflow/assets/97735/e9d202fa-9b79-4766-b778-a8682a891050)


Running this test, during the execution I see this: The `checkpoint` and `final` tasks are already complete, while the upstream `variable_task` in the group is still running.
![image](https://github.com/apache/airflow/assets/97735/ad335ab5-ee91-4e91-805b-69b58e9bcd99)
I have measured the difference of time when of each the branches' `final` tasks execute, and compared them, to cause a failure condition, which you can see failing here in the `assert_branch_waited` task.

By using just a regular Task, one gets the correct behavior.

### What you think should happen instead

In each branch separately, the `EmptyOperator` should wait for its dependency to complete, before it starts. This would be the same behavior as using a regular `Task` for `checkpoint`.

### How to reproduce

Here are test cases in two dags, one with an `EmptyOperator`, showing incorrect behavior, one with a `Task` in sequence instead of the `EmptyOperator`, that has correct behavior.

```python
import time
from datetime import datetime

from airflow import DAG
from airflow.decorators import task, task_group
from airflow.models import TaskInstance
from airflow.operators.empty import EmptyOperator

branches = [1, 2]
seconds_difference_expected = 10

for use_empty_operator in [False, True]:
    dag_id = "test-mapped-group"
    if use_empty_operator:
        dag_id += "-with-emptyoperator"
    else:
        dag_id += "-no-emptyoperator"

    with DAG(
        dag_id=dag_id,
        schedule=None,
        catchup=False,
        start_date=datetime(2023, 1, 1),
        default_args={"retries": 0},
    ) as dag:

        @task_group(group_id="branch_run")
        def mapped_group(branch_number):
            """Branch 2 will take > `seconds_difference_expected` seconds, branch 1 will be immediately executed"""

            @task(dag=dag)
            def variable_task(branch_number):
                """Waits `seconds_difference_expected` seconds for branch 2"""
                if branch_number == 2:
                    time.sleep(seconds_difference_expected)
                return branch_number

            variable_task_result = variable_task(branch_number)

            if use_empty_operator:
                # emptyoperator as a checkpoint
                checkpoint_result = EmptyOperator(task_id="checkpoint")
            else:

                @task
                def checkpoint():
                    pass

                checkpoint_result = checkpoint()

            @task(dag=dag)
            def final(ti: TaskInstance = None):
                """Return the time at the task execution"""
                return datetime.now()

            final_result = final()
            variable_task_result >> checkpoint_result >> final_result

            return final_result

        @task(dag=dag)
        def assert_branch_waited(times):
            """Check that the difference of the start times of the final step in each branch
            are at least `seconds_difference_expected`, i.e. the branch waited for all steps
            """
            seconds_difference = abs(times[1] - times[0]).total_seconds()
            if not seconds_difference >= seconds_difference_expected:
                raise RuntimeError(
                    "Branch 2 completed too fast with respect to branch 1: "
                    + f"observed [seconds difference]: {seconds_difference}; "
                    + f"expected [seconds difference]: >= {seconds_difference_expected}"
                )

        mapping_results = mapped_group.expand(branch_number=branches)
        assert_branch_waited(mapping_results)
```

### Operating System

Debian GNU/Linux 11 (bullseye) on docker (official image)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/serialization/test_dag_serialization.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_task_group.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_backfill_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/abstractoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py']
Ground Truth : ['a/airflow/models/dagrun.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: Send default email if file "html_content_template" not found
### Apache Airflow version

2.3.2 (latest released)

### What happened

I created a new email template to be sent when there are task failures. I accidentally added the path to the `[email] html_content_template` and `[email] subject_template` with a typo and no email was sent. The task's log is the following:

```
Traceback (most recent call last):
  File "/home/user/.conda/envs/airflow/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1942, in handle_failure
    self.email_alert(error, task)
  File "/home/user/.conda/envs/airflow/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 2323, in email_alert
    subject, html_content, html_content_err = self.get_email_subject_content(exception, task=task)
  File "/home/user/.conda/envs/airflow/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 2315, in get_email_subject_content
    subject = render('subject_template', default_subject)
  File "/home/user/.conda/envs/airflow/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 2311, in render
    with open(path) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/user/airflow/config/templates/email_failure_subject.tmpl'
```

I've looked the TaskInstance class (https://github.com/apache/airflow/blob/main/airflow/models/taskinstance.py).

I've seen that the `render` function (https://github.com/apache/airflow/blob/bcf2c418d261c6244e60e4c2d5de42b23b714bd1/airflow/models/taskinstance.py#L2271) has a `content` parameter, which is not used inside.

I guess the solution to this bug is simple: just add a `try - catch` block and return the default content in the `catch` part.

### What you think should happen instead

_No response_

### How to reproduce

_No response_

### Operating System

CentOS Linux 8

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

Conda environment

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/dag_processing/test_processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py']
Ground Truth : ['a/airflow/models/taskinstance.py']
Current Recall: 0.08627858579678709

=========================================================

ISSUE: AIP-56 - FAB AM - Logout
Move the logout logic to the auth manager

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/views/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/auth_manager/views/test_auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/tests/test_aws_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_user_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_session.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow-providers-fab/img/diagram_fab_auth_manager_airflow_architecture.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/registry.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_role_and_permission_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_setup_teardown_taskflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/decorators/test_setup_teardown.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py']
Ground Truth : ['a/airflow/www/extensions/init_security.py', 'a/airflow/auth/managers/fab/fab_auth_manager.py', 'a/airflow/auth/managers/base_auth_manager.py', 'a/airflow/www/auth.py', 'a/airflow/www/extensions/init_appbuilder.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Allow for specification of cipher in SftpHook
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

**Description**

I want to specify the cipher to use when creating a SftpHook. Currently this is not possible because there is not a way to propagate the value to CnOpts. 

https://github.com/apache/airflow/blob/master/airflow/providers/sftp/hooks/sftp.py
https://pysftp.readthedocs.io/en/release_0.2.9/pysftp.html?#pysftp.CnOpts
https://pysftp.readthedocs.io/en/release_0.2.9/cookbook.html

**Use case / motivation**

<!-- What do you want to happen?

Rather than telling us how you might implement this solution, try to take a
step back and describe what you are trying to achieve.

-->

I want to be able to access SFTP servers that have disabled access from connection with weak ciphers.


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/logging_mixin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/secrets/secrets_manager.py']
Ground Truth : ['a/airflow/providers/sftp/hooks/sftp.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Object of type V1Pod is not JSON serializable after detecting zombie jobs cause Scheduler CrashLoopBack
### Apache Airflow version

2.4.2

### What happened

Some dags have tasks with pod_override in executor_config become zombie tasks. Airflow Scheduler run and crash with exception:

```
[2022-11-10T15:29:59.886+0000] {scheduler_job.py:1526} ERROR - Detected zombie job: {'full_filepath': '/opt/airflow/dags/path.py', 'processor_subdir': '/opt/airflow/dags', 'msg': "{'DAG Id': 'dag_id', 'Task Id': 'taskid', 'Run Id': 'manual__2022-11-10T10:21:25.330307+00:00', 'Hostname': 'hostname'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7fde9c91dcd0>, 'is_failure_callback': True}
[2022-11-10T15:29:59.887+0000] {scheduler_job.py:763} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py", line 746, in _execute
    self._run_scheduler_loop()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py", line 878, in _run_scheduler_loop
    next_event = timers.run(blocking=False)
  File "/usr/local/lib/python3.7/sched.py", line 151, in run
    action(*argument, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/event_scheduler.py", line 37, in repeat
    action(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py", line 1527, in _find_zombies
    self.executor.send_callback(request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/executors/base_executor.py", line 400, in send_callback
    self.callback_sink.send(request)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/callbacks/database_callback_sink.py", line 34, in send
    db_callback = DbCallbackRequest(callback=callback, priority_weight=10)
  File "<string>", line 4, in __init__
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/state.py", line 480, in _initialize_instance
    manager.dispatch.init_failure(self, args, kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 72, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/state.py", line 477, in _initialize_instance
    return manager.original_init(*mixed[1:], **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/db_callback_request.py", line 46, in __init__
    self.callback_data = callback.to_json()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/callbacks/callback_requests.py", line 89, in to_json
    return json.dumps(dict_obj)
  File "/usr/local/lib/python3.7/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/local/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/local/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type V1Pod is not JSON serializable
``` 

### What you think should happen instead

DbCallbackRequest should do to_json successfully

### How to reproduce

Start airflow with KubernetesExecutor
Make zombie task.

### Operating System

docker.io/apache/airflow:2.4.2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/info_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/standalone_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py']
Ground Truth : ['a/airflow/exceptions.py', 'a/airflow/models/taskinstance.py', 'a/airflow/serialization/enums.py', 'a/airflow/callbacks/callback_requests.py', 'a/airflow/serialization/serialized_objects.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Unable to change username while updating user information through API
### Apache Airflow version

2.2.0b2 (beta snapshot)

### Operating System

Debian buster

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

Astro dev start

### What happened

Unable to change username while updating user information through API though it's possible from UI.

### What you expected to happen

Either username should not be editable from UI or It should be editable from API.

### How to reproduce

1. Use a patch request with the below URL.
`{{url}}/users/{{username}}`
2. In payload use a different username
```
 {
        "username": "{{username1}}",
        "password": "password1",
        "email": "{{email}}@example.com",
        "first_name": "{{$randomFirstName}}",
        "last_name": "{{$randomLastName}}",
        "roles":[{ "name": "Op"}]
}
```


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

"Already exists." message is missing while updating user email with existing email id through API
### Apache Airflow version

2.2.0b2 (beta snapshot)

### Operating System

Debian buster

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

Astro dev start

### What happened

"Already exists." error message is missing while updating user email with existing email id through API.

### What you expected to happen

"Email id already exists" error message should appear

### How to reproduce

1. Use a patch request with the below URL.
`{{url}}/users/{{username}}`
2. In payload use an exiting email id 
```
 {
        "username": "{{username}}",
        "password": "password1",
        "email": "{{exiting_email}}@example.com",
        "first_name": "{{$randomFirstName}}",
        "last_name": "{{$randomLastName}}",
        "roles":[{ "name": "Op"}]
}
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_user_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/cli_commands/test_user_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/cli_commands/user_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/api_endpoints/user_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_acl.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/docker/hooks/test_docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/cli_commands/definition.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/api_connexion_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/api_connexion/endpoints/user_endpoint.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Add retry + timeout to Airflow db check
### Description

In my company usage of Airflow, developmental instances of Airflow run on containerized PostgreSQL that are spawned at the same time the Airflow container is spawned. Before the Airflow container runs its initialization scripts, it needs to make sure that the PostgreSQL instance can be reached, for which `airflow db check` is a great option.

However, there is a non-deterministic race condition between the PSQL container and Airflow containers (not sure which will each readiness first and by how much), calling the `airflow db check` command once is not sufficient, and implementing a retry-timeout in shell script is feasible but unpleasant.

It would be great if the `airflow db check` command can take two additional optional arguments: `--retry` and `--retry-delay` (just like with `curl`) so that the database connection can be checked repeatedly for up to a specified number of times. This command should exit with `0` exit code if any of the retries succeeds, and `1` if all of the retries failed.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py']
Ground Truth : ['a/airflow/cli/commands/db_command.py', 'a/airflow/cli/cli_config.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: resolve web ui views warning re DISTINCT ON
### Body

Got this warning in webserver output when loading home page

```
/Users/dstandish/code/airflow/airflow/www/views.py:710 SADeprecationWarning: DISTINCT ON is currently supported only by the PostgreSQL dialect.  Use of DISTINCT ON for other backends is currently silently ignored, however this usage is deprecated, and will raise CompileError in a future release for all backends that do not support this syntax.
```
looks like it's this line
```
            dagtags = session.query(DagTag.name).distinct(DagTag.name).all()
```

may be able to change to `func.distinct` 



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/sqlalchemy.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py']
Ground Truth : ['a/airflow/models/taskinstance.py', 'a/airflow/configuration.py', 'a/airflow/migrations/versions/0062_2_0_0_add_dagrun_run_type.py', 'a/airflow/utils/db.py', 'a/airflow/www/fab_security/sqla/manager.py', 'a/airflow/example_dags/example_python_operator.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/migrations/versions/0068_2_0_0_drop_kuberesourceversion_and_.py', 'a/airflow/example_dags/example_datasets.py', 'a/airflow/utils/context.py', 'a/airflow/migrations/versions/0001_1_5_0_current_schema.py', 'a/airflow/www/extensions/init_wsgi_middlewares.py', 'a/setup.py', 'a/airflow/www/utils.py', 'a/airflow/www/extensions/init_security.py', 'a/airflow/www/views.py', 'a/airflow/migrations/versions/0028_1_10_0_add_kubernetes_resource_checkpointing.py', 'a/airflow/jobs/backfill_job.py', 'a/airflow/example_dags/example_branch_datetime_operator.py', 'a/airflow/compat/sqlalchemy.py', 'a/airflow/www/fab_security/manager.py', 'a/airflow/utils/sqlalchemy.py', 'a/scripts/ci/pre_commit/pre_commit_supported_versions.py', 'a/airflow/migrations/versions/0058_1_10_13_increase_length_of_fab_ab_view_menu_.py', 'a/airflow/models/xcom_arg.py', 'a/airflow/utils/log/file_task_handler.py', 'a/airflow/migrations/versions/0057_1_10_13_add_fab_tables.py', 'a/airflow/decorators/python.py', 'a/airflow/migrations/versions/0065_2_0_0_update_schema_for_smart_sensor.py', 'a/airflow/migrations/versions/0059_2_0_0_drop_user_and_chart.py', 'a/airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py', 'a/airflow/utils/json.py', 'a/airflow/migrations/versions/0023_1_8_2_add_max_tries_column_to_task_instance.py', 'a/airflow/www/app.py', 'a/airflow/models/abstractoperator.py', 'a/airflow/models/baseoperator.py', 'a/airflow/ti_deps/deps/trigger_rule_dep.py', 'a/airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py', 'a/airflow/jobs/scheduler_job.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Production image has only amazon and google providers installed
When "production" image is prepared for, only amazon and google providers are installed from sources.

**Apache Airflow version**:

master

**What you expected to happen**:

All providers should be installed

**How to reproduce it**:

```
./breeze --production-image --python 3.6
```

Then:

```
./breeze --production-image --python 3.6 shell bash
```

then

```
ls -la ~/.local/lib/python3.6/site-packages/airflow/providers/

amazon
google

```


UPDATE:

They are not even installed: 

```
.
./amazon
./amazon/aws
./amazon/aws/hooks
./amazon/aws/hooks/batch_waiters.json
./google
./google/cloud
./google/cloud/example_dags
./google/cloud/example_dags/example_bigquery_query.sql
./google/cloud/example_dags/example_cloud_build.yaml
./google/cloud/example_dags/example_spanner.sql
```


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_selective_checks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vision.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/test_prod_image.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vision.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/mlengine.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/gcp_system_helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/cloud_storage_transfer_service.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/pre_commit_ids.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_pytest_args_for_test_types.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/kubernetes_engine.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: removing upper bound for markupsafe
### Description

Per discussion and guidance from #19753, opening this issue and PR https://github.com/apache/airflow/pull/19760 for review. Based on if all the tests pass, this could be reviewed further.

### Use case/motivation

Currently Jinja2 upper bound is jinja2>=2.10.1,<4 at https://github.com/apache/airflow/blob/main/setup.cfg#L121 as part of #16595

Jinja2 seems to require MarkupSafe>=2.0 at https://github.com/pallets/jinja/blob/main/setup.py#L6 as part of pallets/jinja@e2f673e

Based on this, is it feasible to consider updating upper bound in airflow for markupsafe>=1.1.1, <2.0 to allow 2.0 at https://github.com/apache/airflow/blob/main/setup.cfg#L126 ?

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/stats/get_important_pr_candidates.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_bulk_issues.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_new_session_in_provide_session.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_waiters.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/update_quarantined_test_status.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/pinot/hooks/pinot.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py']
Ground Truth : ['a/airflow/dag_processing/manager.py', 'a/airflow/example_dags/example_dag_decorator.py', 'a/airflow/models/taskinstance.py', 'a/airflow/utils/db.py', '/dev/null', 'a/airflow/example_dags/example_python_operator.py', 'a/airflow/example_dags/example_branch_operator.py', 'a/airflow/migrations/versions/7b2661a43ba3_taskinstance_keyed_to_dagrun.py', 'a/airflow/triggers/base.py', 'a/airflow/example_dags/example_trigger_target_dag.py', 'a/airflow/migrations/versions/54bebd308c5f_add_trigger_table_and_task_info.py', 'a/airflow/operators/sql.py', 'a/airflow/task/task_runner/standard_task_runner.py', 'a/setup.py', 'a/airflow/example_dags/example_kubernetes_executor_config.py', 'a/airflow/www/views.py', 'a/airflow/macros/__init__.py', 'a/airflow/api_connexion/endpoints/dag_run_endpoint.py', 'a/airflow/api_connexion/endpoints/task_instance_endpoint.py', 'a/airflow/ti_deps/deps/valid_state_dep.py', 'a/airflow/models/dag.py', 'a/airflow/cli/commands/task_command.py', 'a/airflow/example_dags/example_nested_branch_dag.py', 'a/airflow/api_connexion/endpoints/log_endpoint.py', 'a/airflow/kubernetes/kube_config.py', 'a/airflow/serialization/serialized_objects.py', 'a/airflow/migrations/versions/e9304a3141f0_make_xcom_pkey_columns_non_nullable.py', 'a/airflow/providers_manager.py', 'a/airflow/settings.py', 'a/airflow/utils/log/secrets_masker.py', 'a/airflow/utils/retries.py', 'a/airflow/example_dags/example_xcom.py', 'a/airflow/task/task_runner/base_task_runner.py', 'a/airflow/example_dags/example_skip_dag.py', 'a/airflow/dag_processing/processor.py', 'a/airflow/operators/python.py', 'a/airflow/models/dagrun.py', 'a/airflow/utils/operator_helpers.py', 'a/airflow/example_dags/example_kubernetes_executor.py', 'a/airflow/executors/kubernetes_executor.py', 'a/airflow/example_dags/example_xcomargs.py', 'a/airflow/example_dags/plugins/workday.py', 'a/airflow/www/forms.py', 'a/airflow/example_dags/example_complex.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Add data_interval_start and data_interval_end columns to DagRun table, and set them when creating DagRuns.
Create a single DB Migration for the following:

- [ ] Use `DagModel.schedule_interval` to calculate `DagRun.data_interval_start` and `DagRun.data_interval_end`.
- [ ] Once`DagRun.data_interval_end` is calculated, use it's value as `DagRun.schedule_date` which will be a "replacement" for `DagRun.execution_date` as mentioned in [AIP-39](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-39+Richer+scheduler_interval#AIP39Richerscheduler_interval-RenameDagRun.execution_datetoschedule_date) (Keep `execution_date` - don't remove it for backwards compatibility.)
Update template context to deprecate `execution_date` and add new "interval" properties
As per the AIP, we want to deprecate some of the current variables in the template context, as well as add some new ones

When one of these deprecated variables is accessed we want to issue a DeprecationWarning., Getting this to show up the location of the dag file may be hard, so we make to include the dag_id and task_id in the warning.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/context.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/sensors/external_task.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/baseoperator.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/executor_loader.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py']
Ground Truth : ['a/airflow/operators/subdag.py', 'a/airflow/models/taskinstance.py', 'a/airflow/api/common/experimental/mark_tasks.py', 'a/airflow/operators/python.py', 'a/airflow/timetables/simple.py', 'a/airflow/utils/file.py', 'a/airflow/timetables/interval.py', '/dev/null', 'a/airflow/models/baseoperator.py', 'a/airflow/api_connexion/endpoints/dag_run_endpoint.py', 'a/airflow/jobs/backfill_job.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dag.py', 'a/airflow/models/dagrun.py', 'a/airflow/timetables/base.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Snowflake python connector monkeypatches urllib and makes many services unusable.
Curreently wnen you run snowflke provider, it monkeypatches urlllb in a way that is not compatible with other libraries (for example presto SSL with kerberos, google,  amazon, qubole and many others).

This is not critical (as in 2.0 we have provider separation and snowflake code will not even be there until you choose [snowflake] extra or install provider manually,

For now we decided to release but immediately yank the snowflake provider!

Additional links: 

* Issue: https://github.com/snowflakedb/snowflake-connector-python/issues/324

Offending code:

* https://github.com/snowflakedb/snowflake-connector-python/blob/133d6215f7920d304c5f2d466bae38127c1b836d/src/snowflake/connector/network.py#L89-L92



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/hooks/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/transfers/copy_into_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/mysql/hooks/mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/hooks/snowflake_sql_api.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/snowflake/operators/test_snowflake_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/utils/sql_api_generate_jwt.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/dbt/cloud/utils/test_openlineage.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/snowflake/hooks/test_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Deferred AWS BatchOperator appears to re-trigger task and error out
### Apache Airflow version

2.6.3

### What happened

I started a long-running job with the BatchOperator in deferrable mode. Traceback is below:

The strange behavior here is that just before erroring out, it looks like the BatchOperator resubmits the job. However, the Batch is not actually submitted (verified in the console) - however, this seems to break the trigger and it errors out immediately after. 

```ip-172-20-18-105.ec2.internal
*** Found local files:
***   * /home/airflow/airflow/logs/dag_id=foxy_salesforce/run_id=scheduled__2023-08-01T19:30:00+00:00/task_id=foxy_salesforce_batch_job/attempt=2.log
***   * /home/airflow/airflow/logs/dag_id=foxy_salesforce/run_id=scheduled__2023-08-01T19:30:00+00:00/task_id=foxy_salesforce_batch_job/attempt=2.log.trigger.17484.log
[2023-08-01, 20:35:37 UTC] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: foxy_salesforce.foxy_salesforce_batch_job scheduled__2023-08-01T19:30:00+00:00 [queued]>
[2023-08-01, 20:35:37 UTC] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: foxy_salesforce.foxy_salesforce_batch_job scheduled__2023-08-01T19:30:00+00:00 [queued]>
[2023-08-01, 20:35:37 UTC] {taskinstance.py:1308} INFO - Starting attempt 2 of 2
[2023-08-01, 20:35:37 UTC] {taskinstance.py:1327} INFO - Executing <Task(BatchOperator): foxy_salesforce_batch_job> on 2023-08-01 19:30:00+00:00
[2023-08-01, 20:35:37 UTC] {standard_task_runner.py:57} INFO - Started process 8834 to run task
[2023-08-01, 20:35:37 UTC] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'foxy_salesforce', 'foxy_salesforce_batch_job', 'scheduled__2023-08-01T19:30:00+00:00', '--job-id', '17555', '--raw', '--subdir', 'DAGS_FOLDER/foxy_salesforce.py', '--cfg-path', '/tmp/tmpr0isg35r']
[2023-08-01, 20:35:37 UTC] {standard_task_runner.py:85} INFO - Job 17555: Subtask foxy_salesforce_batch_job
[2023-08-01, 20:35:37 UTC] {task_command.py:410} INFO - Running <TaskInstance: foxy_salesforce.foxy_salesforce_batch_job scheduled__2023-08-01T19:30:00+00:00 [running]> on host ip-172-20-18-105.ec2.internal
[2023-08-01, 20:35:37 UTC] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='data_engineering_alerts@intelycare.com' AIRFLOW_CTX_DAG_OWNER='nrobinson' AIRFLOW_CTX_DAG_ID='foxy_salesforce' AIRFLOW_CTX_TASK_ID='foxy_salesforce_batch_job' AIRFLOW_CTX_EXECUTION_DATE='2023-08-01T19:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-08-01T19:30:00+00:00'
[2023-08-01, 20:35:37 UTC] {batch.py:255} INFO - Running AWS Batch job - job definition: foxy_dev_batch_job:1 - on queue foxy-queue
[2023-08-01, 20:35:37 UTC] {batch.py:262} INFO - AWS Batch job - container overrides: {'command': ['-tap', 'salesforce', '-ds', 'salesforce', '-to', 'data_engineering_alerts@intelycare.com']}
[2023-08-01, 20:35:37 UTC] {base.py:73} INFO - Using connection ID 'aws_prod_batch' for task execution.
[2023-08-01, 20:35:37 UTC] {credentials.py:1051} INFO - Found credentials from IAM Role: DSAirflowIAMStack-DSDaggerServerRoleBC9B4D69-BVJN2Q2JAYNS
[2023-08-01, 20:35:37 UTC] {batch.py:292} INFO - AWS Batch job (0c52c6e5-6b77-4e48-88d6-e98478fdcae2) started: {'ResponseMetadata': {'RequestId': '4510c251-622d-4882-aec1-50eef30d2b7d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 01 Aug 2023 20:35:37 GMT', 'content-type': 'application/json', 'content-length': '165', 'connection': 'keep-alive', 'x-amzn-requestid': '4510c251-622d-4882-aec1-50eef30d2b7d', 'access-control-allow-origin': '*', 'x-amz-apigw-id': 'I_3oCGPhoAMEa6Q=', 'access-control-expose-headers': 'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date', 'x-amzn-trace-id': 'Root=1-64c96c99-57cff0982ce9c97360d0fd02'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:batch:us-east-1:806657589280:job/0c52c6e5-6b77-4e48-88d6-e98478fdcae2', 'jobName': 'foxy_salesforce', 'jobId': '0c52c6e5-6b77-4e48-88d6-e98478fdcae2'}
[2023-08-01, 20:35:37 UTC] {taskinstance.py:1415} INFO - Pausing task as DEFERRED. dag_id=foxy_salesforce, task_id=foxy_salesforce_batch_job, execution_date=20230801T193000, start_date=20230801T203537
[2023-08-01, 20:35:37 UTC] {local_task_job_runner.py:222} INFO - Task exited with return code 100 (task deferral)
[2023-08-01, 20:35:38 UTC] {base.py:73} INFO - Using connection ID 'aws_prod_batch' for task execution.
[2023-08-01, 20:35:38 UTC] {credentials.py:1051} INFO - Found credentials from IAM Role: DSAirflowIAMStack-DSDaggerServerRoleBC9B4D69-BVJN2Q2JAYNS
[2023-08-01, 20:35:39 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['STARTING']
[2023-08-01, 20:36:09 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['STARTING']
[2023-08-01, 20:36:39 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:37:09 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:37:39 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:38:09 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:38:39 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:39:09 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:39:39 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:40:09 UTC] {waiter_with_logging.py:129} INFO - Batch job 0c52c6e5-6b77-4e48-88d6-e98478fdcae2 not ready yet: ['RUNNING']
[2023-08-01, 20:40:12 UTC] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: foxy_salesforce.foxy_salesforce_batch_job scheduled__2023-08-01T19:30:00+00:00 [queued]>
[2023-08-01, 20:40:12 UTC] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: foxy_salesforce.foxy_salesforce_batch_job scheduled__2023-08-01T19:30:00+00:00 [queued]>
[2023-08-01, 20:40:12 UTC] {taskinstance.py:1306} INFO - Resuming after deferral
[2023-08-01, 20:40:12 UTC] {taskinstance.py:1327} INFO - Executing <Task(BatchOperator): foxy_salesforce_batch_job> on 2023-08-01 19:30:00+00:00
[2023-08-01, 20:40:12 UTC] {standard_task_runner.py:57} INFO - Started process 21621 to run task
[2023-08-01, 20:40:12 UTC] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'foxy_salesforce', 'foxy_salesforce_batch_job', 'scheduled__2023-08-01T19:30:00+00:00', '--job-id', '17561', '--raw', '--subdir', 'DAGS_FOLDER/foxy_salesforce.py', '--cfg-path', '/tmp/tmpyl2o5l2k']
[2023-08-01, 20:40:12 UTC] {standard_task_runner.py:85} INFO - Job 17561: Subtask foxy_salesforce_batch_job
[2023-08-01, 20:40:12 UTC] {task_command.py:410} INFO - Running <TaskInstance: foxy_salesforce.foxy_salesforce_batch_job scheduled__2023-08-01T19:30:00+00:00 [running]> on host ip-172-20-18-105.ec2.internal
[2023-08-01, 20:40:12 UTC] {taskinstance.py:1598} ERROR - Trigger failed:
Traceback (most recent call last):
  File "/home/airflow/dagger/venv/lib/python3.9/site-packages/airflow/providers/amazon/aws/utils/waiter_with_logging.py", line 122, in async_wait
    await waiter.wait(**args, WaiterConfig={"MaxAttempts": 1})
  File "/home/airflow/dagger/venv/lib/python3.9/site-packages/aiobotocore/waiter.py", line 49, in wait
    await AIOWaiter.wait(self, **kwargs)
  File "/home/airflow/dagger/venv/lib/python3.9/site-packages/aiobotocore/waiter.py", line 139, in wait
    raise WaiterError(
botocore.exceptions.WaiterError: Waiter batch_job_complete failed: Max attempts exceeded
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/home/airflow/dagger/venv/lib/python3.9/site-packages/airflow/jobs/triggerer_job_runner.py", line 537, in cleanup_finished_triggers
    result = details["task"].result()
  File "/home/airflow/dagger/venv/lib/python3.9/site-packages/airflow/jobs/triggerer_job_runner.py", line 615, in run_trigger
    async for event in trigger.run():
  File "/home/airflow/dagger/venv/lib/python3.9/site-packages/airflow/providers/amazon/aws/triggers/base.py", line 121, in run
    await async_wait(
  File "/home/airflow/dagger/venv/lib/python3.9/site-packages/airflow/providers/amazon/aws/utils/waiter_with_logging.py", line 131, in async_wait
    raise AirflowException("Waiter error: max attempts reached")
airflow.exceptions.AirflowException: Waiter error: max attempts reached
[2023-08-01, 20:40:12 UTC] {taskinstance.py:1824} ERROR - Task failed with exception
airflow.exceptions.TaskDeferralError: Trigger failure
[2023-08-01, 20:40:12 UTC] {taskinstance.py:1345} INFO - Marking task as FAILED. dag_id=foxy_salesforce, task_id=foxy_salesforce_batch_job, execution_date=20230801T193000, start_date=20230801T203537, end_date=20230801T204012
[2023-08-01, 20:40:12 UTC] {logging_mixin.py:150} WARNING - /home/airflow/dagger/venv/lib/python3.9/site-packages/airflow/utils/email.py:153 RemovedInAirflow3Warning: Fetching SMTP credentials from configuration variables will be deprecated in a future release. Please set credentials using a connection instead.
[2023-08-01, 20:40:12 UTC] {email.py:269} INFO - Email alerting: attempt 1
[2023-08-01, 20:40:12 UTC] {email.py:281} INFO - Sent an alert email to ['data_engineering_alerts@intelycare.com']
[2023-08-01, 20:40:12 UTC] {standard_task_runner.py:104} ERROR - Failed to execute job 17561 for task foxy_salesforce_batch_job (Trigger failure; 21621)
[2023-08-01, 20:40:12 UTC] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-08-01, 20:40:12 UTC] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
```

### What you think should happen instead

_No response_

### How to reproduce

Run a long-running Batch Job using the BatchOperator with `deferrable=True`

### Operating System

AmazonLinux

### Versions of Apache Airflow Providers

Version 8.4.0 of the AWS provider

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_cluster_activity.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_grid.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/elasticmock/fake_elasticsearch.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_openlineage_adapter.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_timezone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/openlineage/plugins/test_listener.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_triggerer_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0044_1_10_7_add_serialized_dag_table.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/utils/test_pod_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/batch.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Provider discovery based on entry_points is rather brittle
The tests we run in CI had shown that provider discovery based  on entry_points is rather brittle. 

Example here:

https://github.com/apache/airflow/pull/12466/checks?check_run_id=1467792592#step:9:4452

This is not a problem with Airflow, but wth PIP which might silently upgrade some packages and cause "version conflict" totally independently from Airflow configuration and totally out-of-our-control. 

Simple installing a whl package on top of the existing airflow installation (as it happened in the case above) might cause inconsistent requirements (in the case above installing .whl packages with all providers on top of existing Airflow installation caused the requests package to be upgraded to 2.25.0, even if airflow has the right requirements set. In this case it was (correct and it is from the "install_requires" section of airflow's setup.cfg):

```
Requirement.parse('requests<2.24.0,>=2.20.0'), {'apache-airflow'}
```

In case you have a version conflict in your env, running entry_point.load() from a package that has this version conflicts results with `pkg_resources.VersionConflict` error or `pkg_resources.ContextualVersionConflict) rather than returning the entry_point. Or at least that's what I observed so far. It's rather easy to reproduce. Simply install requests > 2.24.0 in the current airflow and see what happens.

So far I could not find a way to mitigate this problem, but @ashb - since you have more experience with it, maybe you can find a workaround for this?

I think we have a few options:

1) We fail 'airflow' hard if there is any Version Conflict. We have a way now after I've implemented ##10854  (and after @ephraimbuddy finishes the #12188 ) - we have a good, maintainable list of non-conflicting dependencies for Airflow and it's providers and we can keep that in the future thanks to pip-check. But I am afraid that will give a hard time to people who would like to install airflow with some custom dependencies (Tensorflow for example, depending on versions is notoriously difficult to sync with Airflow when it comes to dependencies). However, this is the most "Proper" (TM) solution.

2)  We find a workaround for the entry_point.load() VersionConflict exception. However, I think that might not be possible or easy looking for example at this SO thread: https://stackoverflow.com/questions/52982603/python-entry-point-fails-for-dependency-conflict . The most upvoted (=1) answer there starts with "Welcome to the world of dependencies hell! I know no clean way to solve this" - which is not very encouraging. I tried also to find it out from docs and code of the entry_point.load() but to no avail. @ashb - maybe you can help here.

3) We go back to the original implementation of mine where I read provider info from provider.yaml embedded into the package. This has disadvantage of being non-standard, but it works independently of version conflicts.


WDYT?




Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/path_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docker_tests/docker_tests_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/test_cli_parser.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/ci_image_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/developer_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/verify_providers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/sagemaker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py']
Ground Truth : ['a/airflow/utils/log/json_formatter.py', 'a/setup.py', 'a/airflow/version.py', 'a/airflow/configuration.py', 'a/airflow/contrib/kubernetes/pod.py', 'a/airflow/settings.py', 'a/airflow/www/app.py', 'a/airflow/bin/cli.py', 'a/airflow/utils/dag_processing.py', 'a/airflow/plugins_manager.py', 'a/airflow/www_rbac/app.py', 'a/airflow/jobs/scheduler_job.py', 'a/airflow/models/dagrun.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Scheduler does not appear to be running properly in default airflow setup
Scheduler seems to be stucka and finaly exits with deadlock error (in mysql) in the current master setup in breeze

**What happened**:

When you run example dags in Breeze in current master you get "missing scheduler" warning and the scheduler seems to be stuck. In MySQL it also timeouts with Deadloc after a while. In Postrges it seems to be stuck indefinitely. 


**How to reproduce it**:

`./breeze  start-airflow --backend mysql --db-reset --load-example-dags`

or

`./breeze  start-airflow --backend postgres --db-reset --load-example-dags`


When you try to run example dags they seem to start initially but they are left in the "SCHEDULED" state and do not continue to run. Scheduler logs stops at:

```
  ____________       _____________                                                                                                                                                                                                                                                 
 ____    |__( )_________  __/__  /________      __                                                                                                                                                                                                                           
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /                                                                                                                                                                                                                                 
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /                                                                                                                                                                                                                                  
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/                                                                                                                                                                                                                                        
[2020-10-23 16:53:08,823] {scheduler_job.py:1270} INFO - Starting the scheduler           
[2020-10-23 16:53:08,823] {scheduler_job.py:1275} INFO - Processing each file at most -1 times                                                                                                                                                                                          
[2020-10-23 16:53:08,824] {scheduler_job.py:1297} INFO - Resetting orphaned tasks for active dag runs
[2020-10-23 16:53:08,831] {dag_processing.py:250} INFO - Launched DagFileProcessorManager with pid: 504                               
[2020-10-23 16:53:08,836] {settings.py:49} INFO - Configured default timezone Timezone('UTC')                         
/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py:593: Warning: (1300, "Invalid utf8mb4 character string: '800495'")
  cursor.execute(statement, parameters)                                                                                               
/usr/local/lib/python3.6/site-packages/MySQLdb/cursors.py:322: Warning: (1300, "Invalid utf8mb4 character string: '800495'")                                                                                                                                                            
  rows += self.execute(sql + postfix)                                                                                                                                                                                                                                                   
[2020-10-23 16:53:29,490] {scheduler_job.py:976} INFO - 4 tasks up for execution:                                                                                                                                                                                                       
        <TaskInstance: example_bash_operator.also_run_this 2020-10-21 00:00:00+00:00 [scheduled]>                                                                                                                                                                                       
        <TaskInstance: example_bash_operator.runme_0 2020-10-21 00:00:00+00:00 [scheduled]>              
        <TaskInstance: example_bash_operator.runme_1 2020-10-21 00:00:00+00:00 [scheduled]>            
        <TaskInstance: example_bash_operator.runme_2 2020-10-21 00:00:00+00:00 [scheduled]>          
[2020-10-23 16:53:29,491] {scheduler_job.py:1011} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 4 task instances ready to be queued
[2020-10-23 16:53:29,491] {scheduler_job.py:1038} INFO - DAG example_bash_operator has 0/16 running and queued tasks
[2020-10-23 16:53:29,491] {scheduler_job.py:1038} INFO - DAG example_bash_operator has 1/16 running and queued tasks
[2020-10-23 16:53:29,491] {scheduler_job.py:1038} INFO - DAG example_bash_operator has 2/16 running and queued tasks                                                                                                                                                                    
[2020-10-23 16:53:29,491] {scheduler_job.py:1038} INFO - DAG example_bash_operator has 3/16 running and queued tasks
[2020-10-23 16:53:29,491] {scheduler_job.py:1090} INFO - Setting the following tasks to queued state:                                                                                                                                                                                   
        <TaskInstance: example_bash_operator.runme_0 2020-10-21 00:00:00+00:00 [scheduled]>                                                                                                                                                                                            
        <TaskInstance: example_bash_operator.runme_1 2020-10-21 00:00:00+00:00 [scheduled]>                                                                                                                                                                                            
        <TaskInstance: example_bash_operator.runme_2 2020-10-21 00:00:00+00:00 [scheduled]>                                                                                                                                                                                            
        <TaskInstance: example_bash_operator.also_run_this 2020-10-21 00:00:00+00:00 [scheduled]>                                                                                                                                                                                       
[2020-10-23 16:53:29,493] {scheduler_job.py:1137} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_0', execution_date=datetime.datetime(2020, 10, 21, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default          
[2020-10-23 16:53:29,493] {base_executor.py:78} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2020-10-21T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/airflow/example_dags/example_bash_operator.py']    
[2020-10-23 16:53:29,493] {scheduler_job.py:1137} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_1', execution_date=datetime.datetime(2020, 10, 21, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default           
[2020-10-23 16:53:29,494] {base_executor.py:78} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', '2020-10-21T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/airflow/example_dags/example_bash_operator.py']     
[2020-10-23 16:53:29,494] {scheduler_job.py:1137} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_2', execution_date=datetime.datetime(2020, 10, 21, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default           
[2020-10-23 16:53:29,494] {base_executor.py:78} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', '2020-10-21T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/airflow/example_dags/example_bash_operator.py']     
[2020-10-23 16:53:29,494] {scheduler_job.py:1137} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='also_run_this', execution_date=datetime.datetime(2020, 10, 21, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default
[2020-10-23 16:53:29,494] {base_executor.py:78} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', '2020-10-21T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/airflow/example_dags/example_bash_operator.py'
]                                                                                                        
[2020-10-23 16:53:29,494] {sequential_executor.py:57} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', '2020-10-21T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/opt/airflow/airflow/example_dags/example_bash_operator.p
y']                                                                                                            
[2020-10-23 16:53:31,139] {dagbag.py:436} INFO - Filling up the DagBag from /opt/airflow/airflow/example_dags/example_bash_operator.py
```

In MySQL you get an extra deadlock exception after a while:

```
Running <TaskInstance: example_bash_operator.runme_1 2020-10-21T00:00:00+00:00 [scheduled]> on host 7216b1a127c3
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B
^[[B^[[BTraceback (most recent call last):                                                               
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context                             
    cursor, statement, parameters, context                                                                                                                                                                                                                                              
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 593, in do_execute                                                                                                                                                                                   
    cursor.execute(statement, parameters)                                                                                                                                                                                                                                               
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/cursors.py", line 255, in execute                                                                                                                                                                                                
    self.errorhandler(self, exc, value)                                                                
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler        
    raise errorvalue                                                                                      
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/cursors.py", line 252, in execute                 
    res = self._query(query)                                                                
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/cursors.py", line 378, in _query                                                                                                                                                                                                
    db.query(q)                                                                                                                                                                                                                                                                         
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/connections.py", line 280, in query                                                                                                                                                                                              
    _mysql.connection.query(self, query)                                                                                                                                                                                                                                                
_mysql_exceptions.OperationalError: (1205, 'Lock wait timeout exceeded; try restarting transaction')       


The above exception was the direct cause of the following exception:                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                   
Traceback (most recent call last):                                                                                                                                                                                                                                           
  File "/usr/local/bin/airflow", line 33, in <module>                                                                                                                                                                                                                              
    sys.exit(load_entry_point('apache-airflow', 'console_scripts', 'airflow')())                                                                                                                                                                                                   
  File "/opt/airflow/airflow/__main__.py", line 40, in main                                                                                                                                                                                                                             
    args.func(args)                                                                       
  File "/opt/airflow/airflow/cli/cli_parser.py", line 53, in command                                                                                                                                                                                                                    
    return func(*args, **kwargs)                                                                     
  File "/opt/airflow/airflow/utils/cli.py", line 84, in wrapper                                                                       
    return f(*args, **kwargs)                                                                                         
  File "/opt/airflow/airflow/cli/commands/task_command.py", line 206, in task_run                                                     
    _run_task_by_selected_method(args, dag, ti)                                                                                       
  File "/opt/airflow/airflow/cli/commands/task_command.py", line 59, in _run_task_by_selected_method                                                                                                                                                                                    
    _run_task_by_local_task_job(args, ti)                                                                                                                                                                                                                                               
  File "/opt/airflow/airflow/cli/commands/task_command.py", line 113, in _run_task_by_local_task_job                                                                                                                                                                                    
    run_job.run()                                                                                                                                                                                                                                                                       
  File "/opt/airflow/airflow/jobs/base_job.py", line 245, in run                                         
    self._execute()                                                                                    
  File "/opt/airflow/airflow/jobs/local_task_job.py", line 91, in _execute                           
    pool=self.pool):                                                                                                                                                     
  File "/opt/airflow/airflow/utils/session.py", line 63, in wrapper                                                 
    return func(*args, **kwargs)                                                                                    
  File "/opt/airflow/airflow/models/taskinstance.py", line 945, in check_and_change_state_before_execution                                                                                                                                                                              
    self.refresh_from_db(session=session, lock_for_update=True)                                                     
  File "/opt/airflow/airflow/utils/session.py", line 59, in wrapper                                                                                                                                                                                                                     
    return func(*args, **kwargs)                                                                                                                                                                                                                                                       
  File "/opt/airflow/airflow/models/taskinstance.py", line 534, in refresh_from_db                                                                                                                                                                                                     
    ti = qry.with_for_update().first()                                                                                                                                                                                                                                                 
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3402, in first                                                                                                                                                                                            
    ret = list(self[0:1])                                                                                                                                                                                                                                                              
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3176, in __getitem__                                                                                                                                                                                     
    return list(res)                                                                                                                                                                                                                                                                    
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3508, in __iter__                                                                                                                                                                                         
    return self._execute_and_instances(context)                                                                                                                                                                                                                                         
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3533, in _execute_and_instances                                                                                                                                                                           
    result = conn.execute(querycontext.statement, self._params)                                                                                                                                                                                                                    
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1011, in execute                                                                                                                                                                                        
    return meth(self, multiparams, params)                                                               
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py", line 298, in _execute_on_connection                                                                                                                                                                         
    return connection._execute_clauseelement(self, multiparams, params)                                        
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1130, in _execute_clauseelement                       
    distilled_params,                                                                                                                                                                                                                                                                   
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1317, in _execute_context
    e, statement, parameters, cursor, context                                                                                                                                                                                                                                           
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1511, in _handle_dbapi_exception                                                                                                                                                                        
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e                                                                                                                                                                                                                           
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_                                                                                                                                                                                          
    raise exception                                                                                                                                                                                                                                                                     
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1277, in _execute_context      
    cursor, statement, parameters, context                                                               
  File "/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 593, in do_execute          
    cursor.execute(statement, parameters)                                                                                                                                                                                                                                              
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/cursors.py", line 255, in execute                                                                                                                                                                                                
    self.errorhandler(self, exc, value)                                                                                                                                                                                                                                                 
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler                                                                                                                                                                                 
    raise errorvalue                                                                                                                                                                                                                                                                   
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/cursors.py", line 252, in execute                                                                                                                                                                                                
    res = self._query(query)                                                                                                                                                                                                                                                           
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/cursors.py", line 378, in _query                                                                                                                                                                                                
    db.query(q)                                                                                                 
  File "/usr/local/lib/python3.6/site-packages/MySQLdb/connections.py", line 280, in query                                                                                                                                                                                              
    _mysql.connection.query(self, query)                                                                 
sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction')
[SQL: SELECT task_instance.try_number AS task_instance_try_number, task_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task_instance_dag_id, task_instance.execution_date AS task_instance_execution_date, task_instance.start_date AS task_instance_start_date, tas
k_instance.end_date AS task_instance_end_date, task_instance.duration AS task_instance_duration, task_instance.state AS task_instance_state, task_instance.max_tries AS task_instance_max_tries, task_instance.hostname AS task_instance_hostname, task_instance.unixname AS task_instan
ce_unixname, task_instance.job_id AS task_instance_job_id, task_instance.pool AS task_instance_pool, task_instance.pool_slots AS task_instance_pool_slots, task_instance.queue AS task_instance_queue, task_instance.priority_weight AS task_instance_priority_weight, task_instance.ope
rator AS task_instance_operator, task_instance.queued_dttm AS task_instance_queued_dttm, task_instance.queued_by_job_id AS task_instance_queued_by_job_id, task_instance.pid AS task_instance_pid, task_instance.executor_config AS task_instance_executor_config, task_instance.externa
l_executor_id AS task_instance_external_executor_id                                                    
FROM task_instance                                                                                             
WHERE task_instance.dag_id = %s AND task_instance.task_id = %s AND task_instance.execution_date = %s      
 LIMIT %s FOR UPDATE]                                                                                    
[parameters: ('example_bash_operator', 'runme_1', datetime.datetime(2020, 10, 21, 0, 0), 1)]
```




Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/operators/test_datetime.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/hooks/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_log_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_time_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/beam/hooks/test_beam.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py']
Ground Truth : ['a/airflow/jobs/scheduler_job.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: SSHOperator - Allow specific command timeout 
### Description

Following #29282, command timeout is set at the `SSHHook` level while it used to be able to set at the `SSHOperator` level.

I will work on a PR as soon as i can.

### Use case/motivation

Ideally, i think we could have a default value set on `SSHHook`, but with the possibility of overriding it at the `SSHOperator` level.

### Related issues

#29282 

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ssh/operators/ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/task/task_runner/cgroup_task_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/psrp/hooks/psrp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/process_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/batch/test_batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/ssh/operators/test_ssh.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_setup_teardown_taskflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/executors/ecs/test_ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py']
Ground Truth : ['a/airflow/providers/ssh/hooks/ssh.py', 'a/airflow/providers/ssh/operators/ssh.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: `GCSToLocalFilesystemOperator` unnecessarily downloads objects when it checks object size
`GCSToLocalFilesystemOperator` in `airflow/providers/google/cloud/transfers/gcs_to_local.py` checks the file size if `store_to_xcom_key` is `True`.

https://github.com/apache/airflow/blob/b40dffa08547b610162f8cacfa75847f3c4ca364/airflow/providers/google/cloud/transfers/gcs_to_local.py#L137-L142

How it checks size is to download the object as `bytes` then check the size. This unnecessarily downloads the objects. `google.cloud.storage.blob.Blob` itself already has a `size` property ([documentation reference](https://googleapis.dev/python/storage/1.30.0/blobs.html#google.cloud.storage.blob.Blob.size)), and it should be used instead.

In extreme cases, if the object is big size, it adds unnecessary burden on the instance resources.

A new method, `object_size()`, can be added to `GCSHook`, then this can be addressed in `GCSToLocalFilesystemOperator`.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/gcs_to_local.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/transfers/glacier_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/common/hooks/base_google.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/sensors/s3.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/s3_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/marketing_platform/operators/analytics.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/ftp/hooks/ftp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/batch_prediction_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/log/stackdriver_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/suite/hooks/drive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/transfers/sql_to_gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/sensors/gcs.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/vertex_ai/batch_prediction_job.py']
Ground Truth : ['a/airflow/providers/google/cloud/transfers/gcs_to_local.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: Consider depending on `azure-keyvault-secrets` instead of `azure-keyvault` metapackage
### Description

It appears that the `microsoft-azure` provider only depends on `azure-keyvault-secrets`:
https://github.com/apache/airflow/blob/388723950de9ca519108e0a8f6818f0fc0dd91d4/airflow/providers/microsoft/azure/secrets/key_vault.py#L24
and not the other 2 packages in the `azure-keyvault` metapackage.

### Use case/motivation

I am the maintainer of the `apache-airflow-providers-*` packages on `conda-forge` and I'm running into small issues with the way `azure-keyvault` is maintained as a metapackage on `conda-forge`.  I think depending on `azure-keyvault-secrets` explicitly would solve my problem and also provide better clarity for the `microsoft-azure` provider in general.

### Related issues

https://github.com/conda-forge/azure-keyvault-feedstock/issues/6
https://github.com/conda-forge/apache-airflow-providers-microsoft-azure-feedstock/pull/13

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/secrets/key_vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_packages.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/secrets/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/_internal_client/vault_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/cosmos.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/hashicorp/hooks/vault.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/always/test_project_structure.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/transfers/copy_into_snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/global_constants.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/data_lake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/test_utils/azure_system_helpers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/container_volume.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/base_azure.py']
Ground Truth : ['a/setup.py']
Current Recall: 0.08670685132141236

=========================================================

ISSUE: DagRun for <FOO> with run_id or execution_date of 'manual__XXXXX' not found
### Apache Airflow version

2.2.2 (latest released)

### What happened

After upgrading from Airflow 2.1.4 to 2.2.2, every DAG gives this error upon execution:

> [2021-12-17, 15:01:12 UTC] {taskinstance.py:1259} INFO - Executing <Task(_PythonDecoratedOperator): print_the_context> on 2021-12-17 15:01:08.943254+00:00
[2021-12-17, 15:01:12 UTC] {standard_task_runner.py:52} INFO - Started process 873 to run task
[2021-12-17, 15:01:12 UTC] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'example_python_operator', 'print_the_context', 'manual__2021-12-17T15:01:08.943254+00:00', '--job-id', '326', '--raw', '--subdir', 'DAGS_FOLDER/test.py', '--cfg-path', '/tmp/tmpej1imvkr', '--error-file', '/tmp/tmpqn9ad7em']
[2021-12-17, 15:01:12 UTC] {standard_task_runner.py:77} INFO - Job 326: Subtask print_the_context
[2021-12-17, 15:01:12 UTC] {standard_task_runner.py:92} ERROR - Failed to execute job 326 for task print_the_context
Traceback (most recent call last):
  File "/home/ec2-user/venv/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/ec2-user/venv/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/ec2-user/venv/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/ec2-user/venv/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 287, in task_run
    ti = _get_ti(task, args.execution_date_or_run_id)
  File "/home/ec2-user/venv/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/ec2-user/venv/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 86, in _get_ti
    dag_run = _get_dag_run(task.dag, exec_date_or_run_id, create_if_necssary, session)
  File "/home/ec2-user/venv/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 80, in _get_dag_run
    ) from None
airflow.exceptions.DagRunNotFound: DagRun for example_python_operator with run_id or execution_date of 'manual__2021-12-17T15:01:08.943254+00:00' not found

Both tables `airflowdb.task_instance` and `airflowdb.dag_run` have rows with `run_id` equal to "manual__2021-12-17T15:01:08.943254+00:00".

The issue seems to arise in the `_get_dag_run()` function from [airflow/cli/commands/task_command.py](https://github.com/apache/airflow/blob/bb82cc0fbb7a6630eac1155d0c3b445dff13ceb6/airflow/cli/commands/task_command.py#L61-L72):

``` 
    execution_date = None
    with suppress(ParserError, TypeError):
        execution_date = timezone.parse(exec_date_or_run_id)

    if create_if_necessary and not execution_date:
        return DagRun(dag_id=dag.dag_id, run_id=exec_date_or_run_id)
    try:
        return (
            session.query(DagRun)
            .filter(
                DagRun.dag_id == dag.dag_id,
                DagRun.execution_date == execution_date,
            )
            .one()
        )
```
Here, `exec_date_or_run_id == 'manual__2021-12-17T15:01:08.943254+00:00'` and `timezone.parse(exec_date_or_run_id)` fails, meaning `execution_date` stays as `None` and the session query returns no results.

### What you expected to happen

Expect DAGs to run without the above error.

### How to reproduce

Upgraded from 2.1.4 to 2.2.2 and manually ran a few DAGs. The above log is from the [example_python_operator](https://github.com/apache/airflow/blob/main/airflow/example_dags/example_python_operator.py) DAG provided on the Airflow repo.

### Operating System

Amazon Linux 2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

Tried `airflow db upgrade` and `airflow db reset`  without any luck. The same issue appears on 2.2.3rc2.

Using MySQL 8.0.23.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/endpoints/test_task_instance_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_trigger_dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/api/experimental/endpoints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/elasticsearch/log/test_es_task_handler.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py']
Current Recall: 0.0888481789445387

=========================================================

ISSUE: AIP-56 - Create auth manager interface


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/timetables/_cron.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_appless.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/amazon/aws/tests/test_aws_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/models/batch_apis.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/azure/hooks/wasb.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0095_2_2_4_add_session_table_to_db.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/security_manager/aws_security_manager_override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/xcom_arg.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/amazon/aws/auth_manager/cli/test_avp_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/system/providers/google/marketing_platform/example_analytics_admin.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/hooks/databricks_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0075_2_0_0_add_description_field_to_connection.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/cli/celery_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/cli/idc_commands.py']
Ground Truth : ['a/airflow/www/extensions/init_jinja_globals.py', '/dev/null', 'a/airflow/configuration.py']
Current Recall: 0.0888481789445387

=========================================================

ISSUE: Getting error in scheduler logs "Task killed externally" when running a dag with task group mapping
### Apache Airflow version

main (development)

### What happened

Getting error in scheduler logs "Task killed externally" when running a dag with task group mapping
Logs - 
```
scheduler | [2022-11-24 11:30:05,683] {local_executor.py:130} ERROR - Failed to execute task map_index passed to non-mapped task.
 scheduler | Traceback (most recent call last):
 scheduler | File "/Users/jyotsananamdev/dev-airflow/airflow/airflow/executors/local_executor.py", line 126, in _execute_work_in_fork
 scheduler | args.func(args)
 scheduler | File "/Users/jyotsananamdev/dev-airflow/airflow/airflow/cli/cli_parser.py", line 52, in command
 scheduler | return func(*args, **kwargs)
 scheduler | File "/Users/jyotsananamdev/dev-airflow/airflow/airflow/utils/cli.py", line 108, in wrapper
 scheduler | return f(*args, **kwargs)
 scheduler | File "/Users/jyotsananamdev/dev-airflow/airflow/airflow/cli/commands/task_command.py", line 376, in task_run
 scheduler | ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
 scheduler | File "/Users/jyotsananamdev/dev-airflow/airflow/airflow/utils/session.py", line 75, in wrapper
 scheduler | return func(*args, session=session, **kwargs)
 scheduler | File "/Users/jyotsananamdev/dev-airflow/airflow/airflow/cli/commands/task_command.py", line 156, in _get_ti
 scheduler | raise RuntimeError("map_index passed to non-mapped task")
 scheduler | RuntimeError: map_index passed to non-mapped task
```

other logs- 
```
scheduler | [2022-11-24 11:30:06,614] {scheduler_job.py:681} ERROR - Executor reports task instance <TaskInstance: taskmap_taskgroup.tg.hello_there manual__2022-11-24T05:59:58.057750+00:00 map_index=0 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?
 scheduler | [2022-11-24 11:30:06,615] {taskinstance.py:1751} ERROR - Executor reports task instance <TaskInstance: taskmap_taskgroup.tg.hello_there manual__2022-11-24T05:59:58.057750+00:00 map_index=0 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?
```

### What you think should happen instead

It should pass

### How to reproduce

Run the below dag to reproduce this -
```
from datetime import datetime

from airflow import DAG
from airflow.decorators import task, task_group
from airflow.operators.empty import EmptyOperator

with DAG(
    dag_id="taskmap_taskgroup",
    tags=["AIP_42"],
    start_date=datetime(1970, 1, 1),
    schedule=None,
) as dag:

    @task
    def hello_there(arg):
        print(arg)

    @task_group
    def tg(x):
        hello_there(x)

    tg.expand(x=[1]) >> EmptyOperator(task_id="done")
```

### Operating System

mac os

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/backfill_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/executors/base_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dagrun.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/models/test_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/processor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/sensors/test_external_task_sensor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_scheduler_job.py']
Ground Truth : ['a/airflow/cli/commands/task_command.py', 'a/airflow/models/taskinstance.py', 'a/airflow/models/operator.py', 'a/airflow/www/views.py', 'a/airflow/models/baseoperator.py', 'a/airflow/api_connexion/schemas/task_schema.py', 'a/airflow/models/mappedoperator.py', 'a/airflow/ti_deps/deps/trigger_rule_dep.py', 'a/airflow/models/xcom_arg.py', 'a/airflow/api_connexion/endpoints/task_instance_endpoint.py', 'a/airflow/ti_deps/deps/ready_to_reschedule.py']
Current Recall: 0.08904284509209563

=========================================================

ISSUE: RedshiftDataOperator: Add support for Redshift serverless clusters
### Description

This feature adds support for Redshift Serverless clusters for the given operator.

### Use case/motivation

RedshiftDataOperator currently only supports provisioned clusters since it has the capability of adding `ClusterIdentifier` as a parameter but not `WorkgroupName`. The addition of this feature would help users to use this operator for their serverless cluster as well.

### Related issues

None

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/redshift_data.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/redshift_sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/redshift_data.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/tests/test_provider_documentation.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/operators/redshift_cluster.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/redshift_cluster.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/emr.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/exceptions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/hooks/batch_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/assign_cherry_picked_prs_with_milestone.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_release_issue.py']
Ground Truth : ['a/airflow/providers/amazon/aws/operators/redshift_data.py', 'a/airflow/providers/amazon/aws/hooks/redshift_data.py']
Current Recall: 0.09118417271522197

=========================================================

ISSUE: Allow for Markup in UI page title
### Description

A custom page title can be set on the UI with the  `instance_name` variable in `airflow.cfg`. It would be nice to have the option to include Markup text in that variable for further customization of the title, similar to how dashboard alerts introduced in #18284 allow for `html=True`.

### Use case/motivation

It would be useful to be able to use formatting like underline, bold, color, etc, in the UI title. For example, color-coded environments to minimize chances of a developer triggering a DAG in the wrong environment:

![title](https://user-images.githubusercontent.com/50751110/149528236-403473ed-eba5-4102-a804-0fa78fca8856.JPG)


### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/example_dags/example_params_ui_tutorial.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/exts/exampleinclude.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/test_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/slack/hooks/slack.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/batch/batch_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/prepare_bulk_issues.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/executors/ecs/ecs_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/vertex_ai/custom_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/databricks/operators/databricks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/clients/python/test_python_client.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/parallel.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/run_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py']
Ground Truth : ['a/airflow/www/views.py']
Current Recall: 0.09118417271522197

=========================================================

ISSUE: UndefinedJinjaVariablesRule fails on non-standard python objects
**Apache Airflow version**: 1.10.14

**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):

**Environment**: 

- **Cloud provider or hardware configuration**: X
- **OS** (e.g. from /etc/os-release): X
- **Kernel** (e.g. `uname -a`): X
- **Install tools**: X
- **Others**: X

**What happened**:

The `UndefinedJinjaVariablesRule` appears to expect non-standard python objects to have a `template_fields` attribute, when many (`pathlib.Path`, `None`) do not.

<details><summary>Path example</summary>

```python
========================================================================================================================================================== STATUS ==========================================================================================================================================================

Check for latest versions of apache-airflow and checker...........................................................................................................................................................................................................................................................SUCCESS
Remove airflow.AirflowMacroPlugin class...........................................................................................................................................................................................................................................................................SUCCESS
Chain between DAG and operator not allowed........................................................................................................................................................................................................................................................................SUCCESS
Connection.conn_id is not unique..................................................................................................................................................................................................................................................................................SUCCESS
Connection.conn_type is not nullable..............................................................................................................................................................................................................................................................................SUCCESS
Ensure users are not using custom metaclasses in custom operators.................................................................................................................................................................................................................................................SUCCESS
Fernet is enabled by default......................................................................................................................................................................................................................................................................................FAIL
GCP service account key deprecation...............................................................................................................................................................................................................................................................................SUCCESS
Unify hostname_callable option in core section....................................................................................................................................................................................................................................................................FAIL
Changes in import paths of hooks, operators, sensors and others...................................................................................................................................................................................................................................................SUCCESS
Users must delete deprecated configs for KubernetesExecutor.......................................................................................................................................................................................................................................................FAIL
Legacy UI is deprecated by default................................................................................................................................................................................................................................................................................SUCCESS
Logging configuration has been moved to new section...............................................................................................................................................................................................................................................................FAIL
Removal of Mesos Executor.........................................................................................................................................................................................................................................................................................SUCCESS
No additional argument allowed in BaseOperator....................................................................................................................................................................................................................................................................SUCCESS
Users must set a kubernetes.pod_template_file value...............................................................................................................................................................................................................................................................SKIPPED
SendGrid email uses old airflow.contrib module....................................................................................................................................................................................................................................................................SUCCESS
Changes in import path of remote task handlers....................................................................................................................................................................................................................................................................SUCCESS
Traceback (most recent call last):
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/bin/airflow", line 37, in <module>
    args.func(args)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/checker.py", line 88, in run
    all_problems = check_upgrade(formatter, rules)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/checker.py", line 37, in check_upgrade
    rule_status = RuleStatus.from_rule(rule)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/problem.py", line 44, in from_rule
    result = rule.check()
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 141, in check
    dag_messages = self.iterate_over_dag_tasks(dag)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 128, in iterate_over_dag_tasks
    error_messages = self.iterate_over_template_fields(task)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 117, in iterate_over_template_fields
    self._check_rendered_content(rendered_content, set())
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 63, in _check_rendered_content
    debug_error_messages.update(self._check_rendered_content(value))
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 69, in _check_rendered_content
    return self._nested_check_rendered(rendered_content, seen_oids)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 75, in _nested_check_rendered
    nested_template_fields = rendered_content.template_fields
AttributeError: 'PosixPath' object has no attribute 'template_fields'
```

</details>

<details><summary>None example</summary>

```python
========================================================================================================================================================== STATUS ==========================================================================================================================================================

Check for latest versions of apache-airflow and checker...........................................................................................................................................................................................................................................................SUCCESS
Remove airflow.AirflowMacroPlugin class...........................................................................................................................................................................................................................................................................SUCCESS
Chain between DAG and operator not allowed........................................................................................................................................................................................................................................................................SUCCESS
Connection.conn_id is not unique..................................................................................................................................................................................................................................................................................SUCCESS
Connection.conn_type is not nullable..............................................................................................................................................................................................................................................................................SUCCESS
Ensure users are not using custom metaclasses in custom operators.................................................................................................................................................................................................................................................SUCCESS
Fernet is enabled by default......................................................................................................................................................................................................................................................................................FAIL
GCP service account key deprecation...............................................................................................................................................................................................................................................................................SUCCESS
Unify hostname_callable option in core section....................................................................................................................................................................................................................................................................FAIL
Changes in import paths of hooks, operators, sensors and others...................................................................................................................................................................................................................................................SUCCESS
Users must delete deprecated configs for KubernetesExecutor.......................................................................................................................................................................................................................................................FAIL
Legacy UI is deprecated by default................................................................................................................................................................................................................................................................................SUCCESS
Logging configuration has been moved to new section...............................................................................................................................................................................................................................................................FAIL
Removal of Mesos Executor.........................................................................................................................................................................................................................................................................................SUCCESS
No additional argument allowed in BaseOperator....................................................................................................................................................................................................................................................................SUCCESS
Users must set a kubernetes.pod_template_file value...............................................................................................................................................................................................................................................................SKIPPED
SendGrid email uses old airflow.contrib module....................................................................................................................................................................................................................................................................SUCCESS
Changes in import path of remote task handlers....................................................................................................................................................................................................................................................................SUCCESS
Traceback (most recent call last):
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/bin/airflow", line 37, in <module>
    args.func(args)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/checker.py", line 88, in run
    all_problems = check_upgrade(formatter, rules)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/checker.py", line 37, in check_upgrade
    rule_status = RuleStatus.from_rule(rule)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/problem.py", line 44, in from_rule
    result = rule.check()
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 146, in check
    dag_messages = self.iterate_over_dag_tasks(dag)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 130, in iterate_over_dag_tasks
    error_messages = self.iterate_over_template_fields(task)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 118, in iterate_over_template_fields
    self._check_rendered_content(rendered_content, set())
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 64, in _check_rendered_content
    debug_error_messages.update(self._check_rendered_content(value))
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 70, in _check_rendered_content
    return self._nested_check_rendered(rendered_content, seen_oids)
  File "/Users/madison/programs/anaconda3/envs/memphis-airflow/lib/python3.8/site-packages/airflow/upgrade/rules/undefined_jinja_varaibles.py", line 76, in _nested_check_rendered
    nested_template_fields = rendered_content.template_fields
AttributeError: 'NoneType' object has no attribute 'template_fields'
```

</details>

**What you expected to happen**:

I expect these types to be handled appropriately by the upgrade rule and not assume that a `template_fields` attribute exists on them.

**How to reproduce it**:

Create a simple DAG with a `PythonOperator` task that has `op_kwargs: {"foo": None}` and run the upgrade check.

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/log/test_secrets_masker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/initialize_virtualenv.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/in_container/run_generate_constraints.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/docker/operators/docker.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/log/file_task_handler.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/bash.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/tools/generate-integrations-json.py']
Ground Truth : ['a/airflow/upgrade/rules/undefined_jinja_varaibles.py']
Current Recall: 0.09118417271522197

=========================================================

ISSUE: AIP-56 - FAB AM - User's statistics view
Move user's statistics view to FAB Auth manager

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/security_manager/override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/security_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/auth/managers/base_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/fab/auth_manager/fab_auth_manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_user_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/extensions/init_appbuilder.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/endpoints/forward_to_fab_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/apache-airflow-providers-fab/img/diagram_fab_auth_manager_airflow_architecture.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/fab/auth_manager/api_endpoints/test_role_and_permission_endpoint.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/views/auth.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/core/test_impersonation_tests.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/security_manager/aws_security_manager_override.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/api_connexion/security.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/security/permissions.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/amazon/aws/auth_manager/avp/entities.py']
Ground Truth : ['/dev/null', 'a/airflow/www/security.py', 'a/airflow/www/fab_security/views.py', 'a/airflow/auth/managers/fab/views/user_details.py']
Current Recall: 0.09118417271522197

=========================================================

ISSUE: Kubernetes executors hangs on pod submission
**Apache Airflow version**: 1.10.10, 1.10.11, 1.10.12


**Kubernetes version (if you are using kubernetes)** (use `kubectl version`): 1.15.11, 1.17.7

**Environment**:

- **Cloud provider or hardware configuration**: AKS
- **Others**: Python 3.6, Python 3.7, Python 3.8

Kubernetes executors hangs from time to time on worker pod submission. Pod creation request is timeouting after 15 minutes and scheduler loop continues. I have recreated the problem in several Kubernetes / Python / Airflow version configurations. 

py-spy dump:
```
Process 6: /usr/local/bin/python /usr/local/bin/airflow scheduler
Python v3.7.9 (/usr/local/bin/python3.7)

Thread 6 (idle): "MainThread"
    read (ssl.py:929)
    recv_into (ssl.py:1071)
    readinto (socket.py:589)
    _read_status (http/client.py:271)
    begin (http/client.py:310)
    getresponse (http/client.py:1369)
    _make_request (urllib3/connectionpool.py:421)
    urlopen (urllib3/connectionpool.py:677)
    urlopen (urllib3/poolmanager.py:336)
    request_encode_body (urllib3/request.py:171)
    request (urllib3/request.py:80)
    request (kubernetes/client/rest.py:170)
    POST (kubernetes/client/rest.py:278)
    request (kubernetes/client/api_client.py:388)
    __call_api (kubernetes/client/api_client.py:176)
    call_api (kubernetes/client/api_client.py:345)
    create_namespaced_pod_with_http_info (kubernetes/client/api/core_v1_api.py:6265)
    create_namespaced_pod (kubernetes/client/api/core_v1_api.py:6174)
    run_pod_async (airflow/contrib/kubernetes/pod_launcher.py:81)
    run_next (airflow/contrib/executors/kubernetes_executor.py:486)
    sync (airflow/contrib/executors/kubernetes_executor.py:878)
    heartbeat (airflow/executors/base_executor.py:134)
    _validate_and_run_task_instances (airflow/jobs/scheduler_job.py:1505)
    _execute_helper (airflow/jobs/scheduler_job.py:1443)
    _execute (airflow/jobs/scheduler_job.py:1382)
    run (airflow/jobs/base_job.py:221)
    scheduler (airflow/bin/cli.py:1040)
    wrapper (airflow/utils/cli.py:75)
    <module> (airflow:37)
```
logs:
```
020-08-26 18:26:25,721] {base_executor.py:122} DEBUG - 0 running task instances
[2020-08-26 18:26:25,722] {base_executor.py:123} DEBUG - 1 in queue
[2020-08-26 18:26:25,722] {base_executor.py:124} DEBUG - 32 open slots
[2020-08-26 18:26:25,722] {kubernetes_executor.py:840} INFO - Add task ('ProcessingTask', 'exec_spark_notebook', datetime.datetime(2020, 8, 26, 18, 26, 21, 61159, tzinfo=<
Timezone [UTC]>), 1) with command ['airflow', 'run', 'ProcessingTask', 'exec_spark_notebook', '2020-08-26T18:26:21.061159+00:00', '--local', '--pool', 'default_pool', '-sd
', '/usr/local/airflow/dags/qubole_processing.py'] with executor_config {}
[2020-08-26 18:26:25,723] {base_executor.py:133} DEBUG - Calling the <class 'airflow.contrib.executors.kubernetes_executor.KubernetesExecutor'> sync method
[2020-08-26 18:26:25,723] {kubernetes_executor.py:848} DEBUG - self.running: {('ProcessingTask', 'exec_spark_notebook', datetime.datetime(2020, 8, 26, 18, 26, 21, 61159, t
zinfo=<Timezone [UTC]>), 1): ['airflow', 'run', 'ProcessingTask', 'exec_spark_notebook', '2020-08-26T18:26:21.061159+00:00', '--local', '--pool', 'default_pool', '-sd', '/
usr/local/airflow/dags/qubole_processing.py']}
[2020-08-26 18:26:25,725] {kubernetes_executor.py:471} INFO - Kubernetes job is (('ProcessingTask', 'exec_spark_notebook', datetime.datetime(2020, 8, 26, 18, 26, 21, 61159
, tzinfo=<Timezone [UTC]>), 1), ['airflow', 'run', 'ProcessingTask', 'exec_spark_notebook', '2020-08-26T18:26:21.061159+00:00', '--local', '--pool', 'default_pool', '-sd',
 '/usr/local/airflow/dags/qubole_processing.py'], KubernetesExecutorConfig(image=None, image_pull_policy=None, request_memory=None, request_cpu=None, limit_memory=None, limi
t_cpu=None, limit_gpu=None, gcp_service_account_key=None, node_selectors=None, affinity=None, annotations={}, volumes=[], volume_mounts=[], tolerations=None, labels={}))
[2020-08-26 18:26:25,725] {kubernetes_executor.py:474} DEBUG - Kubernetes running for command ['airflow', 'run', 'ProcessingTask', 'exec_spark_notebook', '2020-08-26T18:26
:21.061159+00:00', '--local', '--pool', 'default_pool', '-sd', '/usr/local/airflow/dags/qubole_processing.py']
[2020-08-26 18:26:25,726] {kubernetes_executor.py:475} DEBUG - Kubernetes launching image dpadevairflowacr01.azurecr.io/airflow:1.10.10-20200826-v2
[2020-08-26 18:26:25,729] {pod_launcher.py:79} DEBUG - Pod Creation Request:
{{ POD JSON }}
[2020-08-26 18:26:26,003] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5546)
[2020-08-26 18:26:26,612] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor779-Process, stopped)>
[2020-08-26 18:26:28,148] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5553)
[2020-08-26 18:26:28,628] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor780-Process, stopped)>
[2020-08-26 18:26:31,473] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5560)
[2020-08-26 18:26:32,005] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor781-Process, stopped)>
[2020-08-26 18:26:32,441] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5567)
[2020-08-26 18:26:33,017] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor782-Process, stopped)>
[2020-08-26 18:26:37,501] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5578)
[2020-08-26 18:26:38,044] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor784-Process, stopped)>
[2020-08-26 18:26:38,510] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5585)
[2020-08-26 18:26:39,054] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor785-Process, stopped)>
[2020-08-26 18:26:39,481] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5574)
[2020-08-26 18:26:40,057] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor783-Process, stopped)>
[2020-08-26 18:26:44,549] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5599)
[2020-08-26 18:26:45,108] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor787-Process, stopped)>
[2020-08-26 18:26:45,613] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5606)
[2020-08-26 18:26:46,118] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor788-Process, stopped)>
[2020-08-26 18:26:48,742] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5595)
[2020-08-26 18:26:49,127] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor786-Process, stopped)>
[2020-08-26 18:26:50,596] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5616)
[2020-08-26 18:26:51,151] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor789-Process, stopped)>
[2020-08-26 18:26:51,653] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5623)
[2020-08-26 18:26:52,161] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor790-Process, stopped)>
[2020-08-26 18:26:54,664] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5630)
[2020-08-26 18:26:55,179] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor791-Process, stopped)>
[2020-08-26 18:26:56,645] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5637)
[2020-08-26 18:26:57,194] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor792-Process, stopped)>
[2020-08-26 18:26:57,592] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5644)
[2020-08-26 18:26:58,207] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor793-Process, stopped)>
[2020-08-26 18:27:00,809] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5651)
[2020-08-26 18:27:01,599] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor794-Process, stopped)>
[2020-08-26 18:27:03,124] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5658)
[2020-08-26 18:27:03,615] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor795-Process, stopped)>
[2020-08-26 18:27:04,120] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5665)
[2020-08-26 18:27:04,627] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor796-Process, stopped)>
[2020-08-26 18:27:07,167] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5672)
[2020-08-26 18:27:07,642] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor797-Process, stopped)>
[2020-08-26 18:27:09,125] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5679)
[2020-08-26 18:27:09,654] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor798-Process, stopped)>
[2020-08-26 18:27:10,201] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5686)
[2020-08-26 18:27:10,664] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor799-Process, stopped)>
[2020-08-26 18:27:15,149] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5697)
[2020-08-26 18:27:15,706] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor801-Process, stopped)>
[2020-08-26 18:27:16,128] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5704)
[2020-08-26 18:27:16,717] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor802-Process, stopped)>
[2020-08-26 18:27:18,167] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5693)
[2020-08-26 18:27:18,725] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor800-Process, stopped)>
[2020-08-26 18:27:21,200] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5714)
[2020-08-26 18:27:21,751] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor803-Process, stopped)>
[2020-08-26 18:27:22,221] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5721)
[2020-08-26 18:27:22,760] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor804-Process, stopped)>
[2020-08-26 18:27:24,192] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5728)
[2020-08-26 18:27:24,773] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor805-Process, stopped)>
[2020-08-26 18:27:27,249] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5735)
[2020-08-26 18:27:27,787] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor806-Process, stopped)>
[2020-08-26 18:27:28,246] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5742)
[2020-08-26 18:27:28,798] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor807-Process, stopped)>
[2020-08-26 18:27:30,318] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5749)
[2020-08-26 18:27:30,810] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor808-Process, stopped)>
[2020-08-26 18:27:33,747] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5756)
[2020-08-26 18:27:34,260] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor809-Process, stopped)>
[2020-08-26 18:27:34,670] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5763)
[2020-08-26 18:27:35,271] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor810-Process, stopped)>
[2020-08-26 18:27:36,765] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5770)
[2020-08-26 18:27:37,286] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor811-Process, stopped)>
[2020-08-26 18:27:39,802] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5777)
[2020-08-26 18:27:40,304] {scheduler_job.py:268} DEBUG - Waiting for <Process(DagFileProcessor812-Process, stopped)>
[2020-08-26 18:27:45,820] {settings.py:278} DEBUG - Disposing DB connection pool (PID 5784)
[2020-08-26 18:42:04,209] {kubernetes_executor.py:885} WARNING - HTTPError when attempting to run task, re-queueing. Exception: HTTPSConnectionPool(host='10.2.0.1', port=443
): Read timed out. (read timeout=None)
[2020-08-26 18:42:04,211] {scheduler_job.py:1450} DEBUG - Heartbeating the scheduler
[2020-08-26 18:42:04,352] {base_job.py:200} DEBUG - [heartbeat]
[2020-08-26 18:42:04,353] {scheduler_job.py:1459} DEBUG - Ran scheduling loop in 938.71 seconds
[2020-08-26 18:42:04,353] {scheduler_job.py:1462} DEBUG - Sleeping for 1.00 seconds
[2020-08-26 18:42:05,354] {scheduler_job.py:1425} DEBUG - Starting Loop...
[2020-08-26 18:42:05,355] {scheduler_job.py:1436} DEBUG - Harvesting DAG parsing results
[2020-08-26 18:42:05,355] {dag_processing.py:648} DEBUG - Received message of type SimpleDag
[2020-08-26 18:42:05,355] {dag_processing.py:648} DEBUG - Received message of type DagParsingStat
[2020-08-26 18:42:05,356] {dag_processing.py:648} DEBUG - Received message of type DagParsingStat
```

How I can configure Airflow to emit logs from imported packages ? I would like to check `urllib3` and `http.client` logs in order to understand the problem. Airflow scheduler logs shows only Airflow codebase logs. 



Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/timetables/test_interval_timetable.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/api_connexion/schemas/test_task_instance_schema.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0134_2_9_0_add_rendered_map_index_to_taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/utils/test_log_handlers.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/hive/macros/test_hive.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/celery/executors/test_celery_kubernetes_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0002_1_5_0_create_is_encrypted.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0117_2_4_0_add_processor_subdir_to_dagmodel_and_.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/local_task_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/jobs/test_local_task_job.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/providers/apache/spark/hooks/test_spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/migrations/versions/0026_1_8_2_increase_text_size_for_mysql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_internal_api_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/microsoft/psrp/hooks/psrp.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/cli_config.py']
Ground Truth : ['a/airflow/kubernetes/kube_client.py']
Current Recall: 0.09118417271522197

=========================================================

ISSUE: both airflow dags test and airflow backfill cli commands got same error in airflow Version 2.0.1
**Apache Airflow version: 2.0.1**
**What happened:**
Running an airflow dags test or backfill  CLI command shown in tutorial, produces the same error.

**dags test cli command result**

```
(airflow_venv) (base) app@lunar_01:airflow$ airflow dags test tutorial 2015-06-01
[2021-02-16 04:29:22,355] {dagbag.py:448} INFO - Filling up the DagBag from /home/app/Lunar/src/airflow/dags
[2021-02-16 04:29:22,372] {example_kubernetes_executor_config.py:174} WARNING - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'
[2021-02-16 04:29:22,373] {example_kubernetes_executor_config.py:175} WARNING - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes']
Traceback (most recent call last):
  File "/home/app/airflow_venv/bin/airflow", line 10, in <module>
    sys.exit(main())
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 65, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/cli.py", line 89, in wrapper
    return f(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/cli/commands/dag_command.py", line 389, in dag_test
    dag.run(executor=DebugExecutor(), start_date=args.execution_date, end_date=args.execution_date)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/models/dag.py", line 1706, in run
    job.run()
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/base_job.py", line 237, in run
    self._execute()
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 65, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/backfill_job.py", line 805, in _execute
    session=session,
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 62, in wrapper
    return func(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/backfill_job.py", line 715, in _execute_for_run_dates
    tis_map = self._task_instances_for_dag_run(dag_run, session=session)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 62, in wrapper
    return func(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/backfill_job.py", line 359, in _task_instances_for_dag_run
    dag_run.refresh_from_db()
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 65, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/models/dagrun.py", line 178, in refresh_from_db
    DR.run_id == self.run_id,
  File "/home/app/airflow_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 3500, in one
    raise orm_exc.NoResultFound("No row was found for one()")
sqlalchemy.orm.exc.NoResultFound: No row was found for one()
```

**backfill cli command result:**
```
(airflow_venv) (base) app@lunar_01:airflow$ airflow dags backfill tutorial --start-date 2015-06-01 --end-date 2015-06-07
/home/app/airflow_venv/lib/python3.7/site-packages/airflow/cli/commands/dag_command.py:62 PendingDeprecationWarning: --ignore-first-depends-on-past is deprecated as the value is always set to True
[2021-02-16 04:30:16,979] {dagbag.py:448} INFO - Filling up the DagBag from /home/app/Lunar/src/airflow/dags
[2021-02-16 04:30:16,996] {example_kubernetes_executor_config.py:174} WARNING - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'
[2021-02-16 04:30:16,996] {example_kubernetes_executor_config.py:175} WARNING - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes']
Traceback (most recent call last):
  File "/home/app/airflow_venv/bin/airflow", line 10, in <module>
    sys.exit(main())
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/cli.py", line 89, in wrapper
    return f(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/cli/commands/dag_command.py", line 116, in dag_backfill
    run_backwards=args.run_backwards,
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/models/dag.py", line 1706, in run
    job.run()
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/base_job.py", line 237, in run
    self._execute()
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 65, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/backfill_job.py", line 805, in _execute
    session=session,
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 62, in wrapper
    return func(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/backfill_job.py", line 715, in _execute_for_run_dates
    tis_map = self._task_instances_for_dag_run(dag_run, session=session)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 62, in wrapper
    return func(*args, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/jobs/backfill_job.py", line 359, in _task_instances_for_dag_run
    dag_run.refresh_from_db()
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/utils/session.py", line 65, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/app/airflow_venv/lib/python3.7/site-packages/airflow/models/dagrun.py", line 178, in refresh_from_db
    DR.run_id == self.run_id,
  File "/home/app/airflow_venv/lib/python3.7/site-packages/sqlalchemy/orm/query.py", line 3500, in one
    raise orm_exc.NoResultFound("No row was found for one()")
sqlalchemy.orm.exc.NoResultFound: No row was found for one()
```

Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_base.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/commands/release_management_commands.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_home.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/utils/cli.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_log.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/apache/spark/hooks/spark_submit.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/docs/conf.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/__init__.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/dag_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/cdxgen.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/cli/commands/test_task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_tasks.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/scripts/ci/pre_commit/common_precommit_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/cli/commands/task_command.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/celery/executors/celery_executor.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/www/views/test_views_trigger_dag.py']
Ground Truth : ['a/airflow/models/dagrun.py']
Current Recall: 0.09118417271522197

=========================================================

ISSUE: Snowflake hook doesn't parameterize SQL passed as a string type, causing SnowflakeOperator to fail
<!--

Welcome to Apache Airflow!  For a smooth issue process, try to answer the following questions.
Don't worry if they're not all applicable; just try to include what you can :-)

If you need to include code snippets or logs, please put them in fenced code
blocks.  If they're super-long, please use the details tag like
<details><summary>super-long log</summary> lots of stuff </details>

Please delete these comment blocks before submitting the issue.

-->

<!--

IMPORTANT!!!

PLEASE CHECK "SIMILAR TO X EXISTING ISSUES" OPTION IF VISIBLE
NEXT TO "SUBMIT NEW ISSUE" BUTTON!!!

PLEASE CHECK IF THIS ISSUE HAS BEEN REPORTED PREVIOUSLY USING SEARCH!!!

Please complete the next sections or the issue will be closed.
These questions are the first thing we need to know to understand the context.

-->

**Apache Airflow version**: 2.1.0

**What happened**: The new Snowflake hook run method is not taking parameters into account when the SQL is passed as a string; it's using Snowflake connector's execute_string method, which does not support parameterization. So the only way to parameterize your query from a SnowflakeOperator is to put the SQL into a list. 

https://github.com/apache/airflow/blob/304e174674ff6921cb7ed79c0158949b50eff8fe/airflow/providers/snowflake/hooks/snowflake.py#L272-L279

https://docs.snowflake.com/en/user-guide/python-connector-api.html#execute_string

**How to reproduce it**: Pass a sql string and parameters to SnowflakeOperator; the query will not be parameterized, and will fail as a SQL syntax error on the parameterization characters, e.g. %(param)s.
<!---

As minimally and precisely as possible. Keep in mind we do not have access to your cluster or dags.

If you are using kubernetes, please attempt to recreate the issue using minikube or kind.

## Install minikube/kind

- Minikube https://minikube.sigs.k8s.io/docs/start/
- Kind https://kind.sigs.k8s.io/docs/user/quick-start/

If this is a UI bug, please provide a screenshot of the bug or a link to a youtube video of the bug in action

You can include images using the .md style of
![alt text](http://url/to/img.png)

To record a screencast, mac users can use QuickTime and then create an unlisted youtube video with the resulting .mov file.

--->


**Anything else we need to know**: Quick workaround is to put your sql string into a list, these are still being parameterized correctly. 

<!--

How often does this problem occur? Once? Every time etc?

Any relevant logs to include? Put them here in side a detail tag:
<details><summary>x.log</summary> lots of stuff </details>

-->


Retrieved_files : ['/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/hooks/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataflow.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/bigquery.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/operators/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/dag.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/operators/python.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/scheduler_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/google/cloud/operators/dataproc.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/views.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/snowflake/hooks/snowflake.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/dev/breeze/src/airflow_breeze/utils/docker_command_utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/tests/conftest.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/models/taskinstance.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/operators/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/settings.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/providers/common/sql/hooks/sql.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/jobs/triggerer_job_runner.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/dag_processing/manager.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/www/utils.py', '/home/fdse/zqc/RepoEdit/repos/data/raw_repos/apache/airflow/airflow/configuration.py']
Ground Truth : ['a/airflow/providers/snowflake/hooks/snowflake.py']
Current Recall: 0.09118417271522197

=========================================================

